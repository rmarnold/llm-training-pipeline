{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Mutation Training Data (Colab)\n",
    "\n",
    "Standalone notebook for generating Rust mutation training data using `cargo-mutants`.\n",
    "\n",
    "**What this does:**\n",
    "1. Clones curated Rust repositories\n",
    "2. Runs `cargo-mutants` to introduce mutations\n",
    "3. Captures (buggy code, compiler error / test failure, fix) triples\n",
    "4. Saves as JSONL + HuggingFace dataset to Google Drive\n",
    "\n",
    "**Requirements:** Colab instance with high RAM (recommended: High-RAM runtime with 50+ GB)\n",
    "\n",
    "**Time estimate:** 2-4 hours for all 21 repos at 100 mutations each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 0.1 Mount Drive & Clone Repo\n",
    "import os\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted\")\n",
    "else:\n",
    "    raise RuntimeError(\"This notebook is designed for Google Colab. Use generate_mutations_local.ipynb for local Jupyter.\")\n",
    "\n",
    "REPO_URL = \"https://github.com/rmarnold/llm-training-pipeline.git\"  #@param {type:\"string\"}\n",
    "BRANCH = \"main\"  #@param {type:\"string\"}\n",
    "\n",
    "REPO_DIR = \"/content/llm-training-pipeline\"\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    %cd {REPO_DIR}\n",
    "    !git pull origin {BRANCH}\n",
    "else:\n",
    "    !git clone -b {BRANCH} {REPO_URL} {REPO_DIR}\n",
    "    %cd {REPO_DIR}\n",
    "\n",
    "PROJECT_ROOT = REPO_DIR\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"\\nProject root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 0.2 Install Dependencies\n",
    "#@markdown Installs Python deps and the Rust toolchain (rustup + cargo-mutants).\n",
    "\n",
    "print(\"Installing Python dependencies...\")\n",
    "print(\"=\" * 60)\n",
    "!pip install -q -e \".[gpt_oss,colab]\"\n",
    "\n",
    "print(\"\\nInstalling Rust toolchain...\")\n",
    "print(\"=\" * 60)\n",
    "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
    "os.environ[\"PATH\"] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "!cargo install cargo-mutants\n",
    "\n",
    "# Verification\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Verification:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import subprocess\n",
    "for cmd, label in [(\"cargo --version\", \"cargo\"), (\"cargo mutants --version\", \"cargo-mutants\")]:\n",
    "    result = subprocess.run(cmd.split(), capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"  \\u2713 {label}: {result.stdout.strip()}\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 {label}: not found\")\n",
    "\n",
    "for pkg in [\"datasets\", \"yaml\"]:\n",
    "    try:\n",
    "        __import__(pkg if pkg != \"yaml\" else \"yaml\")\n",
    "        print(f\"  \\u2713 {pkg}\")\n",
    "    except ImportError:\n",
    "        print(f\"  \\u2717 {pkg}: not installed\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 0.3 Configure { run: \"auto\" }\n",
    "#@markdown ---\n",
    "\n",
    "max_mutations_per_repo = 100  #@param {type:\"integer\"}\n",
    "#@markdown Maximum mutations to generate per repository\n",
    "\n",
    "timeout_per_mutation = 300  #@param {type:\"integer\"}\n",
    "#@markdown Timeout per mutation test in seconds\n",
    "\n",
    "# ============================================================\n",
    "# Auto-detect CPU count and RAM for safe parallelism\n",
    "# ============================================================\n",
    "import multiprocessing\n",
    "\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "\n",
    "# RAM-aware: each cargo-mutants worker + its cargo subprocesses can use\n",
    "# 1-4 GB depending on the crate size. Allow ~20 GB per worker to be safe.\n",
    "try:\n",
    "    _mem_bytes = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')\n",
    "    total_ram_gb = _mem_bytes / (1024**3)\n",
    "    ram_jobs = max(1, int(total_ram_gb / 20))\n",
    "except (ValueError, OSError):\n",
    "    total_ram_gb = 0\n",
    "    ram_jobs = cpu_count\n",
    "\n",
    "mutation_jobs = min(max(1, cpu_count - 2), ram_jobs)\n",
    "\n",
    "# Storage paths\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/gpt-oss-20b-rust-agent-v2\"\n",
    "CLONE_DIR = \"/tmp/rust_repos\"  # Local SSD for fast clones\n",
    "OUTPUT_DIR = os.path.join(DRIVE_BASE, \"data/rust/mutations\")  # Direct to Drive\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(CLONE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MUTATION GENERATION CONFIG\")\n",
    "print(\"=\" * 60)\n",
    "ram_str = f\"{total_ram_gb:.0f} GB\" if total_ram_gb else \"unknown\"\n",
    "print(f\"  CPUs: {cpu_count}\")\n",
    "print(f\"  RAM: {ram_str}\")\n",
    "print(f\"  Parallel jobs: {mutation_jobs}\")\n",
    "print(f\"  Max mutations/repo: {max_mutations_per_repo}\")\n",
    "print(f\"  Timeout/mutation: {timeout_per_mutation}s\")\n",
    "print(f\"  Clone dir: {CLONE_DIR} (local SSD)\")\n",
    "print(f\"  Output dir: {OUTPUT_DIR} (Drive)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Generate Mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 1.1 Run Mutation Generation\n",
    "#@markdown Clones all curated Rust repos and runs cargo-mutants on each.\n",
    "#@markdown Progress is printed per-repo.\n",
    "\n",
    "print(f\"Starting mutation generation ({mutation_jobs} parallel jobs)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "!python scripts/16_generate_mutations.py \\\n",
    "    --config configs/data_sources_rust.yaml \\\n",
    "    --clone_dir {CLONE_DIR} \\\n",
    "    --output_dir {OUTPUT_DIR} \\\n",
    "    --max_mutations_per_repo {max_mutations_per_repo} \\\n",
    "    --timeout_per_mutation {timeout_per_mutation} \\\n",
    "    --jobs {mutation_jobs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 1.2 Monitor Progress (run in a separate tab)\n",
    "#@markdown Run this cell while 1.1 is executing to check status.\n",
    "\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# Check JSONL output so far\n",
    "jsonl_path = os.path.join(OUTPUT_DIR, \"mutations.jsonl\")\n",
    "if os.path.exists(jsonl_path):\n",
    "    with open(jsonl_path) as f:\n",
    "        lines = f.readlines()\n",
    "    print(f\"Training examples so far: {len(lines)}\")\n",
    "\n",
    "    if lines:\n",
    "        last = json.loads(lines[-1])\n",
    "        print(f\"Last example file: {last.get('explanation', '')[:80]}...\")\n",
    "else:\n",
    "    print(\"No output file yet (generation still starting up)\")\n",
    "\n",
    "# Check which repos have been cloned\n",
    "if os.path.exists(CLONE_DIR):\n",
    "    repos = [d for d in os.listdir(CLONE_DIR) if os.path.isdir(os.path.join(CLONE_DIR, d))]\n",
    "    print(f\"\\nRepos cloned: {len(repos)}\")\n",
    "    for r in sorted(repos):\n",
    "        print(f\"  - {r}\")\n",
    "\n",
    "# Show system resource usage\n",
    "print(\"\\nSystem resources:\")\n",
    "!free -h 2>/dev/null | head -3 || vm_stat | head -5\n",
    "print()\n",
    "!uptime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Verify & Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 2.1 Verify Output\n",
    "\n",
    "import json\n",
    "\n",
    "jsonl_path = os.path.join(OUTPUT_DIR, \"mutations.jsonl\")\n",
    "hf_path = os.path.join(OUTPUT_DIR, \"hf_dataset\")\n",
    "\n",
    "print(\"Output Verification:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check JSONL\n",
    "if os.path.exists(jsonl_path):\n",
    "    with open(jsonl_path) as f:\n",
    "        lines = f.readlines()\n",
    "    size_mb = os.path.getsize(jsonl_path) / (1024 * 1024)\n",
    "    print(f\"  \\u2713 JSONL: {len(lines):,} examples ({size_mb:.1f} MB)\")\n",
    "\n",
    "    # Count by type\n",
    "    caught = sum(1 for l in lines if '\"Test failure:' in l)\n",
    "    unviable = sum(1 for l in lines if '\"Compiler error:' in l)\n",
    "    print(f\"    Caught mutations (test failures): {caught:,}\")\n",
    "    print(f\"    Unviable mutations (compiler errors): {unviable:,}\")\n",
    "else:\n",
    "    print(f\"  \\u2717 JSONL not found at {jsonl_path}\")\n",
    "\n",
    "# Check HF dataset\n",
    "if os.path.exists(hf_path):\n",
    "    items = os.listdir(hf_path)\n",
    "    print(f\"  \\u2713 HF dataset: {hf_path} ({len(items)} files)\")\n",
    "else:\n",
    "    print(f\"  \\u2014 HF dataset not found (generated after JSONL)\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 2.2 Inspect Sample Examples\n",
    "\n",
    "import json\n",
    "\n",
    "jsonl_path = os.path.join(OUTPUT_DIR, \"mutations.jsonl\")\n",
    "\n",
    "if not os.path.exists(jsonl_path):\n",
    "    print(\"No data yet. Run Step 1 first.\")\n",
    "else:\n",
    "    with open(jsonl_path) as f:\n",
    "        examples = [json.loads(line) for line in f.readlines()[:5]]\n",
    "\n",
    "    for i, ex in enumerate(examples, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Example {i}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Explanation: {ex.get('explanation', 'N/A')[:120]}\")\n",
    "        print(f\"\\nBuggy code (first 200 chars):\")\n",
    "        print(ex.get('buggy_code', '')[:200])\n",
    "        print(f\"\\nError (first 200 chars):\")\n",
    "        print(ex.get('error_message', '')[:200])\n",
    "        print(f\"\\nFixed code (first 200 chars):\")\n",
    "        print(ex.get('fixed_code', '')[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 2.3 Stats by Repository\n",
    "#@markdown Shows how many training examples came from each repo.\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "jsonl_path = os.path.join(OUTPUT_DIR, \"mutations.jsonl\")\n",
    "\n",
    "if not os.path.exists(jsonl_path):\n",
    "    print(\"No data yet. Run Step 1 first.\")\n",
    "else:\n",
    "    repo_counts = Counter()\n",
    "    type_counts = Counter()\n",
    "\n",
    "    with open(jsonl_path) as f:\n",
    "        for line in f:\n",
    "            ex = json.loads(line)\n",
    "            explanation = ex.get('explanation', '')\n",
    "            # Extract file path from explanation\n",
    "            if '(' in explanation and ')' in explanation:\n",
    "                file_path = explanation.split('(')[-1].rstrip(')')\n",
    "                # Get crate-level path\n",
    "                parts = file_path.split('/')\n",
    "                repo_counts[parts[0] if parts else 'unknown'] += 1\n",
    "            # Count error types\n",
    "            if 'Test failure' in ex.get('error_message', ''):\n",
    "                type_counts['caught (test failure)'] += 1\n",
    "            elif 'Compiler error' in ex.get('error_message', ''):\n",
    "                type_counts['unviable (compiler error)'] += 1\n",
    "\n",
    "    print(\"Examples by source file prefix:\")\n",
    "    print(\"=\" * 60)\n",
    "    for repo, count in repo_counts.most_common():\n",
    "        print(f\"  {repo:<30} {count:>5}\")\n",
    "    print(f\"  {'TOTAL':<30} {sum(repo_counts.values()):>5}\")\n",
    "\n",
    "    print(f\"\\nExamples by type:\")\n",
    "    print(\"=\" * 60)\n",
    "    for t, count in type_counts.most_common():\n",
    "        print(f\"  {t:<35} {count:>5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Re-run Failed Repos (Optional)\n",
    "\n",
    "If some repos failed or timed out, you can re-run them individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 3.1 Re-run Specific Repos\n",
    "#@markdown Comma-separated list of repos to re-run (e.g., \"tokio-rs/tokio,serde-rs/serde\")\n",
    "\n",
    "retry_repos = \"\"  #@param {type:\"string\"}\n",
    "#@markdown Leave empty to skip\n",
    "\n",
    "if retry_repos.strip():\n",
    "    repos = [r.strip() for r in retry_repos.split(\",\") if r.strip()]\n",
    "    repos_arg = \" \".join(repos)\n",
    "\n",
    "    # Use a separate output dir to avoid overwriting\n",
    "    retry_output = os.path.join(OUTPUT_DIR, \"retry\")\n",
    "    os.makedirs(retry_output, exist_ok=True)\n",
    "\n",
    "    print(f\"Re-running {len(repos)} repos...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/16_generate_mutations.py \\\n",
    "        --repos {repos_arg} \\\n",
    "        --clone_dir {CLONE_DIR} \\\n",
    "        --output_dir {retry_output} \\\n",
    "        --max_mutations_per_repo {max_mutations_per_repo} \\\n",
    "        --timeout_per_mutation {timeout_per_mutation} \\\n",
    "        --jobs {mutation_jobs}\n",
    "\n",
    "    # Merge retry results into main output\n",
    "    retry_jsonl = os.path.join(retry_output, \"mutations.jsonl\")\n",
    "    main_jsonl = os.path.join(OUTPUT_DIR, \"mutations.jsonl\")\n",
    "    if os.path.exists(retry_jsonl):\n",
    "        with open(retry_jsonl) as f:\n",
    "            retry_lines = f.readlines()\n",
    "        with open(main_jsonl, \"a\") as f:\n",
    "            f.writelines(retry_lines)\n",
    "        print(f\"\\nMerged {len(retry_lines)} retry examples into {main_jsonl}\")\n",
    "else:\n",
    "    print(\"No repos to retry. Set retry_repos above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Verify Drive Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 4.1 Verify Data on Drive\n",
    "\n",
    "print(\"Drive Data Verification:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, path in [\n",
    "    (\"mutations.jsonl\", os.path.join(OUTPUT_DIR, \"mutations.jsonl\")),\n",
    "    (\"hf_dataset/\", os.path.join(OUTPUT_DIR, \"hf_dataset\")),\n",
    "]:\n",
    "    if os.path.exists(path):\n",
    "        if os.path.isdir(path):\n",
    "            items = os.listdir(path)\n",
    "            print(f\"  \\u2713 {name} ({len(items)} files)\")\n",
    "        else:\n",
    "            size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "            print(f\"  \\u2713 {name} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 {name} not found\")\n",
    "\n",
    "print(f\"\\nDrive path: {OUTPUT_DIR}\")\n",
    "print(\"\\nThis data is ready for use by the training notebook.\")\n",
    "print(\"Set skip_data_generation=True in train_gpt_oss_rust_agent_v2.ipynb\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Done!\n",
    "\n",
    "Your mutation training data is saved to Google Drive.\n",
    "\n",
    "**Next steps:**\n",
    "- Open `train_gpt_oss_rust_agent_v2.ipynb`\n",
    "- Set `skip_data_generation = True` in Step 0.3\n",
    "- The training notebook will use this pre-generated data from Drive\n",
    "\n",
    "**Output location:**\n",
    "- JSONL: `Drive/gpt-oss-20b-rust-agent-v2/data/rust/mutations/mutations.jsonl`\n",
    "- HF Dataset: `Drive/gpt-oss-20b-rust-agent-v2/data/rust/mutations/hf_dataset/`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
