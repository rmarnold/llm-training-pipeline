{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Custom LLM from Scratch\n",
    "\n",
    "This notebook trains a language model from scratch with configurable:\n",
    "- **Model size** (125M to 7B parameters)\n",
    "- **Model type** (Reasoning Agent, Code Assistant, General Purpose, etc.)\n",
    "- **Dataset selection** (automatically matched to model type)\n",
    "\n",
    "## Requirements\n",
    "- **GPU**: A100 40GB+ (80GB recommended for 3B+)\n",
    "- **Storage**: Local storage for checkpoints\n",
    "- **Time**: Varies by model size and type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Environment Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "IN_JUPYTER = not IN_COLAB  # Standard Jupyter environment\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted\")\n",
    "else:\n",
    "    print(\"Running in standard Jupyter environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "REPO_URL = \"https://github.com/rmarnold/llm-training-pipeline.git\"\n",
    "BRANCH = \"main\"\n",
    "\n",
    "if IN_COLAB:\n",
    "    REPO_DIR = \"/content/llm-training-pipeline\"\n",
    "    if os.path.exists(REPO_DIR):\n",
    "        os.chdir(REPO_DIR)\n",
    "        !git pull origin {BRANCH}\n",
    "    else:\n",
    "        !git clone -b {BRANCH} {REPO_URL} {REPO_DIR}\n",
    "        os.chdir(REPO_DIR)\n",
    "    \n",
    "    print(\"\\nInstalling dependencies...\")\n",
    "    !pip install -q -e \".[colab]\"\n",
    "    !pip install -q flash-attn --no-build-isolation 2>/dev/null || true\n",
    "    !pip install -q liger-kernel bitsandbytes\n",
    "    !pip install -q datasketch>=1.6\n",
    "    \n",
    "    PROJECT_ROOT = REPO_DIR\n",
    "else:\n",
    "    # For local Jupyter, assume we're in the repo directory\n",
    "    PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd())))\n",
    "    if os.path.exists(os.path.join(PROJECT_ROOT, 'scripts')):\n",
    "        pass  # Already in correct location\n",
    "    elif os.path.exists(os.path.join(os.getcwd(), 'scripts')):\n",
    "        PROJECT_ROOT = os.getcwd()\n",
    "    elif os.path.exists(os.path.join(os.path.dirname(os.getcwd()), 'scripts')):\n",
    "        PROJECT_ROOT = os.path.dirname(os.getcwd())\n",
    "    \n",
    "    print(f\"Project root detected: {PROJECT_ROOT}\")\n",
    "    print(\"\\nTo install dependencies, run in terminal:\")\n",
    "    print(\"  pip install -e '.[dev]'\")\n",
    "    print(\"  pip install flash-attn --no-build-isolation\")\n",
    "    print(\"  pip install liger-kernel bitsandbytes datasketch\")\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"\\nWorking directory: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Model Configuration\n",
    "\n",
    "Configure your model type and size below:\n",
    "\n",
    "**Model Types:**\n",
    "- `reasoning_agent`: Math, logic, function calling, tool use\n",
    "- `code_assistant`: Code generation, debugging, explanation\n",
    "- `general_assistant`: Balanced instruction following\n",
    "- `chat_model`: Conversational, helpful responses\n",
    "\n",
    "**Model Sizes:**\n",
    "- `125m`: Fast training, testing (~2-4 hours pretrain)\n",
    "- `350m`: Small but capable (~4-8 hours)\n",
    "- `1b`: Good balance (~12-16 hours)\n",
    "- `3b`: Strong performance (~30-40 hours)\n",
    "- `7b`: Full capability (~60-80 hours)\n",
    "\n",
    "**Data Prep Quality Modes:**\n",
    "- `fast`: Skip quality filters, just clean + dedup (~5 min for 12M docs)\n",
    "- `balanced`: Basic quality filter, no toxicity (~30 min)\n",
    "- `thorough`: All quality filters + toxicity (~2-3 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# USER CONFIGURATION - Modify these values\n",
    "# ============================================================\n",
    "\n",
    "model_type = \"reasoning_agent\"  # Options: reasoning_agent, code_assistant, general_assistant, chat_model\n",
    "model_size = \"1b\"               # Options: 125m, 350m, 1b, 3b, 7b\n",
    "context_length = 2048           # Max context length in tokens\n",
    "pretrain_tokens_billions = 50   # Recommended: 20x model params (1B model = 20B tokens minimum)\n",
    "data_prep_speed = \"fast\"        # Options: fast, balanced, thorough\n",
    "use_gpu_data_prep = True        # Use RAPIDS cuDF + NeMo Curator (10-30x faster)\n",
    "\n",
    "# ============================================================\n",
    "# MODEL TYPE CONFIGURATIONS\n",
    "# ============================================================\n",
    "\n",
    "MODEL_TYPE_CONFIGS = {\n",
    "    \"reasoning_agent\": {\n",
    "        \"description\": \"Optimized for math, logic, and tool use\",\n",
    "        \"pretraining_datasets\": [\n",
    "            \"slimpajama\", \"wikipedia\", \"the-stack-python\", \n",
    "            \"openwebtext\", \"arxiv\", \"stackexchange\"\n",
    "        ],\n",
    "        \"sft_datasets\": [\n",
    "            \"gsm8k\", \"orca-math\", \"openorca\", \"cot-collection\", \"metamath\",\n",
    "            \"glaive-function-calling\", \"hermes-function-calling\", \"toolbench\",\n",
    "            \"logiqa\"\n",
    "        ],\n",
    "        \"dpo_datasets\": [\"hh-rlhf\", \"ultrafeedback\"],\n",
    "        \"data_prep_script\": \"06_prepare_reasoning_data.py\",\n",
    "        \"eval_benchmarks\": [\"gsm8k\", \"arc\", \"function_calling\", \"safety\"],\n",
    "        \"sft_focus\": \"reasoning\",\n",
    "    },\n",
    "    \"code_assistant\": {\n",
    "        \"description\": \"Optimized for code generation and understanding\",\n",
    "        \"pretraining_datasets\": [\n",
    "            \"the-stack\", \"slimpajama\", \"wikipedia\", \"arxiv\"\n",
    "        ],\n",
    "        \"sft_datasets\": [\n",
    "            \"code-alpaca\", \"python-code-instructions\", \"evol-instruct-code\",\n",
    "            \"glaive-function-calling\", \"oasst1\"\n",
    "        ],\n",
    "        \"dpo_datasets\": [\"hh-rlhf\"],\n",
    "        \"data_prep_script\": \"prepare_lora_data.py\",\n",
    "        \"eval_benchmarks\": [\"humaneval\", \"mbpp\", \"safety\"],\n",
    "        \"sft_focus\": \"code\",\n",
    "    },\n",
    "    \"general_assistant\": {\n",
    "        \"description\": \"Balanced instruction following\",\n",
    "        \"pretraining_datasets\": [\n",
    "            \"slimpajama\", \"wikipedia\", \"openwebtext\", \"pg19\"\n",
    "        ],\n",
    "        \"sft_datasets\": [\n",
    "            \"oasst1\", \"dolly-15k\", \"alpaca-cleaned\", \"openorca\"\n",
    "        ],\n",
    "        \"dpo_datasets\": [\"hh-rlhf\", \"ultrafeedback\"],\n",
    "        \"data_prep_script\": \"06_prepare_sft_data.py\",\n",
    "        \"eval_benchmarks\": [\"mmlu\", \"hellaswag\", \"safety\"],\n",
    "        \"sft_focus\": \"instruction\",\n",
    "    },\n",
    "    \"chat_model\": {\n",
    "        \"description\": \"Conversational and helpful\",\n",
    "        \"pretraining_datasets\": [\n",
    "            \"slimpajama\", \"wikipedia\", \"openwebtext\"\n",
    "        ],\n",
    "        \"sft_datasets\": [\n",
    "            \"oasst1\", \"dolly-15k\", \"sharegpt\"\n",
    "        ],\n",
    "        \"dpo_datasets\": [\"hh-rlhf\", \"ultrafeedback\"],\n",
    "        \"data_prep_script\": \"06_prepare_sft_data.py\",\n",
    "        \"eval_benchmarks\": [\"mt-bench\", \"safety\"],\n",
    "        \"sft_focus\": \"chat\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Data prep speed configurations\n",
    "DATA_PREP_CONFIGS = {\n",
    "    \"fast\": {\n",
    "        \"flags\": \"--native-pipeline --fast-quality --no-toxicity --fresh\",\n",
    "        \"gpu_flags\": \"--skip-quality --no-toxicity\",\n",
    "        \"description\": \"Fastest: Skip quality filters, just clean + dedup (~5 min)\",\n",
    "    },\n",
    "    \"balanced\": {\n",
    "        \"flags\": \"--native-pipeline --fast-quality --fresh\",\n",
    "        \"gpu_flags\": \"--fast-quality --no-toxicity\",\n",
    "        \"description\": \"Balanced: Basic quality filter, no toxicity (~30 min)\",\n",
    "    },\n",
    "    \"thorough\": {\n",
    "        \"flags\": \"--native-pipeline --fresh\",\n",
    "        \"gpu_flags\": \"\",\n",
    "        \"description\": \"Thorough: All quality filters + toxicity (~2-3 hours)\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Size configurations optimized for A100 40GB/80GB\n",
    "SIZE_CONFIGS = {\n",
    "    \"125m\": {\n",
    "        \"batch_size\": 64,\n",
    "        \"grad_accum\": 1,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"warmup_ratio\": 0.05,\n",
    "        \"use_torch_compile\": False,\n",
    "        \"use_8bit_optim\": False,\n",
    "        \"dataloader_workers\": 16,\n",
    "        \"pretrain_hours\": 3,\n",
    "        \"sft_hours\": 0.5,\n",
    "        \"dpo_hours\": 0.25,\n",
    "        \"expected_throughput\": \"8-12 it/s\",\n",
    "    },\n",
    "    \"350m\": {\n",
    "        \"batch_size\": 32,\n",
    "        \"grad_accum\": 2,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"warmup_ratio\": 0.05,\n",
    "        \"use_torch_compile\": False,\n",
    "        \"use_8bit_optim\": False,\n",
    "        \"dataloader_workers\": 16,\n",
    "        \"pretrain_hours\": 6,\n",
    "        \"sft_hours\": 1,\n",
    "        \"dpo_hours\": 0.5,\n",
    "        \"expected_throughput\": \"5-8 it/s\",\n",
    "    },\n",
    "    \"1b\": {\n",
    "        \"batch_size\": 16,\n",
    "        \"grad_accum\": 4,\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"warmup_ratio\": 0.03,\n",
    "        \"use_torch_compile\": True,\n",
    "        \"use_8bit_optim\": True,\n",
    "        \"dataloader_workers\": 12,\n",
    "        \"pretrain_hours\": 15,\n",
    "        \"sft_hours\": 4,\n",
    "        \"dpo_hours\": 1.5,\n",
    "        \"expected_throughput\": \"3-5 it/s\",\n",
    "    },\n",
    "    \"3b\": {\n",
    "        \"batch_size\": 8,\n",
    "        \"grad_accum\": 8,\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"warmup_ratio\": 0.03,\n",
    "        \"use_torch_compile\": True,\n",
    "        \"use_8bit_optim\": True,\n",
    "        \"dataloader_workers\": 8,\n",
    "        \"pretrain_hours\": 35,\n",
    "        \"sft_hours\": 10,\n",
    "        \"dpo_hours\": 4,\n",
    "        \"expected_throughput\": \"1.5-2.5 it/s\",\n",
    "    },\n",
    "    \"7b\": {\n",
    "        \"batch_size\": 4,\n",
    "        \"grad_accum\": 16,\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"warmup_ratio\": 0.03,\n",
    "        \"use_torch_compile\": True,\n",
    "        \"use_8bit_optim\": True,\n",
    "        \"dataloader_workers\": 8,\n",
    "        \"pretrain_hours\": 70,\n",
    "        \"sft_hours\": 20,\n",
    "        \"dpo_hours\": 8,\n",
    "        \"expected_throughput\": \"0.8-1.2 it/s\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Build configuration\n",
    "type_config = MODEL_TYPE_CONFIGS[model_type]\n",
    "size_config = SIZE_CONFIGS[model_size]\n",
    "data_prep_config = DATA_PREP_CONFIGS[data_prep_speed]\n",
    "\n",
    "CONFIG = {\n",
    "    'model_type': model_type,\n",
    "    'model_size': model_size,\n",
    "    'context_length': context_length,\n",
    "    'pretrain_tokens_b': pretrain_tokens_billions,\n",
    "    'batch_size': size_config['batch_size'],\n",
    "    'grad_accum': size_config['grad_accum'],\n",
    "    'learning_rate': size_config['learning_rate'],\n",
    "    'warmup_ratio': size_config['warmup_ratio'],\n",
    "    'use_torch_compile': size_config['use_torch_compile'],\n",
    "    'use_8bit_optim': size_config['use_8bit_optim'],\n",
    "    'dataloader_workers': size_config['dataloader_workers'],\n",
    "    'pretraining_datasets': type_config['pretraining_datasets'],\n",
    "    'sft_datasets': type_config['sft_datasets'],\n",
    "    'dpo_datasets': type_config['dpo_datasets'],\n",
    "    'data_prep_script': type_config['data_prep_script'],\n",
    "    'eval_benchmarks': type_config['eval_benchmarks'],\n",
    "    'sft_focus': type_config['sft_focus'],\n",
    "    'data_prep_flags': data_prep_config['flags'],\n",
    "    'gpu_data_prep_flags': data_prep_config['gpu_flags'],\n",
    "    'data_prep_speed': data_prep_speed,\n",
    "    'use_gpu_data_prep': use_gpu_data_prep,\n",
    "}\n",
    "\n",
    "# Calculate training steps\n",
    "effective_batch = CONFIG['batch_size'] * CONFIG['grad_accum']\n",
    "tokens_per_step = effective_batch * context_length\n",
    "total_tokens = int(pretrain_tokens_billions * 1e9)\n",
    "pretrain_steps = total_tokens // tokens_per_step\n",
    "CONFIG['pretrain_steps'] = pretrain_steps\n",
    "CONFIG['effective_batch_size'] = effective_batch\n",
    "\n",
    "# Estimate time\n",
    "time_scale = pretrain_tokens_billions / 50\n",
    "pretrain_hours = size_config['pretrain_hours'] * time_scale\n",
    "total_hours = pretrain_hours + size_config['sft_hours'] + size_config['dpo_hours']\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nType: {model_type.upper().replace('_', ' ')}\")\n",
    "print(f\"Size: {model_size.upper()}, Context: {context_length} tokens\")\n",
    "print(f\"\\nData Prep: {data_prep_speed.upper()} - {data_prep_config['description']}\")\n",
    "print(f\"GPU Acceleration: {'ENABLED' if use_gpu_data_prep else 'DISABLED'}\")\n",
    "print(f\"\\nPretraining: {pretrain_tokens_billions}B tokens, {pretrain_steps:,} steps\")\n",
    "print(f\"Expected time: ~{total_hours:.0f} hours ({total_hours/24:.1f} days)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Set Up Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    DRIVE_BASE = f\"/content/drive/MyDrive/llm-{model_size}-{model_type.replace('_', '-')}\"\n",
    "    print(f\"Setting up storage at: {DRIVE_BASE}\")\n",
    "    \n",
    "    # Create Drive directories\n",
    "    for subdir in ['checkpoints', 'data', 'data/raw', 'data/packed', 'data/sft', 'data/dpo', 'logs', 'evals']:\n",
    "        os.makedirs(os.path.join(DRIVE_BASE, subdir), exist_ok=True)\n",
    "    \n",
    "    # Create symlinks\n",
    "    for dir_name in ['checkpoints', 'data', 'logs', 'evals']:\n",
    "        local_path = os.path.join(PROJECT_ROOT, dir_name)\n",
    "        drive_path = os.path.join(DRIVE_BASE, dir_name)\n",
    "        \n",
    "        if os.path.exists(local_path) and not os.path.islink(local_path):\n",
    "            !cp -r {local_path}/* {drive_path}/ 2>/dev/null || true\n",
    "            !rm -rf {local_path}\n",
    "        elif os.path.islink(local_path):\n",
    "            os.unlink(local_path)\n",
    "        \n",
    "        os.symlink(drive_path, local_path)\n",
    "        print(f\"  {dir_name} -> Drive\")\n",
    "    \n",
    "    print(\"\\nStorage ready!\")\n",
    "else:\n",
    "    # Local Jupyter - create directories in project root\n",
    "    DRIVE_BASE = None\n",
    "    for d in ['checkpoints', 'data', 'data/raw', 'data/packed', 'data/sft', 'data/dpo', 'logs', 'evals']:\n",
    "        os.makedirs(os.path.join(PROJECT_ROOT, d), exist_ok=True)\n",
    "    print(\"Local directories created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5 Set Up Local Data Paths\n",
    "\n",
    "For Colab: Copies data from Google Drive to local NVMe SSD for 5-10x faster I/O during training.\n",
    "\n",
    "For local Jupyter: Uses local paths directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Local SSD paths (much faster than Drive)\n",
    "    LOCAL_DATA = \"/content/local_data\"\n",
    "    LOCAL_RAW = f\"{LOCAL_DATA}/raw\"\n",
    "    LOCAL_PROCESSED = f\"{LOCAL_DATA}/processed\"\n",
    "    LOCAL_PACKED = f\"{LOCAL_DATA}/packed\"\n",
    "    LOCAL_CACHE = \"/content/.gpu_cache\"\n",
    "    LOCAL_CHECKPOINTS = \"/content/local_checkpoints\"\n",
    "    \n",
    "    # Drive paths (persistent)\n",
    "    DRIVE_RAW = f\"{DRIVE_BASE}/data/raw\"\n",
    "    DRIVE_PACKED = f\"{DRIVE_BASE}/data/packed\"\n",
    "    DRIVE_CHECKPOINTS = f\"{DRIVE_BASE}/checkpoints\"\n",
    "    \n",
    "    print(\"Setting up local SSD storage for faster I/O...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create local directories\n",
    "    for d in [LOCAL_DATA, LOCAL_RAW, LOCAL_PROCESSED, LOCAL_PACKED, LOCAL_CACHE, LOCAL_CHECKPOINTS]:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "    \n",
    "    # Check if we have existing data on Drive to copy\n",
    "    drive_raw_files = []\n",
    "    if os.path.exists(DRIVE_RAW):\n",
    "        drive_raw_files = [f for f in os.listdir(DRIVE_RAW) if f.endswith('.parquet')]\n",
    "    \n",
    "    if drive_raw_files:\n",
    "        print(f\"\\nFound {len(drive_raw_files)} raw data files on Drive\")\n",
    "        print(\"Copying to local SSD for faster processing...\")\n",
    "        for f in drive_raw_files:\n",
    "            src = os.path.join(DRIVE_RAW, f)\n",
    "            dst = os.path.join(LOCAL_RAW, f)\n",
    "            if not os.path.exists(dst):\n",
    "                size_mb = os.path.getsize(src) / (1024*1024)\n",
    "                print(f\"  Copying {f} ({size_mb:.0f} MB)...\")\n",
    "                shutil.copy2(src, dst)\n",
    "            else:\n",
    "                print(f\"  {f} already on local SSD\")\n",
    "        print(\"Raw data copied!\")\n",
    "    else:\n",
    "        print(\"\\nNo existing raw data on Drive (will download fresh)\")\n",
    "    \n",
    "    # Check for existing packed data\n",
    "    drive_packed_files = []\n",
    "    if os.path.exists(DRIVE_PACKED):\n",
    "        drive_packed_files = list(os.listdir(DRIVE_PACKED))\n",
    "    \n",
    "    if drive_packed_files:\n",
    "        print(f\"\\nFound packed data on Drive - copying to local SSD...\")\n",
    "        !cp -r {DRIVE_PACKED}/* {LOCAL_PACKED}/ 2>/dev/null || true\n",
    "        print(\"Packed data copied!\")\n",
    "    \n",
    "    # Store paths in CONFIG for later use\n",
    "    CONFIG['local_raw'] = LOCAL_RAW\n",
    "    CONFIG['local_processed'] = LOCAL_PROCESSED\n",
    "    CONFIG['local_packed'] = LOCAL_PACKED\n",
    "    CONFIG['local_cache'] = LOCAL_CACHE\n",
    "    CONFIG['local_checkpoints'] = LOCAL_CHECKPOINTS\n",
    "    CONFIG['drive_raw'] = DRIVE_RAW\n",
    "    CONFIG['drive_packed'] = DRIVE_PACKED\n",
    "    CONFIG['drive_checkpoints'] = DRIVE_CHECKPOINTS\n",
    "    CONFIG['use_local_ssd'] = True\n",
    "    \n",
    "    # Check local disk space\n",
    "    import subprocess\n",
    "    result = subprocess.run(['df', '-h', '/content'], capture_output=True, text=True)\n",
    "    print(f\"\\nLocal SSD status:\")\n",
    "    print(result.stdout.split('\\n')[1])\n",
    "    \n",
    "    print(\"\\nLocal SSD paths configured:\")\n",
    "    print(f\"  Raw data: {LOCAL_RAW}\")\n",
    "    print(f\"  Processed: {LOCAL_PROCESSED}\")\n",
    "    print(f\"  Packed: {LOCAL_PACKED}\")\n",
    "    print(f\"  Cache: {LOCAL_CACHE}\")\n",
    "    print(\"=\" * 50)\n",
    "else:\n",
    "    # Local Jupyter - use project directories directly\n",
    "    CONFIG['local_raw'] = os.path.join(PROJECT_ROOT, 'data/raw')\n",
    "    CONFIG['local_processed'] = os.path.join(PROJECT_ROOT, 'data/processed')\n",
    "    CONFIG['local_packed'] = os.path.join(PROJECT_ROOT, 'data/packed')\n",
    "    CONFIG['local_cache'] = os.path.join(PROJECT_ROOT, '.gpu_cache')\n",
    "    CONFIG['local_checkpoints'] = os.path.join(PROJECT_ROOT, 'checkpoints')\n",
    "    CONFIG['use_local_ssd'] = False\n",
    "    \n",
    "    # Create directories\n",
    "    for d in [CONFIG['local_raw'], CONFIG['local_processed'], CONFIG['local_packed'], CONFIG['local_cache']]:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "    \n",
    "    print(\"Using local project directories:\")\n",
    "    print(f\"  Raw data: {CONFIG['local_raw']}\")\n",
    "    print(f\"  Processed: {CONFIG['local_processed']}\")\n",
    "    print(f\"  Packed: {CONFIG['local_packed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.6 Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    capability = torch.cuda.get_device_capability()\n",
    "    \n",
    "    CONFIG['use_fp8'] = capability[0] >= 9\n",
    "    \n",
    "    print(f\"GPU: {gpu_name} ({gpu_memory:.0f} GB)\")\n",
    "    print(f\"Compute Capability: {capability[0]}.{capability[1]}\")\n",
    "    print(f\"FP8: {'Available' if CONFIG['use_fp8'] else 'Not available'}\")\n",
    "    \n",
    "    # Memory check for model size\n",
    "    min_memory = {'125m': 8, '350m': 16, '1b': 24, '3b': 40, '7b': 70}\n",
    "    if gpu_memory < min_memory[CONFIG['model_size']]:\n",
    "        print(f\"\\nWARNING: {CONFIG['model_size']} may need {min_memory[CONFIG['model_size']]}+ GB\")\n",
    "else:\n",
    "    print(\"No GPU detected!\")\n",
    "    CONFIG['use_fp8'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Download ALL Data\n",
    "\n",
    "Downloads all datasets matched to your model type (pretraining + SFT + DPO)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Download Pretraining Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Downloading pretraining data for: {CONFIG['model_type']}\")\n",
    "print(f\"Datasets: {CONFIG['pretraining_datasets']}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "!python scripts/01_download_data.py --phases pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Download SFT & DPO Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Downloading SFT data for: {CONFIG['model_type']}\")\n",
    "print(f\"SFT datasets: {CONFIG['sft_datasets']}\")\n",
    "print(f\"DPO datasets: {CONFIG['dpo_datasets']}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Download based on model type\n",
    "if CONFIG['sft_focus'] == 'reasoning':\n",
    "    !python scripts/01_download_data.py --phases reasoning function_calling logic instruction_tuning preference_data\n",
    "elif CONFIG['sft_focus'] == 'code':\n",
    "    !python scripts/01_download_data.py --phases instruction_tuning preference_data\n",
    "    # Code data handled by prepare_lora_data.py\n",
    "else:\n",
    "    !python scripts/01_download_data.py --phases instruction_tuning preference_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Prepare Data\n",
    "\n",
    "Process and prepare all datasets for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Clean & Tokenize Pretraining Data\n",
    "\n",
    "GPU mode uses RAPIDS cuDF (150x faster text cleaning) + NeMo Curator (16x faster dedup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = CONFIG.get('use_gpu_data_prep', False)\n",
    "data_prep_speed = CONFIG['data_prep_speed']\n",
    "use_local_ssd = CONFIG.get('use_local_ssd', False)\n",
    "\n",
    "print(f\"Data prep mode: {data_prep_speed.upper()}\")\n",
    "print(f\"GPU Acceleration: {'ENABLED' if use_gpu else 'DISABLED'}\")\n",
    "print(f\"Local SSD: {'ENABLED (5-10x faster I/O)' if use_local_ssd else 'DISABLED'}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "local_raw = CONFIG['local_raw']\n",
    "local_processed = CONFIG['local_processed']\n",
    "local_packed = CONFIG['local_packed']\n",
    "local_cache = CONFIG['local_cache']\n",
    "\n",
    "if use_gpu:\n",
    "    # GPU-accelerated pipeline\n",
    "    gpu_flags = CONFIG['gpu_data_prep_flags']\n",
    "    print(f\"Using GPU pipeline...\")\n",
    "    print(f\"  Input: {local_raw}\")\n",
    "    print(f\"  Output: {local_processed}\")\n",
    "    !python scripts/02_gpu_clean_deduplicate.py \\\n",
    "        --input {local_raw} \\\n",
    "        --output {local_processed} \\\n",
    "        --cache {local_cache} \\\n",
    "        {gpu_flags}\n",
    "else:\n",
    "    # CPU pipeline\n",
    "    data_prep_flags = CONFIG['data_prep_flags']\n",
    "    print(f\"Using CPU pipeline with flags: {data_prep_flags}\")\n",
    "    !python scripts/02_clean_deduplicate_optimized.py \\\n",
    "        --input {local_raw} \\\n",
    "        --output {local_processed} \\\n",
    "        {data_prep_flags}\n",
    "\n",
    "# Tokenize and pack\n",
    "print(f\"\\nTokenizing and packing...\")\n",
    "!python scripts/03_tokenize_and_pack.py \\\n",
    "    --input-dir {local_processed} \\\n",
    "    --output-dir {local_packed}\n",
    "\n",
    "# Backup to Drive if using Colab\n",
    "if IN_COLAB and use_local_ssd:\n",
    "    drive_packed = CONFIG['drive_packed']\n",
    "    print(f\"\\nBacking up packed data to Drive...\")\n",
    "    !cp -r {local_packed}/* {drive_packed}/ 2>/dev/null || true\n",
    "    print(\"Backup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Prepare SFT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_script = CONFIG['data_prep_script']\n",
    "print(f\"Using: {prep_script} (optimized for {CONFIG['model_type']})\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "!python scripts/{prep_script}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Prepare DPO Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/08_prepare_dpo_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Initialize Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = CONFIG['model_size']\n",
    "context = CONFIG['context_length']\n",
    "\n",
    "print(f\"Initializing {model_size.upper()} model with {context} context...\")\n",
    "\n",
    "!python scripts/04_init_model.py --size {model_size} --context-length {context}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Verify Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checks = [\n",
    "    ('Model', 'checkpoints/init'),\n",
    "    ('Tokenizer', 'configs/tokenizer'),\n",
    "    ('Pretrain data', 'data/packed'),\n",
    "    ('SFT data', 'data/sft/train'),\n",
    "    ('DPO data', 'data/dpo/train'),\n",
    "]\n",
    "\n",
    "print(\"Setup verification:\")\n",
    "all_ok = True\n",
    "for name, path in checks:\n",
    "    full_path = os.path.join(PROJECT_ROOT, path)\n",
    "    ok = os.path.exists(full_path)\n",
    "    print(f\"  {name}: {'OK' if ok else 'MISSING'}\")\n",
    "    all_ok = all_ok and ok\n",
    "\n",
    "if all_ok:\n",
    "    print(\"\\nReady for training!\")\n",
    "else:\n",
    "    print(\"\\nFix missing items before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Pretraining\n",
    "\n",
    "Uses auto-optimized parameters based on model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = CONFIG['pretrain_steps']\n",
    "use_fp8 = CONFIG.get('use_fp8', False)\n",
    "use_local_ssd = CONFIG.get('use_local_ssd', False)\n",
    "\n",
    "# Build optimized command with size-specific parameters\n",
    "cmd = f\"python scripts/05_pretrain.py --max_steps {steps}\"\n",
    "\n",
    "# Local paths for data\n",
    "local_packed = CONFIG['local_packed']\n",
    "local_checkpoints = CONFIG['local_checkpoints']\n",
    "cmd += f\" --train_data_path {local_packed}\"\n",
    "cmd += f\" --output_dir {local_checkpoints}/pretrain\"\n",
    "\n",
    "# Always use Liger Kernel (compatible with all sizes)\n",
    "cmd += \" --use-liger-kernel\"\n",
    "\n",
    "# Size-specific optimizations\n",
    "cmd += f\" --per_device_train_batch_size {CONFIG['batch_size']}\"\n",
    "cmd += f\" --gradient_accumulation_steps {CONFIG['grad_accum']}\"\n",
    "cmd += f\" --learning_rate {CONFIG['learning_rate']}\"\n",
    "cmd += f\" --warmup_ratio {CONFIG['warmup_ratio']}\"\n",
    "cmd += f\" --dataloader_num_workers {CONFIG['dataloader_workers']}\"\n",
    "\n",
    "# torch.compile: Enable only for larger models where benefit > overhead\n",
    "if not CONFIG['use_torch_compile']:\n",
    "    cmd += \" --no-torch-compile\"\n",
    "\n",
    "# 8-bit optimizer: Enable for larger models to save memory\n",
    "if CONFIG['use_8bit_optim']:\n",
    "    cmd += \" --optim adamw_bnb_8bit\"\n",
    "else:\n",
    "    cmd += \" --optim adamw_torch_fused\"\n",
    "\n",
    "# FP8 for H100\n",
    "if use_fp8:\n",
    "    cmd += \" --fp8\"\n",
    "\n",
    "# OOM recovery (always useful)\n",
    "cmd += \" --enable-oom-recovery\"\n",
    "\n",
    "print(f\"Model: {CONFIG['model_size'].upper()} {CONFIG['model_type']}\")\n",
    "print(f\"Steps: {steps:,}\")\n",
    "print(f\"\\nOptimized Parameters:\")\n",
    "print(f\"  Batch: {CONFIG['batch_size']} x {CONFIG['grad_accum']} = {CONFIG['effective_batch_size']}\")\n",
    "print(f\"  Learning rate: {CONFIG['learning_rate']:.0e}\")\n",
    "print(f\"  torch.compile: {'ON' if CONFIG['use_torch_compile'] else 'OFF'}\")\n",
    "print(f\"  8-bit optimizer: {'ON' if CONFIG['use_8bit_optim'] else 'OFF'}\")\n",
    "print(f\"\\nData: {CONFIG['local_packed']}\")\n",
    "print(f\"Checkpoints: {CONFIG['local_checkpoints']}/pretrain\")\n",
    "print(f\"\\nCommand: {cmd}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "!{cmd}\n",
    "\n",
    "# Backup checkpoints to Drive after training (Colab only)\n",
    "if IN_COLAB and use_local_ssd:\n",
    "    drive_checkpoints = CONFIG['drive_checkpoints']\n",
    "    print(f\"\\nBacking up checkpoints to Drive...\")\n",
    "    !cp -r {local_checkpoints}/pretrain/* {drive_checkpoints}/pretrain/ 2>/dev/null || true\n",
    "    print(\"Checkpoint backup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: SFT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = \"python scripts/07_sft.py --use-liger-kernel --enable-oom-recovery\"\n",
    "if CONFIG.get('use_fp8', False):\n",
    "    cmd += \" --fp8\"\n",
    "\n",
    "print(f\"SFT focus: {CONFIG['sft_focus']}\")\n",
    "print(f\"Datasets: {CONFIG['sft_datasets']}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: DPO Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = \"python scripts/09_dpo.py --enable-oom-recovery\"\n",
    "if CONFIG.get('use_fp8', False):\n",
    "    cmd += \" --fp8\"\n",
    "\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Benchmarks for {CONFIG['model_type']}: {CONFIG['eval_benchmarks']}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "!python scripts/11_evaluate.py checkpoints/dpo_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Check Gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/12_check_gates.py dpo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "MODEL_PATH = \"checkpoints/dpo_final\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"configs/tokenizer\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Generate Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_tokens=512):\n",
    "    formatted = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n\"\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_tokens, \n",
    "                                 temperature=0.7, do_sample=True, top_p=0.9)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test prompts based on model type\n",
    "TEST_PROMPTS = {\n",
    "    'reasoning_agent': \"Solve step by step: If a train travels 60 mph for 2.5 hours, how far does it go?\",\n",
    "    'code_assistant': \"Write a Python function to find the nth Fibonacci number.\",\n",
    "    'general_assistant': \"Explain the difference between machine learning and deep learning.\",\n",
    "    'chat_model': \"Hello! How are you today? What can you help me with?\",\n",
    "}\n",
    "\n",
    "prompt = TEST_PROMPTS.get(CONFIG['model_type'], \"Hello, how are you?\")\n",
    "print(f\"Test for {CONFIG['model_type']}:\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"\\nResponse:\")\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Custom Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your custom prompt here\n",
    "CUSTOM_PROMPT = \"Your prompt here\"\n",
    "\n",
    "print(generate(CUSTOM_PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Complete!\n",
    "\n",
    "Your model is trained and ready to use. Check the evaluation results above to see how it performs on relevant benchmarks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
