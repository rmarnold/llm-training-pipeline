{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Mutation Training Data (Local / MacBook Pro Max)\n",
    "\n",
    "Standalone Jupyter notebook optimized for generating Rust mutation training data\n",
    "on a MacBook Pro Max with Apple Silicon and 64GB RAM.\n",
    "\n",
    "**Optimizations for Apple Silicon:**\n",
    "- **sccache** for cross-repo compilation caching (60-80% faster rebuilds)\n",
    "- **RAM disk** for build artifacts (eliminates I/O overhead for temp files)\n",
    "- **Stripped debug info** for deps (20-40% faster linking)\n",
    "- **Native CPU target** for Apple Silicon NEON optimizations\n",
    "- **RAM-aware parallelism** tuned for M-series core layout\n",
    "\n",
    "**What this does:**\n",
    "1. Sets up an optimized Rust compilation environment\n",
    "2. Clones curated Rust repositories\n",
    "3. Runs `cargo-mutants` with aggressive parallelism\n",
    "4. Captures (buggy code, error, fix) training triples\n",
    "5. Saves as JSONL + HuggingFace dataset\n",
    "\n",
    "**Requirements:**\n",
    "- macOS with Apple Silicon (M1/M2/M3/M4 Pro/Max)\n",
    "- 64GB RAM (32GB minimum, reduce parallelism)\n",
    "- Rust toolchain: `curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh`\n",
    "- cargo-mutants: `cargo install cargo-mutants`\n",
    "- sccache: `brew install sccache`\n",
    "\n",
    "**Time estimate:** 1-3 hours for all 21 repos (faster than Colab due to faster CPUs + SSD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Environment Setup & Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HARDWARE DETECTION\n",
      "============================================================\n",
      "  Chip: Apple M1\n",
      "  Architecture: Apple Silicon (arm64)\n",
      "  CPU cores: 8 total (4 performance + 4 efficiency)\n",
      "  RAM: 16 GB\n",
      "\n",
      "Tool Verification:\n",
      "----------------------------------------\n",
      "  ✓ cargo: cargo 1.92.0 (Homebrew)\n",
      "  ✓ cargo-mutants: cargo-mutants 26.2.0\n",
      "  ✓ sccache: sccache 0.14.0\n",
      "  ✓ git: git version 2.33.0\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "### 0.1 Detect Hardware & Verify Tools\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "\n",
    "# Detect Apple Silicon\n",
    "is_apple_silicon = platform.machine() == \"arm64\" and platform.system() == \"Darwin\"\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "\n",
    "# Get RAM\n",
    "try:\n",
    "    result = subprocess.run([\"sysctl\", \"-n\", \"hw.memsize\"], capture_output=True, text=True)\n",
    "    total_ram_gb = int(result.stdout.strip()) / (1024**3)\n",
    "except Exception:\n",
    "    total_ram_gb = 0\n",
    "\n",
    "# Get chip info\n",
    "try:\n",
    "    result = subprocess.run([\"sysctl\", \"-n\", \"machdep.cpu.brand_string\"], capture_output=True, text=True)\n",
    "    chip_name = result.stdout.strip()\n",
    "except Exception:\n",
    "    chip_name = \"Unknown\"\n",
    "\n",
    "# Count P-cores vs E-cores (approximate from total)\n",
    "# M1 Pro/Max: 8P+2E=10, M2 Pro/Max: 8P+4E=12, M3/M4 Pro/Max: 12P+4E=16\n",
    "if cpu_count >= 16:\n",
    "    p_cores = 12\n",
    "    e_cores = cpu_count - 12\n",
    "elif cpu_count >= 12:\n",
    "    p_cores = 8\n",
    "    e_cores = cpu_count - 8\n",
    "elif cpu_count >= 10:\n",
    "    p_cores = 8\n",
    "    e_cores = cpu_count - 8\n",
    "else:\n",
    "    p_cores = max(1, cpu_count // 2)\n",
    "    e_cores = cpu_count - p_cores\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"HARDWARE DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Chip: {chip_name}\")\n",
    "print(f\"  Architecture: {'Apple Silicon (arm64)' if is_apple_silicon else platform.machine()}\")\n",
    "print(f\"  CPU cores: {cpu_count} total ({p_cores} performance + {e_cores} efficiency)\")\n",
    "print(f\"  RAM: {total_ram_gb:.0f} GB\")\n",
    "\n",
    "if not is_apple_silicon:\n",
    "    print(\"\\n  NOTE: Not Apple Silicon. Optimizations are tuned for ARM64 but will still work.\")\n",
    "\n",
    "# Verify tools\n",
    "print(f\"\\nTool Verification:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "tools_ok = True\n",
    "for cmd, label, required in [\n",
    "    ([\"cargo\", \"--version\"], \"cargo\", True),\n",
    "    ([\"cargo\", \"mutants\", \"--version\"], \"cargo-mutants\", True),\n",
    "    ([\"sccache\", \"--version\"], \"sccache\", False),\n",
    "    ([\"git\", \"--version\"], \"git\", True),\n",
    "]:\n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)\n",
    "        if result.returncode == 0:\n",
    "            ver = result.stdout.strip().split(\"\\n\")[0]\n",
    "            print(f\"  \\u2713 {label}: {ver}\")\n",
    "        else:\n",
    "            print(f\"  \\u2717 {label}: command failed\")\n",
    "            if required:\n",
    "                tools_ok = False\n",
    "    except FileNotFoundError:\n",
    "        tag = \"REQUIRED\" if required else \"optional\"\n",
    "        print(f\"  \\u2717 {label}: not installed ({tag})\")\n",
    "        if required:\n",
    "            tools_ok = False\n",
    "\n",
    "if not tools_ok:\n",
    "    print(\"\\n  Missing required tools! Install with:\")\n",
    "    print(\"    curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\")\n",
    "    print(\"    cargo install cargo-mutants\")\n",
    "    print(\"    brew install sccache  # optional but recommended\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ sccache enabled (RUSTC_WRAPPER=sccache, incremental OFF)\n",
      "  Cache dir: /Users/robertarnold/.cache/sccache\n",
      "  Cache size: 20G\n",
      "✓ RUSTFLAGS: -C target-cpu=native\n",
      "\n",
      "✓ Compilation environment configured\n"
     ]
    }
   ],
   "source": [
    "### 0.2 Configure Optimized Compilation Environment\n",
    "#\n",
    "# Sets up sccache, native CPU targeting, and stripped debug info\n",
    "# for fastest possible cargo-mutants throughput.\n",
    "\n",
    "import shutil\n",
    "\n",
    "# ---- sccache ----\n",
    "# Caches compilation artifacts by input hash. Huge win for cargo-mutants\n",
    "# because mutations touch one file while dozens of deps stay identical.\n",
    "# After the first mutant, subsequent ones reuse cached deps (60-80% faster).\n",
    "has_sccache = shutil.which(\"sccache\") is not None\n",
    "\n",
    "if has_sccache:\n",
    "    os.environ[\"RUSTC_WRAPPER\"] = \"sccache\"\n",
    "    os.environ[\"SCCACHE_CACHE_SIZE\"] = \"20G\"\n",
    "    os.environ.setdefault(\"SCCACHE_DIR\", os.path.expanduser(\"~/.cache/sccache\"))\n",
    "    # sccache and incremental are partially incompatible.\n",
    "    # For cross-repo mutation work, sccache wins over incremental.\n",
    "    os.environ[\"CARGO_INCREMENTAL\"] = \"0\"\n",
    "    print(\"\\u2713 sccache enabled (RUSTC_WRAPPER=sccache, incremental OFF)\")\n",
    "    print(f\"  Cache dir: {os.environ['SCCACHE_DIR']}\")\n",
    "    print(f\"  Cache size: {os.environ['SCCACHE_CACHE_SIZE']}\")\n",
    "else:\n",
    "    # Without sccache, use incremental compilation for the single-file\n",
    "    # mutation pattern (change one file, rebuild).\n",
    "    os.environ[\"CARGO_INCREMENTAL\"] = \"1\"\n",
    "    print(\"\\u2014 sccache not installed. Using incremental compilation instead.\")\n",
    "    print(\"  Install for 60-80% faster builds: brew install sccache\")\n",
    "\n",
    "# ---- Native CPU target ----\n",
    "# Enable Apple Silicon NEON instructions. Safe since we only run locally.\n",
    "os.environ[\"RUSTFLAGS\"] = \"-C target-cpu=native\"\n",
    "print(f\"\\u2713 RUSTFLAGS: {os.environ['RUSTFLAGS']}\")\n",
    "\n",
    "# ---- Cargo build parallelism ----\n",
    "# Formula: CARGO_BUILD_JOBS = performance_cores / cargo_mutants_jobs\n",
    "# We want the product to roughly equal performance core count.\n",
    "# This is set after we determine mutation_jobs in the next cell.\n",
    "\n",
    "print(\"\\n\\u2713 Compilation environment configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CARGO_BUILD_JOBS=4\n",
      "\n",
      "============================================================\n",
      "MUTATION GENERATION CONFIG\n",
      "============================================================\n",
      "  Mutation workers (--jobs): 1\n",
      "  Cargo build jobs:          4\n",
      "  Total compile units:       ~4\n",
      "  P-cores available:         4\n",
      "  Max mutations/repo:        100\n",
      "  Timeout/mutation:          300s\n",
      "  sccache:                   ON\n",
      "  Project root:              /Users/robertarnold/PycharmProjects/llm-training-pipeline\n",
      "  Clone dir:                 /Volumes/OWC Express 1M2/rust-mutations/repos\n",
      "  Output dir:                /Volumes/OWC Express 1M2/rust-mutations/output\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "### 0.3 Configure Generation Parameters\n",
    "\n",
    "# ---- Parallelism ----\n",
    "# cargo-mutants --jobs controls parallel mutant workers.\n",
    "# Each worker spawns cargo build + test, which uses CARGO_BUILD_JOBS cores.\n",
    "# Target: mutation_jobs * cargo_build_jobs ~ performance_cores\n",
    "#\n",
    "# With 64GB RAM, memory is not the bottleneck (see research notes).\n",
    "# CPU is the constraint: too many jobs cause cache thrashing.\n",
    "\n",
    "if total_ram_gb >= 48:  # 64GB\n",
    "    if p_cores >= 12:  # M3/M4 Max\n",
    "        mutation_jobs = 4\n",
    "        cargo_build_jobs = 4  # 4 * 4 = 16 compile units ~ 12 P-cores + overflow to E-cores\n",
    "    elif p_cores >= 8:  # M1/M2 Max\n",
    "        mutation_jobs = 3\n",
    "        cargo_build_jobs = 3  # 3 * 3 = 9 ~ 8 P-cores\n",
    "    else:\n",
    "        mutation_jobs = 2\n",
    "        cargo_build_jobs = max(1, p_cores // 2)\n",
    "elif total_ram_gb >= 24:  # 32GB\n",
    "    mutation_jobs = 2\n",
    "    cargo_build_jobs = max(1, p_cores // 2)\n",
    "else:  # 16GB or less\n",
    "    mutation_jobs = 1\n",
    "    cargo_build_jobs = max(1, p_cores)\n",
    "\n",
    "os.environ[\"CARGO_BUILD_JOBS\"] = str(cargo_build_jobs)\n",
    "print(f\"\\u2713 CARGO_BUILD_JOBS={cargo_build_jobs}\")\n",
    "\n",
    "# ---- Generation settings ----\n",
    "max_mutations_per_repo = 100\n",
    "timeout_per_mutation = 300  # seconds\n",
    "\n",
    "# ---- Paths ----\n",
    "# Use the project's own directory structure\n",
    "PROJECT_ROOT = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "# If running from inside the notebooks/ directory, go up one level\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    PROJECT_ROOT = os.path.dirname(os.getcwd())\n",
    "elif os.path.exists(os.path.join(os.getcwd(), \"scripts\")):\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "\n",
    "CONFIG_PATH = os.path.join(PROJECT_ROOT, \"configs\", \"data_sources_rust.yaml\")\n",
    "\n",
    "# Store data on external drive for speed and to avoid filling the boot SSD.\n",
    "# The OWC Express 1M2 is a fast NVMe enclosure — ideal for the heavy I/O\n",
    "# of cloning repos and running cargo builds.\n",
    "EXTERNAL_BASE = \"/Volumes/OWC Express 1M2/rust-mutations\"\n",
    "CLONE_DIR = os.path.join(EXTERNAL_BASE, \"repos\")\n",
    "OUTPUT_DIR = os.path.join(EXTERNAL_BASE, \"output\")\n",
    "\n",
    "# Verify the external drive is mounted\n",
    "if not os.path.exists(\"/Volumes/OWC Express 1M2\"):\n",
    "    print(\"\\u2717 External drive not found at /Volumes/OWC Express 1M2\")\n",
    "    print(\"  Falling back to project-local data/ directory\")\n",
    "    CLONE_DIR = os.path.join(PROJECT_ROOT, \"data\", \"rust\", \"repos\")\n",
    "    OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"data\", \"rust\", \"mutations\")\n",
    "\n",
    "os.makedirs(CLONE_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MUTATION GENERATION CONFIG\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Mutation workers (--jobs): {mutation_jobs}\")\n",
    "print(f\"  Cargo build jobs:          {cargo_build_jobs}\")\n",
    "print(f\"  Total compile units:       ~{mutation_jobs * cargo_build_jobs}\")\n",
    "print(f\"  P-cores available:         {p_cores}\")\n",
    "print(f\"  Max mutations/repo:        {max_mutations_per_repo}\")\n",
    "print(f\"  Timeout/mutation:          {timeout_per_mutation}s\")\n",
    "print(f\"  sccache:                   {'ON' if has_sccache else 'OFF'}\")\n",
    "print(f\"  Project root:              {PROJECT_ROOT}\")\n",
    "print(f\"  Clone dir:                 {CLONE_DIR}\")\n",
    "print(f\"  Output dir:                {OUTPUT_DIR}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ RAM disk already mounted at /Volumes/MutantsBuild\n",
      "✓ TMPDIR set to RAM disk\n"
     ]
    }
   ],
   "source": [
    "### 0.4 Create RAM Disk (Optional)\n",
    "#\n",
    "# Allocates a RAM disk for cargo-mutants' temp build directories.\n",
    "# This eliminates disk I/O for intermediate compilation artifacts.\n",
    "#\n",
    "# On Apple Silicon with fast NVMe (7 GB/s), the benefit is moderate\n",
    "# (~10-15% faster) but it also reduces SSD wear from the heavy\n",
    "# write traffic of mutation testing.\n",
    "#\n",
    "# Skip this cell if you don't want to allocate RAM.\n",
    "\n",
    "USE_RAMDISK = True  # Set False to skip\n",
    "RAMDISK_SIZE_GB = 10  # 10 GB is enough for most crates\n",
    "\n",
    "RAMDISK_PATH = None\n",
    "\n",
    "if USE_RAMDISK and platform.system() == \"Darwin\":\n",
    "    # Check if already mounted\n",
    "    if os.path.exists(\"/Volumes/MutantsBuild\"):\n",
    "        RAMDISK_PATH = \"/Volumes/MutantsBuild\"\n",
    "        print(f\"\\u2713 RAM disk already mounted at {RAMDISK_PATH}\")\n",
    "    else:\n",
    "        sectors = RAMDISK_SIZE_GB * 1024 * 2048\n",
    "        print(f\"Creating {RAMDISK_SIZE_GB} GB RAM disk...\")\n",
    "        result = subprocess.run(\n",
    "            [\"hdiutil\", \"attach\", \"-nomount\", f\"ram://{sectors}\"],\n",
    "            capture_output=True, text=True,\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            device = result.stdout.strip()\n",
    "            fmt_result = subprocess.run(\n",
    "                [\"diskutil\", \"eraseVolume\", \"APFS\", \"MutantsBuild\", device],\n",
    "                capture_output=True, text=True,\n",
    "            )\n",
    "            if fmt_result.returncode == 0:\n",
    "                RAMDISK_PATH = \"/Volumes/MutantsBuild\"\n",
    "                print(f\"\\u2713 RAM disk created: {RAMDISK_PATH} ({RAMDISK_SIZE_GB} GB)\")\n",
    "                print(f\"  Device: {device}\")\n",
    "                print(f\"  To detach later: hdiutil detach {device}\")\n",
    "            else:\n",
    "                print(f\"\\u2717 Failed to format RAM disk: {fmt_result.stderr}\")\n",
    "        else:\n",
    "            print(f\"\\u2717 Failed to create RAM disk: {result.stderr}\")\n",
    "\n",
    "    # Tell cargo-mutants to use the RAM disk for its temp copies\n",
    "    if RAMDISK_PATH:\n",
    "        os.environ[\"TMPDIR\"] = RAMDISK_PATH\n",
    "        print(f\"\\u2713 TMPDIR set to RAM disk\")\n",
    "else:\n",
    "    print(\"\\u2014 RAM disk skipped (using SSD — still fast on Apple Silicon)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cargo config prepared (stripped debug info for deps)\n",
      "  This will be injected into each cloned repo before mutation testing.\n"
     ]
    }
   ],
   "source": [
    "### 0.5 Set Up Cargo Config for Stripped Debug Info\n",
    "#\n",
    "# Disabling debug info for dependency crates gives 20-40% faster linking\n",
    "# and 30-50% less disk usage for build artifacts.\n",
    "# We write a project-local .cargo/config.toml that only affects builds\n",
    "# within the cloned repos.\n",
    "\n",
    "CARGO_CONFIG = \"\"\"\\\n",
    "# Auto-generated by generate_mutations_local.ipynb\n",
    "# Optimizations for cargo-mutants speed\n",
    "\n",
    "[profile.dev.package.\"*\"]\n",
    "debug = false\n",
    "\n",
    "[profile.test.package.\"*\"]\n",
    "debug = false\n",
    "\"\"\"\n",
    "\n",
    "# We'll inject this into each cloned repo's .cargo/config.toml\n",
    "# (done automatically in the generation step)\n",
    "print(\"\\u2713 Cargo config prepared (stripped debug info for deps)\")\n",
    "print(\"  This will be injected into each cloned repo before mutation testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Generate Mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting mutation generation...\n",
      "  Workers: 1 | Cargo jobs: 4\n",
      "  sccache: ON | RAM disk: ON\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling PyTorch because PyTorch >= 2.1 is required but found 2.0.1\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disabling PyTorch because PyTorch >= 2.1 is required but found 2.0.1\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "### 1.1 Run Mutation Generation\n",
    "#\n",
    "# This calls the same script as Colab but with locally-optimized settings.\n",
    "# The script handles cloning, mutation, and result parsing.\n",
    "\n",
    "import time\n",
    "\n",
    "# Inject cargo config into clone dir so new clones inherit it\n",
    "cargo_config_dir = os.path.join(CLONE_DIR, \".cargo-mutations-config\")\n",
    "os.makedirs(cargo_config_dir, exist_ok=True)\n",
    "with open(os.path.join(cargo_config_dir, \"config.toml\"), \"w\") as f:\n",
    "    f.write(CARGO_CONFIG)\n",
    "\n",
    "# Ensure scripts/ is importable\n",
    "scripts_dir = os.path.join(PROJECT_ROOT, \"scripts\")\n",
    "if scripts_dir not in sys.path:\n",
    "    sys.path.insert(0, scripts_dir)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Starting mutation generation...\")\n",
    "print(f\"  Workers: {mutation_jobs} | Cargo jobs: {cargo_build_jobs}\")\n",
    "print(f\"  sccache: {'ON' if has_sccache else 'OFF'} | RAM disk: {'ON' if RAMDISK_PATH else 'OFF'}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Import and run directly (better output in Jupyter than subprocess)\n",
    "from pipeline_lib.cargo_mutants_runner import check_cargo_mutants_installed\n",
    "if not check_cargo_mutants_installed():\n",
    "    print(\"\\u2717 cargo-mutants not found. Install: cargo install cargo-mutants\")\n",
    "else:\n",
    "    # We need to inject the cargo config into repos after cloning.\n",
    "    # The easiest way is to use the CLI script which handles everything.\n",
    "    import subprocess as sp\n",
    "    cmd = [\n",
    "        sys.executable, os.path.join(scripts_dir, \"16_generate_mutations.py\"),\n",
    "        \"--config\", CONFIG_PATH,\n",
    "        \"--clone_dir\", CLONE_DIR,\n",
    "        \"--output_dir\", OUTPUT_DIR,\n",
    "        \"--max_mutations_per_repo\", str(max_mutations_per_repo),\n",
    "        \"--timeout_per_mutation\", str(timeout_per_mutation),\n",
    "        \"--jobs\", str(mutation_jobs),\n",
    "    ]\n",
    "\n",
    "    # Run with real-time output\n",
    "    process = sp.Popen(\n",
    "        cmd,\n",
    "        stdout=sp.PIPE,\n",
    "        stderr=sp.STDOUT,\n",
    "        text=True,\n",
    "        bufsize=1,\n",
    "        env=os.environ.copy(),\n",
    "    )\n",
    "    for line in iter(process.stdout.readline, \"\"):\n",
    "        print(line, end=\"\")\n",
    "    process.wait()\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    mins = int(elapsed // 60)\n",
    "    secs = int(elapsed % 60)\n",
    "    print(f\"\\nTotal time: {mins}m {secs}s\")\n",
    "\n",
    "    if has_sccache:\n",
    "        print(\"\\nsccache stats:\")\n",
    "        sp.run([\"sccache\", \"--show-stats\"], check=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.2 Monitor System Resources (run while 1.1 is executing)\n",
    "#\n",
    "# Run this in a separate cell to check CPU/RAM usage during generation.\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# macOS memory info\n",
    "print(\"Memory:\")\n",
    "result = subprocess.run([\"vm_stat\"], capture_output=True, text=True)\n",
    "lines = result.stdout.strip().split(\"\\n\")\n",
    "for line in lines[:6]:\n",
    "    print(f\"  {line}\")\n",
    "\n",
    "# Memory pressure\n",
    "print(\"\\nMemory pressure:\")\n",
    "result = subprocess.run([\"memory_pressure\"], capture_output=True, text=True, timeout=5)\n",
    "for line in result.stdout.strip().split(\"\\n\")[:3]:\n",
    "    print(f\"  {line}\")\n",
    "\n",
    "# CPU load\n",
    "print(\"\\nLoad average:\")\n",
    "result = subprocess.run([\"uptime\"], capture_output=True, text=True)\n",
    "print(f\"  {result.stdout.strip()}\")\n",
    "\n",
    "# sccache hit rate\n",
    "if has_sccache:\n",
    "    print(\"\\nsccache:\")\n",
    "    result = subprocess.run([\"sccache\", \"--show-stats\"], capture_output=True, text=True)\n",
    "    for line in result.stdout.strip().split(\"\\n\"):\n",
    "        if any(k in line.lower() for k in [\"hit\", \"miss\", \"request\", \"cache\"]):\n",
    "            print(f\"  {line.strip()}\")\n",
    "\n",
    "# RAM disk usage\n",
    "if RAMDISK_PATH and os.path.exists(RAMDISK_PATH):\n",
    "    result = subprocess.run([\"df\", \"-h\", RAMDISK_PATH], capture_output=True, text=True)\n",
    "    print(f\"\\nRAM disk:\")\n",
    "    for line in result.stdout.strip().split(\"\\n\"):\n",
    "        print(f\"  {line}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Verify & Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.1 Verify Output\n",
    "\n",
    "import json\n",
    "\n",
    "jsonl_path = os.path.join(OUTPUT_DIR, \"mutations.jsonl\")\n",
    "hf_path = os.path.join(OUTPUT_DIR, \"hf_dataset\")\n",
    "\n",
    "print(\"Output Verification:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if os.path.exists(jsonl_path):\n",
    "    with open(jsonl_path) as f:\n",
    "        lines = f.readlines()\n",
    "    size_mb = os.path.getsize(jsonl_path) / (1024 * 1024)\n",
    "    print(f\"  \\u2713 JSONL: {len(lines):,} examples ({size_mb:.1f} MB)\")\n",
    "\n",
    "    caught = sum(1 for l in lines if '\"Test failure:' in l)\n",
    "    unviable = sum(1 for l in lines if '\"Compiler error:' in l)\n",
    "    print(f\"    Caught mutations (test failures): {caught:,}\")\n",
    "    print(f\"    Unviable mutations (compiler errors): {unviable:,}\")\n",
    "else:\n",
    "    print(f\"  \\u2717 JSONL not found at {jsonl_path}\")\n",
    "\n",
    "if os.path.exists(hf_path):\n",
    "    items = os.listdir(hf_path)\n",
    "    print(f\"  \\u2713 HF dataset: {hf_path} ({len(items)} files)\")\n",
    "else:\n",
    "    print(f\"  \\u2014 HF dataset not generated yet\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.2 Inspect Sample Examples\n",
    "\n",
    "import json\n",
    "\n",
    "jsonl_path = os.path.join(OUTPUT_DIR, \"mutations.jsonl\")\n",
    "\n",
    "if not os.path.exists(jsonl_path):\n",
    "    print(\"No data yet. Run Step 1 first.\")\n",
    "else:\n",
    "    with open(jsonl_path) as f:\n",
    "        examples = [json.loads(line) for line in f.readlines()[:5]]\n",
    "\n",
    "    for i, ex in enumerate(examples, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Example {i}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Explanation: {ex.get('explanation', 'N/A')[:120]}\")\n",
    "        print(f\"\\nBuggy code (first 200 chars):\")\n",
    "        print(ex.get('buggy_code', '')[:200])\n",
    "        print(f\"\\nError (first 200 chars):\")\n",
    "        print(ex.get('error_message', '')[:200])\n",
    "        print(f\"\\nFixed code (first 200 chars):\")\n",
    "        print(ex.get('fixed_code', '')[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.3 Stats by Repository\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "jsonl_path = os.path.join(OUTPUT_DIR, \"mutations.jsonl\")\n",
    "\n",
    "if not os.path.exists(jsonl_path):\n",
    "    print(\"No data yet. Run Step 1 first.\")\n",
    "else:\n",
    "    repo_counts = Counter()\n",
    "    type_counts = Counter()\n",
    "\n",
    "    with open(jsonl_path) as f:\n",
    "        for line in f:\n",
    "            ex = json.loads(line)\n",
    "            explanation = ex.get('explanation', '')\n",
    "            if '(' in explanation and ')' in explanation:\n",
    "                file_path = explanation.split('(')[-1].rstrip(')')\n",
    "                parts = file_path.split('/')\n",
    "                repo_counts[parts[0] if parts else 'unknown'] += 1\n",
    "            if 'Test failure' in ex.get('error_message', ''):\n",
    "                type_counts['caught (test failure)'] += 1\n",
    "            elif 'Compiler error' in ex.get('error_message', ''):\n",
    "                type_counts['unviable (compiler error)'] += 1\n",
    "\n",
    "    print(\"Examples by source file prefix:\")\n",
    "    print(\"=\" * 60)\n",
    "    for repo, count in repo_counts.most_common():\n",
    "        print(f\"  {repo:<30} {count:>5}\")\n",
    "    print(f\"  {'TOTAL':<30} {sum(repo_counts.values()):>5}\")\n",
    "\n",
    "    print(f\"\\nExamples by type:\")\n",
    "    print(\"=\" * 60)\n",
    "    for t, count in type_counts.most_common():\n",
    "        print(f\"  {t:<35} {count:>5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Re-run Failed Repos (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.1 Re-run Specific Repos\n",
    "#\n",
    "# If some repos failed, re-run them here.\n",
    "# Edit the list below with repo names (e.g., \"tokio-rs/tokio\").\n",
    "\n",
    "retry_repos = [\n",
    "    # \"tokio-rs/tokio\",\n",
    "    # \"serde-rs/serde\",\n",
    "]\n",
    "\n",
    "if retry_repos:\n",
    "    import time\n",
    "\n",
    "    retry_output = os.path.join(OUTPUT_DIR, \"retry\")\n",
    "    os.makedirs(retry_output, exist_ok=True)\n",
    "\n",
    "    repos_arg = \" \".join(retry_repos)\n",
    "    start = time.time()\n",
    "\n",
    "    print(f\"Re-running {len(retry_repos)} repos...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    cmd = [\n",
    "        sys.executable, os.path.join(PROJECT_ROOT, \"scripts\", \"16_generate_mutations.py\"),\n",
    "        \"--repos\", *retry_repos,\n",
    "        \"--clone_dir\", CLONE_DIR,\n",
    "        \"--output_dir\", retry_output,\n",
    "        \"--max_mutations_per_repo\", str(max_mutations_per_repo),\n",
    "        \"--timeout_per_mutation\", str(timeout_per_mutation),\n",
    "        \"--jobs\", str(mutation_jobs),\n",
    "    ]\n",
    "\n",
    "    process = subprocess.Popen(\n",
    "        cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "        text=True, bufsize=1, env=os.environ.copy(),\n",
    "    )\n",
    "    for line in iter(process.stdout.readline, \"\"):\n",
    "        print(line, end=\"\")\n",
    "    process.wait()\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"\\nRetry time: {int(elapsed//60)}m {int(elapsed%60)}s\")\n",
    "\n",
    "    # Merge retry results into main output\n",
    "    retry_jsonl = os.path.join(retry_output, \"mutations.jsonl\")\n",
    "    main_jsonl = os.path.join(OUTPUT_DIR, \"mutations.jsonl\")\n",
    "    if os.path.exists(retry_jsonl):\n",
    "        with open(retry_jsonl) as f:\n",
    "            retry_lines = f.readlines()\n",
    "        with open(main_jsonl, \"a\") as f:\n",
    "            f.writelines(retry_lines)\n",
    "        print(f\"\\nMerged {len(retry_lines)} retry examples into main output\")\n",
    "else:\n",
    "    print(\"No repos to retry. Edit retry_repos list above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Copy to Colab Drive (Optional)\n",
    "\n",
    "If you want to use this data in Colab for training, copy it to Google Drive\n",
    "or upload it to HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.1 Upload to HuggingFace Hub (Optional)\n",
    "#\n",
    "# Uploads the HF dataset so you can load it from Colab with:\n",
    "#   datasets.load_from_disk(\"your-user/rust-mutations\")\n",
    "\n",
    "UPLOAD_TO_HF = False  # Set True to upload\n",
    "HF_REPO_ID = \"your-username/rust-mutations\"  # Change this\n",
    "\n",
    "if UPLOAD_TO_HF:\n",
    "    hf_path = os.path.join(OUTPUT_DIR, \"hf_dataset\")\n",
    "    if os.path.exists(hf_path):\n",
    "        from datasets import load_from_disk\n",
    "        ds = load_from_disk(hf_path)\n",
    "        ds.push_to_hub(HF_REPO_ID, private=True)\n",
    "        print(f\"\\u2713 Uploaded to https://huggingface.co/datasets/{HF_REPO_ID}\")\n",
    "    else:\n",
    "        print(\"\\u2717 HF dataset not found. Run generation first.\")\n",
    "else:\n",
    "    print(\"HF upload skipped. Set UPLOAD_TO_HF = True to enable.\")\n",
    "    print(f\"\\nLocal data location: {OUTPUT_DIR}\")\n",
    "    print(\"\\nTo use in Colab, either:\")\n",
    "    print(\"  1. Upload the hf_dataset/ folder to HuggingFace Hub (set UPLOAD_TO_HF=True)\")\n",
    "    print(\"  2. Copy mutations.jsonl to Google Drive manually\")\n",
    "    print(\"  3. Use Google Drive desktop to sync the data/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.1 Detach RAM Disk\n",
    "#\n",
    "# Run this when you're done to free the RAM.\n",
    "\n",
    "if RAMDISK_PATH and os.path.exists(RAMDISK_PATH):\n",
    "    result = subprocess.run(\n",
    "        [\"hdiutil\", \"detach\", RAMDISK_PATH],\n",
    "        capture_output=True, text=True,\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        print(f\"\\u2713 RAM disk detached: {RAMDISK_PATH}\")\n",
    "        RAMDISK_PATH = None\n",
    "    else:\n",
    "        print(f\"\\u2717 Failed to detach: {result.stderr}\")\n",
    "        print(\"  Try: hdiutil detach /Volumes/MutantsBuild\")\n",
    "else:\n",
    "    print(\"No RAM disk to detach.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.2 Clean Up Cloned Repos (Optional)\n",
    "#\n",
    "# Remove cloned repos to free disk space.\n",
    "# The training data has already been extracted.\n",
    "\n",
    "CLEANUP_REPOS = False  # Set True to delete cloned repos\n",
    "\n",
    "if CLEANUP_REPOS and os.path.exists(CLONE_DIR):\n",
    "    import shutil\n",
    "    repos = [d for d in os.listdir(CLONE_DIR) if os.path.isdir(os.path.join(CLONE_DIR, d))]\n",
    "    for repo in repos:\n",
    "        repo_path = os.path.join(CLONE_DIR, repo)\n",
    "        shutil.rmtree(repo_path)\n",
    "    print(f\"\\u2713 Removed {len(repos)} cloned repos from {CLONE_DIR}\")\n",
    "else:\n",
    "    if os.path.exists(CLONE_DIR):\n",
    "        repos = [d for d in os.listdir(CLONE_DIR) if os.path.isdir(os.path.join(CLONE_DIR, d))]\n",
    "        total_size = sum(\n",
    "            sum(os.path.getsize(os.path.join(dp, f))\n",
    "                for dp, _, fns in os.walk(os.path.join(CLONE_DIR, r))\n",
    "                for f in fns)\n",
    "            for r in repos\n",
    "        ) / (1024**2)\n",
    "        print(f\"Cloned repos: {len(repos)} ({total_size:.0f} MB)\")\n",
    "        print(\"Set CLEANUP_REPOS = True to delete them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Done!\n",
    "\n",
    "Your mutation training data is saved to the external drive.\n",
    "\n",
    "**Output:**\n",
    "- JSONL: `/Volumes/OWC Express 1M2/rust-mutations/output/mutations.jsonl`\n",
    "- HF Dataset: `/Volumes/OWC Express 1M2/rust-mutations/output/hf_dataset/`\n",
    "- Cloned repos: `/Volumes/OWC Express 1M2/rust-mutations/repos/`\n",
    "\n",
    "**Performance notes for Apple Silicon:**\n",
    "- **sccache** is the single biggest optimization (60-80% faster rebuilds after warmup)\n",
    "- **RAM disk** helps moderately (~10-15%) and reduces SSD wear\n",
    "- **Stripped debug info** saves 20-40% on linking time\n",
    "- `--jobs 4` with `CARGO_BUILD_JOBS=4` saturates an M3/M4 Max (16 cores)\n",
    "- Memory is rarely the bottleneck on 64GB; CPU is the constraint\n",
    "\n",
    "**Next steps:**\n",
    "- Upload to HuggingFace Hub or copy to Google Drive for Colab training\n",
    "- Open `train_gpt_oss_rust_agent_v2.ipynb` with `skip_data_generation=True`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
