{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Custom LLM from Scratch\n",
    "\n",
    "This notebook trains a language model from scratch with configurable:\n",
    "- **Model size** (125M to 7B parameters)\n",
    "- **Model type** (Reasoning Agent, Code Assistant, General Purpose, etc.)\n",
    "- **Dataset selection** (automatically matched to model type)\n",
    "\n",
    "## Requirements\n",
    "- **GPU**: A100 40GB+ (80GB recommended for 3B+)\n",
    "- **Storage**: Google Drive for checkpoints\n",
    "- **Time**: Varies by model size and type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 0.1 Mount Google Drive & Clone Repository\n",
    "import os\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted\")\n",
    "else:\n",
    "    print(\"Running locally\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "#@title ### 0.2 Clone Repository & Install Dependencies\nREPO_URL = \"https://github.com/rmarnold/llm-training-pipeline.git\"  #@param {type:\"string\"}\nBRANCH = \"main\"  #@param {type:\"string\"}\n\nREPO_DIR = \"/content/llm-training-pipeline\"\n\nif IN_COLAB:\n    if os.path.exists(REPO_DIR):\n        %cd {REPO_DIR}\n        !git pull origin {BRANCH}\n    else:\n        !git clone -b {BRANCH} {REPO_URL} {REPO_DIR}\n        %cd {REPO_DIR}\n    \n    print(\"\\nInstalling dependencies...\")\n    !pip install -q -e \".[colab]\"\n    !pip install -q flash-attn --no-build-isolation 2>/dev/null || true\n    !pip install -q liger-kernel bitsandbytes\n    \n    # Install GPU acceleration libraries (RAPIDS + NeMo Curator)\n    print(\"\\nInstalling GPU acceleration libraries...\")\n    \n    # RAPIDS cuDF for GPU text cleaning (100-150x faster)\n    !pip install -q --extra-index-url=https://pypi.nvidia.com cudf-cu12 2>/dev/null || echo \"RAPIDS cuDF not available\"\n    \n    # NeMo Curator for GPU deduplication (16-107x faster)\n    # Requires dask-cuda for GPU cluster management\n    !pip install -q dask-cuda 2>/dev/null || echo \"dask-cuda not available\"\n    !pip install -q nemo-curator 2>/dev/null || echo \"NeMo Curator not available\"\n    \n    # Verify installations\n    print(\"\\n\" + \"=\" * 50)\n    print(\"GPU Library Verification:\")\n    print(\"=\" * 50)\n    \n    # Check RAPIDS cuDF\n    try:\n        import cudf\n        print(f\"✓ RAPIDS cuDF: OK (version {cudf.__version__})\")\n    except ImportError as e:\n        print(f\"✗ RAPIDS cuDF: NOT AVAILABLE ({e})\")\n    \n    # Check NeMo Curator - discover and print actual API structure\n    print(\"\\nNeMo Curator API Discovery:\")\n    nemo_ok = False\n    nemo_api = None\n    \n    try:\n        import nemo_curator\n        nemo_version = getattr(nemo_curator, '__version__', 'unknown')\n        print(f\"  Package version: {nemo_version}\")\n        \n        # List top-level modules\n        top_level = [x for x in dir(nemo_curator) if not x.startswith('_')]\n        print(f\"  Top-level: {top_level}\")\n        \n        # Check stages module structure\n        if hasattr(nemo_curator, 'stages'):\n            import nemo_curator.stages as stages\n            stages_contents = [x for x in dir(stages) if not x.startswith('_')]\n            print(f\"  stages: {stages_contents}\")\n            \n            # Check for deduplication in stages\n            if hasattr(stages, 'deduplication'):\n                import nemo_curator.stages.deduplication as dedup\n                dedup_contents = [x for x in dir(dedup) if not x.startswith('_')]\n                print(f\"  stages.deduplication: {dedup_contents}\")\n                \n                # Check for fuzzy module\n                if hasattr(dedup, 'fuzzy'):\n                    import nemo_curator.stages.deduplication.fuzzy as fuzzy\n                    fuzzy_contents = [x for x in dir(fuzzy) if not x.startswith('_')]\n                    print(f\"  stages.deduplication.fuzzy: {fuzzy_contents}\")\n                    \n                    # Check for workflow\n                    if hasattr(fuzzy, 'workflow'):\n                        import nemo_curator.stages.deduplication.fuzzy.workflow as workflow\n                        workflow_contents = [x for x in dir(workflow) if not x.startswith('_')]\n                        print(f\"  stages.deduplication.fuzzy.workflow: {workflow_contents}\")\n                        \n                        if 'FuzzyDeduplicationWorkflow' in workflow_contents:\n                            nemo_ok = True\n                            nemo_api = \"stages.deduplication.fuzzy.workflow.FuzzyDeduplicationWorkflow\"\n                            print(f\"  ✓ Found: {nemo_api}\")\n            \n            # Also check for text module (alternative path)\n            if hasattr(stages, 'text') and not nemo_ok:\n                import nemo_curator.stages.text as text\n                text_contents = [x for x in dir(text) if not x.startswith('_')]\n                print(f\"  stages.text: {text_contents}\")\n        \n        # Check tasks module\n        if hasattr(nemo_curator, 'tasks') and not nemo_ok:\n            import nemo_curator.tasks as tasks\n            tasks_contents = [x for x in dir(tasks) if not x.startswith('_')]\n            print(f\"  tasks: {tasks_contents}\")\n        \n        # Try direct import as fallback\n        if not nemo_ok:\n            try:\n                from nemo_curator.stages.deduplication.fuzzy.workflow import FuzzyDeduplicationWorkflow\n                nemo_ok = True\n                nemo_api = \"workflow (direct import)\"\n                print(f\"  ✓ Direct import succeeded: FuzzyDeduplicationWorkflow\")\n            except ImportError as e:\n                print(f\"  ✗ Direct import failed: {e}\")\n                \n    except ImportError as e:\n        print(f\"  ✗ Cannot import nemo_curator: {e}\")\n    except Exception as e:\n        print(f\"  ✗ Error during discovery: {e}\")\n    \n    # Check dask-cuda\n    try:\n        from dask_cuda import LocalCUDACluster\n        dask_cuda_ok = True\n        print(f\"\\n✓ dask-cuda: OK\")\n    except ImportError:\n        dask_cuda_ok = False\n        print(f\"\\n✗ dask-cuda: NOT AVAILABLE\")\n    \n    # Summary\n    print(\"\\n\" + \"=\" * 50)\n    if nemo_ok:\n        print(f\"✓ NeMo Curator: READY (API: {nemo_api})\")\n    else:\n        print(\"✗ NeMo Curator: Deduplication API not found\")\n        print(\"  Will use CPU datasketch fallback for deduplication\")\n    print(\"=\" * 50)\n    \n    PROJECT_ROOT = REPO_DIR\nelse:\n    PROJECT_ROOT = os.getcwd()\n\nos.chdir(PROJECT_ROOT)\nprint(f\"\\nProject root: {PROJECT_ROOT}\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "#@title ### 0.3 Choose Model Type & Size { run: \"auto\" }\n#@markdown ---\n#@markdown ### Model Configuration\n\nmodel_type = \"reasoning_agent\"  #@param [\"reasoning_agent\", \"code_assistant\", \"general_assistant\", \"chat_model\"]\n#@markdown **Model Types:**\n#@markdown - `reasoning_agent`: Math, logic, function calling, tool use\n#@markdown - `code_assistant`: Code generation, debugging, explanation\n#@markdown - `general_assistant`: Balanced instruction following\n#@markdown - `chat_model`: Conversational, helpful responses\n\nmodel_size = \"1b\"  #@param [\"125m\", \"350m\", \"1b\", \"3b\", \"7b\"]\n#@markdown **Model Sizes:**\n#@markdown - `125m`: Fast training, testing (~2-4 hours pretrain)\n#@markdown - `350m`: Small but capable (~4-8 hours)\n#@markdown - `1b`: Good balance (~12-16 hours)\n#@markdown - `3b`: Strong performance (~30-40 hours)\n#@markdown - `7b`: Full capability (~60-80 hours)\n\n#@markdown ---\n#@markdown ### Training Parameters\ncontext_length = 2048  #@param {type:\"integer\"}\npretrain_tokens_billions = 50  #@param {type:\"number\"}\n#@markdown *Recommended: 20x model params (1B model = 20B tokens minimum)*\n\n#@markdown ---\n#@markdown ### Data Preparation Settings\ndata_prep_speed = \"fast\"  #@param [\"fast\", \"balanced\", \"thorough\"]\n#@markdown **Data Prep Quality Modes:**\n#@markdown - `fast`: Skip quality filters, just clean + dedup (~5 min for 12M docs)\n#@markdown - `balanced`: Basic quality filter, no toxicity (~30 min)\n#@markdown - `thorough`: All quality filters + toxicity (~2-3 hours)\n\nuse_gpu_data_prep = True  #@param {type:\"boolean\"}\n#@markdown **GPU Data Prep (Recommended for A100/H100):**\n#@markdown - `True`: Use RAPIDS cuDF + NeMo Curator (10-30x faster)\n#@markdown - `False`: Use CPU-only pipeline (slower but more compatible)\n\n# ============================================================\n# MODEL TYPE CONFIGURATIONS\n# ============================================================\n\nMODEL_TYPE_CONFIGS = {\n    \"reasoning_agent\": {\n        \"description\": \"Optimized for math, logic, and tool use\",\n        \"pretraining_datasets\": [\n            \"slimpajama\", \"wikipedia\", \"the-stack-python\", \n            \"openwebtext\", \"arxiv\", \"stackexchange\"\n        ],\n        \"sft_datasets\": [\n            \"gsm8k\", \"orca-math\", \"openorca\", \"cot-collection\", \"metamath\",\n            \"glaive-function-calling\", \"hermes-function-calling\", \"toolbench\",\n            \"logiqa\"\n        ],\n        \"dpo_datasets\": [\"hh-rlhf\", \"ultrafeedback\"],\n        \"data_prep_script\": \"06_prepare_reasoning_data.py\",\n        \"eval_benchmarks\": [\"gsm8k\", \"arc\", \"function_calling\", \"safety\"],\n        \"sft_focus\": \"reasoning\",\n    },\n    \"code_assistant\": {\n        \"description\": \"Optimized for code generation and understanding\",\n        \"pretraining_datasets\": [\n            \"the-stack\", \"slimpajama\", \"wikipedia\", \"arxiv\"\n        ],\n        \"sft_datasets\": [\n            \"code-alpaca\", \"python-code-instructions\", \"evol-instruct-code\",\n            \"glaive-function-calling\", \"oasst1\"\n        ],\n        \"dpo_datasets\": [\"hh-rlhf\"],\n        \"data_prep_script\": \"prepare_lora_data.py\",\n        \"eval_benchmarks\": [\"humaneval\", \"mbpp\", \"safety\"],\n        \"sft_focus\": \"code\",\n    },\n    \"general_assistant\": {\n        \"description\": \"Balanced instruction following\",\n        \"pretraining_datasets\": [\n            \"slimpajama\", \"wikipedia\", \"openwebtext\", \"pg19\"\n        ],\n        \"sft_datasets\": [\n            \"oasst1\", \"dolly-15k\", \"alpaca-cleaned\", \"openorca\"\n        ],\n        \"dpo_datasets\": [\"hh-rlhf\", \"ultrafeedback\"],\n        \"data_prep_script\": \"06_prepare_sft_data.py\",\n        \"eval_benchmarks\": [\"mmlu\", \"hellaswag\", \"safety\"],\n        \"sft_focus\": \"instruction\",\n    },\n    \"chat_model\": {\n        \"description\": \"Conversational and helpful\",\n        \"pretraining_datasets\": [\n            \"slimpajama\", \"wikipedia\", \"openwebtext\"\n        ],\n        \"sft_datasets\": [\n            \"oasst1\", \"dolly-15k\", \"sharegpt\"\n        ],\n        \"dpo_datasets\": [\"hh-rlhf\", \"ultrafeedback\"],\n        \"data_prep_script\": \"06_prepare_sft_data.py\",\n        \"eval_benchmarks\": [\"mt-bench\", \"safety\"],\n        \"sft_focus\": \"chat\",\n    },\n}\n\n# Data prep speed configurations\nDATA_PREP_CONFIGS = {\n    \"fast\": {\n        \"flags\": \"--native-pipeline --fast-quality --no-toxicity --fresh\",\n        \"gpu_flags\": \"--skip-quality --no-toxicity\",\n        \"description\": \"Fastest: Skip quality filters, just clean + dedup (~5 min)\",\n    },\n    \"balanced\": {\n        \"flags\": \"--native-pipeline --fast-quality --fresh\",\n        \"gpu_flags\": \"--fast-quality --no-toxicity\",\n        \"description\": \"Balanced: Basic quality filter, no toxicity (~30 min)\",\n    },\n    \"thorough\": {\n        \"flags\": \"--native-pipeline --fresh\",\n        \"gpu_flags\": \"\",\n        \"description\": \"Thorough: All quality filters + toxicity (~2-3 hours)\",\n    },\n}\n\n# ============================================================\n# OPTIMIZED SIZE CONFIGURATIONS (A100 40GB/80GB)\n# ============================================================\nSIZE_CONFIGS = {\n    \"125m\": {\n        \"batch_size\": 64,\n        \"grad_accum\": 1,\n        \"learning_rate\": 1e-4,\n        \"warmup_ratio\": 0.05,\n        \"use_torch_compile\": False,\n        \"use_8bit_optim\": False,\n        \"dataloader_workers\": 16,\n        \"pretrain_hours\": 3,\n        \"sft_hours\": 0.5,\n        \"dpo_hours\": 0.25,\n        \"expected_throughput\": \"8-12 it/s\",\n    },\n    \"350m\": {\n        \"batch_size\": 32,\n        \"grad_accum\": 2,\n        \"learning_rate\": 1e-4,\n        \"warmup_ratio\": 0.05,\n        \"use_torch_compile\": False,\n        \"use_8bit_optim\": False,\n        \"dataloader_workers\": 16,\n        \"pretrain_hours\": 6,\n        \"sft_hours\": 1,\n        \"dpo_hours\": 0.5,\n        \"expected_throughput\": \"5-8 it/s\",\n    },\n    \"1b\": {\n        \"batch_size\": 16,\n        \"grad_accum\": 4,\n        \"learning_rate\": 3e-4,\n        \"warmup_ratio\": 0.03,\n        \"use_torch_compile\": True,\n        \"use_8bit_optim\": True,\n        \"dataloader_workers\": 12,\n        \"pretrain_hours\": 15,\n        \"sft_hours\": 4,\n        \"dpo_hours\": 1.5,\n        \"expected_throughput\": \"3-5 it/s\",\n    },\n    \"3b\": {\n        \"batch_size\": 8,\n        \"grad_accum\": 8,\n        \"learning_rate\": 3e-4,\n        \"warmup_ratio\": 0.03,\n        \"use_torch_compile\": True,\n        \"use_8bit_optim\": True,\n        \"dataloader_workers\": 8,\n        \"pretrain_hours\": 35,\n        \"sft_hours\": 10,\n        \"dpo_hours\": 4,\n        \"expected_throughput\": \"1.5-2.5 it/s\",\n    },\n    \"7b\": {\n        \"batch_size\": 4,\n        \"grad_accum\": 16,\n        \"learning_rate\": 3e-4,\n        \"warmup_ratio\": 0.03,\n        \"use_torch_compile\": True,\n        \"use_8bit_optim\": True,\n        \"dataloader_workers\": 8,\n        \"pretrain_hours\": 70,\n        \"sft_hours\": 20,\n        \"dpo_hours\": 8,\n        \"expected_throughput\": \"0.8-1.2 it/s\",\n    },\n}\n\n# Build configuration\ntype_config = MODEL_TYPE_CONFIGS[model_type]\nsize_config = SIZE_CONFIGS[model_size]\ndata_prep_config = DATA_PREP_CONFIGS[data_prep_speed]\n\nCONFIG = {\n    'model_type': model_type,\n    'model_size': model_size,\n    'context_length': context_length,\n    'pretrain_tokens_b': pretrain_tokens_billions,\n    'batch_size': size_config['batch_size'],\n    'grad_accum': size_config['grad_accum'],\n    'learning_rate': size_config['learning_rate'],\n    'warmup_ratio': size_config['warmup_ratio'],\n    'use_torch_compile': size_config['use_torch_compile'],\n    'use_8bit_optim': size_config['use_8bit_optim'],\n    'dataloader_workers': size_config['dataloader_workers'],\n    'pretraining_datasets': type_config['pretraining_datasets'],\n    'sft_datasets': type_config['sft_datasets'],\n    'dpo_datasets': type_config['dpo_datasets'],\n    'data_prep_script': type_config['data_prep_script'],\n    'eval_benchmarks': type_config['eval_benchmarks'],\n    'sft_focus': type_config['sft_focus'],\n    'data_prep_flags': data_prep_config['flags'],\n    'gpu_data_prep_flags': data_prep_config['gpu_flags'],\n    'data_prep_speed': data_prep_speed,\n    'use_gpu_data_prep': use_gpu_data_prep,\n}\n\n# Calculate training steps\neffective_batch = CONFIG['batch_size'] * CONFIG['grad_accum']\ntokens_per_step = effective_batch * context_length\ntotal_tokens = int(pretrain_tokens_billions * 1e9)\npretrain_steps = total_tokens // tokens_per_step\nCONFIG['pretrain_steps'] = pretrain_steps\nCONFIG['effective_batch_size'] = effective_batch\n\n# Estimate time\ntime_scale = pretrain_tokens_billions / 50\npretrain_hours = size_config['pretrain_hours'] * time_scale\ntotal_hours = pretrain_hours + size_config['sft_hours'] + size_config['dpo_hours']\n\nDRIVE_BASE = f\"/content/drive/MyDrive/llm-{model_size}-{model_type.replace('_', '-')}\"\n\nprint(\"=\" * 60)\nprint(\"MODEL CONFIGURATION\")\nprint(\"=\" * 60)\nprint(f\"\\nType: {model_type.upper().replace('_', ' ')}\")\nprint(f\"Size: {model_size.upper()}, Context: {context_length} tokens\")\nprint(f\"\\nData Prep: {data_prep_speed.upper()} - {data_prep_config['description']}\")\nprint(f\"GPU Acceleration: {'ENABLED' if use_gpu_data_prep else 'DISABLED'}\")\nprint(f\"\\nPretraining: {pretrain_tokens_billions}B tokens, {pretrain_steps:,} steps\")\nprint(f\"Expected time: ~{total_hours:.0f} hours ({total_hours/24:.1f} days)\")\nprint(\"=\" * 60)",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "#@title ### 0.4 Set Up Persistent Storage\n\nif IN_COLAB:\n    print(f\"Setting up storage at: {DRIVE_BASE}\")\n    \n    # Create Drive directories\n    for subdir in ['checkpoints', 'data', 'data/raw', 'data/packed', 'data/sft', 'data/dpo', 'logs', 'evals']:\n        os.makedirs(os.path.join(DRIVE_BASE, subdir), exist_ok=True)\n    \n    # Create symlinks\n    for dir_name in ['checkpoints', 'data', 'logs', 'evals']:\n        local_path = os.path.join(PROJECT_ROOT, dir_name)\n        drive_path = os.path.join(DRIVE_BASE, dir_name)\n        \n        if os.path.exists(local_path) and not os.path.islink(local_path):\n            !cp -r {local_path}/* {drive_path}/ 2>/dev/null || true\n            !rm -rf {local_path}\n        elif os.path.islink(local_path):\n            os.unlink(local_path)\n        \n        os.symlink(drive_path, local_path)\n        print(f\"  {dir_name} -> Drive\")\n    \n    print(\"\\nStorage ready!\")\nelse:\n    for d in ['checkpoints', 'data', 'logs', 'evals']:\n        os.makedirs(d, exist_ok=True)",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#@title ### 0.5 Copy Data to Local SSD (Faster I/O)\n#@markdown Copies data from Google Drive to local NVMe SSD for 5-10x faster I/O during training.\n#@markdown This significantly speeds up data prep and training.\n\nimport shutil\n\n# Local SSD paths (much faster than Drive)\nLOCAL_DATA = \"/content/local_data\"\nLOCAL_RAW = f\"{LOCAL_DATA}/raw\"\nLOCAL_PROCESSED = f\"{LOCAL_DATA}/processed\"\nLOCAL_PACKED = f\"{LOCAL_DATA}/packed\"\nLOCAL_CACHE = \"/content/.gpu_cache\"\nLOCAL_CHECKPOINTS = \"/content/local_checkpoints\"\n\n# Drive paths (persistent)\nDRIVE_RAW = f\"{DRIVE_BASE}/data/raw\"\nDRIVE_PACKED = f\"{DRIVE_BASE}/data/packed\"\nDRIVE_CHECKPOINTS = f\"{DRIVE_BASE}/checkpoints\"\n\nif IN_COLAB:\n    print(\"Setting up local SSD storage for faster I/O...\")\n    print(\"=\" * 50)\n    \n    # Create local directories\n    for d in [LOCAL_DATA, LOCAL_RAW, LOCAL_PROCESSED, LOCAL_PACKED, LOCAL_CACHE, LOCAL_CHECKPOINTS]:\n        os.makedirs(d, exist_ok=True)\n    \n    # Check if we have existing data on Drive to copy\n    drive_raw_files = []\n    if os.path.exists(DRIVE_RAW):\n        drive_raw_files = [f for f in os.listdir(DRIVE_RAW) if f.endswith('.parquet')]\n    \n    if drive_raw_files:\n        print(f\"\\nFound {len(drive_raw_files)} raw data files on Drive\")\n        print(\"Copying to local SSD for faster processing...\")\n        for f in drive_raw_files:\n            src = os.path.join(DRIVE_RAW, f)\n            dst = os.path.join(LOCAL_RAW, f)\n            if not os.path.exists(dst):\n                size_mb = os.path.getsize(src) / (1024*1024)\n                print(f\"  Copying {f} ({size_mb:.0f} MB)...\")\n                shutil.copy2(src, dst)\n            else:\n                print(f\"  {f} already on local SSD\")\n        print(\"Raw data copied!\")\n    else:\n        print(\"\\nNo existing raw data on Drive (will download fresh)\")\n    \n    # Check for existing packed data\n    drive_packed_files = []\n    if os.path.exists(DRIVE_PACKED):\n        drive_packed_files = list(os.listdir(DRIVE_PACKED))\n    \n    if drive_packed_files:\n        print(f\"\\nFound packed data on Drive - copying to local SSD...\")\n        !cp -r {DRIVE_PACKED}/* {LOCAL_PACKED}/ 2>/dev/null || true\n        print(\"Packed data copied!\")\n    \n    # Store paths in CONFIG for later use\n    CONFIG['local_raw'] = LOCAL_RAW\n    CONFIG['local_processed'] = LOCAL_PROCESSED\n    CONFIG['local_packed'] = LOCAL_PACKED\n    CONFIG['local_cache'] = LOCAL_CACHE\n    CONFIG['local_checkpoints'] = LOCAL_CHECKPOINTS\n    CONFIG['drive_raw'] = DRIVE_RAW\n    CONFIG['drive_packed'] = DRIVE_PACKED\n    CONFIG['drive_checkpoints'] = DRIVE_CHECKPOINTS\n    CONFIG['use_local_ssd'] = True\n    \n    # Check local disk space\n    import subprocess\n    result = subprocess.run(['df', '-h', '/content'], capture_output=True, text=True)\n    print(f\"\\nLocal SSD status:\")\n    print(result.stdout.split('\\n')[1])\n    \n    print(\"\\nLocal SSD paths configured:\")\n    print(f\"  Raw data: {LOCAL_RAW}\")\n    print(f\"  Processed: {LOCAL_PROCESSED}\")\n    print(f\"  Packed: {LOCAL_PACKED}\")\n    print(f\"  Cache: {LOCAL_CACHE}\")\n    print(\"=\" * 50)\nelse:\n    CONFIG['use_local_ssd'] = False\n    print(\"Running locally - no SSD optimization needed\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 0.5 Check GPU\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    capability = torch.cuda.get_device_capability()\n",
    "    \n",
    "    CONFIG['use_fp8'] = capability[0] >= 9\n",
    "    \n",
    "    print(f\"GPU: {gpu_name} ({gpu_memory:.0f} GB)\")\n",
    "    print(f\"FP8: {'Available' if CONFIG['use_fp8'] else 'Not available'}\")\n",
    "    \n",
    "    # Memory check for model size\n",
    "    min_memory = {'125m': 8, '350m': 16, '1b': 24, '3b': 40, '7b': 70}\n",
    "    if gpu_memory < min_memory[CONFIG['model_size']]:\n",
    "        print(f\"\\nWARNING: {CONFIG['model_size']} may need {min_memory[CONFIG['model_size']]}+ GB\")\n",
    "else:\n",
    "    print(\"No GPU detected!\")\n",
    "    CONFIG['use_fp8'] = False"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Step 1: Download Data\n\nDownloads datasets matched to your model type.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 1.1 Download Pretraining Data\n",
    "#@markdown Downloads pretraining corpora for your model type.\n",
    "\n",
    "print(f\"Downloading pretraining data for: {CONFIG['model_type']}\")\n",
    "print(f\"Datasets: {CONFIG['pretraining_datasets']}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "!python scripts/01_download_data.py --phases pretraining"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Step 2: Prepare Data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#@title ### 2.1 Clean & Tokenize Pretraining Data\n#@markdown Uses the data prep speed setting from configuration above.\n#@markdown GPU mode uses RAPIDS cuDF (150x faster text cleaning) + NeMo Curator (16x faster dedup).\n#@markdown All processing happens on local SSD with incremental backup to Drive.\n\nuse_gpu = CONFIG.get('use_gpu_data_prep', False)\ndata_prep_speed = CONFIG['data_prep_speed']\nuse_local_ssd = CONFIG.get('use_local_ssd', False)\n\nprint(f\"Data prep mode: {data_prep_speed.upper()}\")\nprint(f\"GPU Acceleration: {'ENABLED' if use_gpu else 'DISABLED'}\")\nprint(f\"Local SSD: {'ENABLED (5-10x faster I/O)' if use_local_ssd else 'DISABLED'}\")\nprint(\"=\" * 50)\n\nif use_local_ssd:\n    local_raw = CONFIG['local_raw']\n    local_processed = CONFIG['local_processed']\n    local_packed = CONFIG['local_packed']\n    local_cache = CONFIG['local_cache']\n    drive_packed = CONFIG['drive_packed']\n    \n    # Create backup dir on Drive for incremental checkpoint\n    drive_cache = f\"{DRIVE_BASE}/data/.gpu_cache\"\n    os.makedirs(drive_cache, exist_ok=True)\n    CONFIG['drive_cache'] = drive_cache\n    \n    if use_gpu:\n        # GPU-accelerated pipeline on local SSD with Drive backup\n        gpu_flags = CONFIG['gpu_data_prep_flags']\n        print(f\"Using GPU pipeline on local SSD...\")\n        print(f\"  Input: {local_raw}\")\n        print(f\"  Output: {local_processed}\")\n        print(f\"  Cache: {local_cache}\")\n        print(f\"  Backup: {drive_cache} (incremental sync)\")\n        !python scripts/02_gpu_clean_deduplicate.py \\\n            --input {local_raw} \\\n            --output {local_processed} \\\n            --cache {local_cache} \\\n            --backup-dir {drive_cache} \\\n            {gpu_flags}\n    else:\n        # CPU pipeline on local SSD\n        data_prep_flags = CONFIG['data_prep_flags']\n        print(f\"Using CPU pipeline on local SSD...\")\n        !python scripts/02_clean_deduplicate_optimized.py \\\n            --input {local_raw} \\\n            --output {local_processed} \\\n            {data_prep_flags}\n    \n    # Tokenize and pack on local SSD\n    print(f\"\\nTokenizing and packing...\")\n    !python scripts/03_tokenize_and_pack.py \\\n        --input-dir {local_processed} \\\n        --output-dir {local_packed}\n    \n    # Backup packed data to Drive for persistence\n    print(f\"\\nBacking up packed data to Drive...\")\n    !cp -r {local_packed}/* {drive_packed}/ 2>/dev/null || true\n    print(\"Backup complete!\")\n    \nelse:\n    # Original paths (Drive-linked)\n    if use_gpu:\n        gpu_flags = CONFIG['gpu_data_prep_flags']\n        print(\"Using GPU pipeline...\")\n        !python scripts/02_gpu_clean_deduplicate.py --input data/raw --output data/processed {gpu_flags}\n    else:\n        data_prep_flags = CONFIG['data_prep_flags']\n        print(f\"Using CPU pipeline with flags: {data_prep_flags}\")\n        !python scripts/02_clean_deduplicate_optimized.py {data_prep_flags}\n    \n    !python scripts/03_tokenize_and_pack.py",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 1.2 Download SFT & DPO Data\n",
    "#@markdown Downloads fine-tuning data matched to your model type.\n",
    "\n",
    "print(f\"Downloading SFT data for: {CONFIG['model_type']}\")\n",
    "print(f\"SFT datasets: {CONFIG['sft_datasets']}\")\n",
    "print(f\"DPO datasets: {CONFIG['dpo_datasets']}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Download based on model type\n",
    "if CONFIG['sft_focus'] == 'reasoning':\n",
    "    !python scripts/01_download_data.py --phases reasoning function_calling logic instruction_tuning preference_data\n",
    "elif CONFIG['sft_focus'] == 'code':\n",
    "    !python scripts/01_download_data.py --phases instruction_tuning preference_data\n",
    "    # Code data handled by prepare_lora_data.py\n",
    "else:\n",
    "    !python scripts/01_download_data.py --phases instruction_tuning preference_data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 2.2 Prepare SFT Data\n",
    "#@markdown Uses the appropriate script for your model type.\n",
    "\n",
    "prep_script = CONFIG['data_prep_script']\n",
    "print(f\"Using: {prep_script} (optimized for {CONFIG['model_type']})\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "!python scripts/{prep_script}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 2.3 Prepare DPO Data\n",
    "\n",
    "!python scripts/08_prepare_dpo_data.py"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Step 3: Initialize Model"
  },
  {
   "cell_type": "code",
   "source": "#@title ### 3.1 Initialize Model\n\nmodel_size = CONFIG['model_size']\ncontext = CONFIG['context_length']\n\nprint(f\"Initializing {model_size.upper()} model with {context} context...\")\n\n!python scripts/04_init_model.py --size {model_size} --context-length {context}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#@title ### 3.2 Verify Setup\nimport os\n\nchecks = [\n    ('Model', 'checkpoints/init'),\n    ('Tokenizer', 'configs/tokenizer'),\n    ('Pretrain data', 'data/packed'),\n    ('SFT data', 'data/sft/train'),\n    ('DPO data', 'data/dpo/train'),\n]\n\nprint(\"Setup verification:\")\nall_ok = True\nfor name, path in checks:\n    ok = os.path.exists(path)\n    print(f\"  {name}: {'OK' if ok else 'MISSING'}\")\n    all_ok = all_ok and ok\n\nif all_ok:\n    print(\"\\nReady for training!\")\nelse:\n    print(\"\\nFix missing items before proceeding.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Step 4: Pretraining",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#@title ### 4.1 Start Pretraining\n#@markdown Uses auto-optimized parameters based on model size.\n#@markdown Training uses local SSD for data loading (5-10x faster I/O).\n#@markdown Checkpoints are saved locally and backed up to Drive periodically.\n\nsteps = CONFIG['pretrain_steps']\nuse_fp8 = CONFIG.get('use_fp8', False)\nuse_local_ssd = CONFIG.get('use_local_ssd', False)\n\n# Build optimized command with size-specific parameters\ncmd = f\"python scripts/05_pretrain.py --max_steps {steps}\"\n\n# Local SSD paths for faster I/O\nif use_local_ssd:\n    local_packed = CONFIG['local_packed']\n    local_checkpoints = CONFIG['local_checkpoints']\n    cmd += f\" --train_data_path {local_packed}\"\n    cmd += f\" --output_dir {local_checkpoints}/pretrain\"\n\n# Always use Liger Kernel (compatible with all sizes)\ncmd += \" --use-liger-kernel\"\n\n# Size-specific optimizations\ncmd += f\" --per_device_train_batch_size {CONFIG['batch_size']}\"\ncmd += f\" --gradient_accumulation_steps {CONFIG['grad_accum']}\"\ncmd += f\" --learning_rate {CONFIG['learning_rate']}\"\ncmd += f\" --warmup_ratio {CONFIG['warmup_ratio']}\"\ncmd += f\" --dataloader_num_workers {CONFIG['dataloader_workers']}\"\n\n# torch.compile: Enable only for larger models where benefit > overhead\nif not CONFIG['use_torch_compile']:\n    cmd += \" --no-torch-compile\"\n\n# 8-bit optimizer: Enable for larger models to save memory\nif CONFIG['use_8bit_optim']:\n    cmd += \" --optim adamw_bnb_8bit\"\nelse:\n    cmd += \" --optim adamw_torch_fused\"\n\n# FP8 for H100\nif use_fp8:\n    cmd += \" --fp8\"\n\n# OOM recovery (always useful)\ncmd += \" --enable-oom-recovery\"\n\nprint(f\"Model: {CONFIG['model_size'].upper()} {CONFIG['model_type']}\")\nprint(f\"Steps: {steps:,}\")\nprint(f\"\\nOptimized Parameters:\")\nprint(f\"  Batch: {CONFIG['batch_size']} x {CONFIG['grad_accum']} = {CONFIG['effective_batch_size']}\")\nprint(f\"  Learning rate: {CONFIG['learning_rate']:.0e}\")\nprint(f\"  torch.compile: {'ON' if CONFIG['use_torch_compile'] else 'OFF'}\")\nprint(f\"  8-bit optimizer: {'ON' if CONFIG['use_8bit_optim'] else 'OFF'}\")\nif use_local_ssd:\n    print(f\"\\nLocal SSD I/O:\")\n    print(f\"  Data: {CONFIG['local_packed']}\")\n    print(f\"  Checkpoints: {CONFIG['local_checkpoints']}/pretrain\")\nprint(f\"\\nCommand: {cmd}\")\nprint(\"=\" * 50)\n\n!{cmd}\n\n# Backup checkpoints to Drive after training\nif use_local_ssd:\n    drive_checkpoints = CONFIG['drive_checkpoints']\n    local_checkpoints = CONFIG['local_checkpoints']\n    print(f\"\\nBacking up checkpoints to Drive...\")\n    !cp -r {local_checkpoints}/pretrain/* {drive_checkpoints}/pretrain/ 2>/dev/null || true\n    print(\"Checkpoint backup complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Step 5: SFT Training",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 5.1 Start SFT\n",
    "\n",
    "cmd = \"python scripts/07_sft.py --use-liger-kernel --enable-oom-recovery\"\n",
    "if CONFIG.get('use_fp8', False):\n",
    "    cmd += \" --fp8\"\n",
    "\n",
    "print(f\"SFT focus: {CONFIG['sft_focus']}\")\n",
    "print(f\"Datasets: {CONFIG['sft_datasets']}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "!{cmd}"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: DPO Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 6.1 Start DPO\n",
    "\n",
    "cmd = \"python scripts/09_dpo.py --enable-oom-recovery\"\n",
    "if CONFIG.get('use_fp8', False):\n",
    "    cmd += \" --fp8\"\n",
    "\n",
    "!{cmd}"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 7.1 Run Evaluation\n",
    "\n",
    "print(f\"Benchmarks for {CONFIG['model_type']}: {CONFIG['eval_benchmarks']}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "!python scripts/11_evaluate.py checkpoints/dpo_final"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 7.2 Check Gates\n",
    "\n",
    "!python scripts/12_check_gates.py dpo"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 8.1 Load Model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "MODEL_PATH = \"checkpoints/dpo_final\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"configs/tokenizer\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"Model loaded!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 8.2 Generate Response\n",
    "\n",
    "def generate(prompt, max_tokens=512):\n",
    "    formatted = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n\"\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_tokens, \n",
    "                                 temperature=0.7, do_sample=True, top_p=0.9)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test prompts based on model type\n",
    "TEST_PROMPTS = {\n",
    "    'reasoning_agent': \"Solve step by step: If a train travels 60 mph for 2.5 hours, how far does it go?\",\n",
    "    'code_assistant': \"Write a Python function to find the nth Fibonacci number.\",\n",
    "    'general_assistant': \"Explain the difference between machine learning and deep learning.\",\n",
    "    'chat_model': \"Hello! How are you today? What can you help me with?\",\n",
    "}\n",
    "\n",
    "prompt = TEST_PROMPTS.get(CONFIG['model_type'], \"Hello, how are you?\")\n",
    "print(f\"Test for {CONFIG['model_type']}:\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"\\nResponse:\")\n",
    "print(generate(prompt))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 8.3 Custom Prompt\n",
    "\n",
    "CUSTOM_PROMPT = \"\"\"Your prompt here\"\"\"  #@param {type:\"string\"}\n",
    "\n",
    "print(generate(CUSTOM_PROMPT))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Complete!\n",
    "\n",
    "Your model is trained and ready to use. Check the evaluation results above to see how it performs on relevant benchmarks."
   ]
  }
 ]
}