{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Custom LLM from Scratch\n",
    "\n",
    "This notebook trains a language model from scratch with configurable:\n",
    "- **Model size** (125M to 7B parameters)\n",
    "- **Model type** (Reasoning Agent, Code Assistant, General Purpose, etc.)\n",
    "- **Dataset selection** (automatically matched to model type)\n",
    "\n",
    "## Requirements\n",
    "- **GPU**: A100 40GB+ (80GB recommended for 3B+)\n",
    "- **Storage**: Google Drive for checkpoints\n",
    "- **Time**: Varies by model size and type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 0.1 Mount Google Drive & Clone Repository\n",
    "import os\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted\")\n",
    "else:\n",
    "    print(\"Running locally\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "#@title ### 0.2 Clone Repository & Install Dependencies\nREPO_URL = \"https://github.com/rmarnold/llm-training-pipeline.git\"  #@param {type:\"string\"}\nBRANCH = \"main\"  #@param {type:\"string\"}\n\nREPO_DIR = \"/content/llm-training-pipeline\"\n\nif IN_COLAB:\n    if os.path.exists(REPO_DIR):\n        %cd {REPO_DIR}\n        !git pull origin {BRANCH}\n    else:\n        !git clone -b {BRANCH} {REPO_URL} {REPO_DIR}\n        %cd {REPO_DIR}\n    \n    print(\"\\nInstalling dependencies...\")\n    !pip install -q -e \".[colab]\"\n    !pip install -q flash-attn --no-build-isolation 2>/dev/null || true\n    !pip install -q liger-kernel bitsandbytes\n    \n    # Install GPU acceleration libraries (RAPIDS + NeMo Curator)\n    print(\"\\nInstalling GPU acceleration libraries...\")\n    !pip install -q --extra-index-url=https://pypi.nvidia.com cudf-cu12 cuml-cu12 2>/dev/null || echo \"RAPIDS not available\"\n    !pip install -q \"nemo-curator[text_cuda12]\" 2>/dev/null || echo \"NeMo Curator not available\"\n    \n    PROJECT_ROOT = REPO_DIR\nelse:\n    PROJECT_ROOT = os.getcwd()\n\nos.chdir(PROJECT_ROOT)\nprint(f\"\\nProject root: {PROJECT_ROOT}\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "#@title ### 0.3 Choose Model Type & Size { run: \"auto\" }\n#@markdown ---\n#@markdown ### Model Configuration\n\nmodel_type = \"reasoning_agent\"  #@param [\"reasoning_agent\", \"code_assistant\", \"general_assistant\", \"chat_model\"]\n#@markdown **Model Types:**\n#@markdown - `reasoning_agent`: Math, logic, function calling, tool use\n#@markdown - `code_assistant`: Code generation, debugging, explanation\n#@markdown - `general_assistant`: Balanced instruction following\n#@markdown - `chat_model`: Conversational, helpful responses\n\nmodel_size = \"1b\"  #@param [\"125m\", \"350m\", \"1b\", \"3b\", \"7b\"]\n#@markdown **Model Sizes:**\n#@markdown - `125m`: Fast training, testing (~2-4 hours pretrain)\n#@markdown - `350m`: Small but capable (~4-8 hours)\n#@markdown - `1b`: Good balance (~40-50 hours)\n#@markdown - `3b`: Strong performance (~80-100 hours)\n#@markdown - `7b`: Full capability (~150+ hours)\n\n#@markdown ---\n#@markdown ### Training Parameters\ncontext_length = 2048  #@param {type:\"integer\"}\npretrain_tokens_billions = 50  #@param {type:\"number\"}\n#@markdown *Recommended: 20x model params (1B model = 20B tokens minimum)*\n\n#@markdown ---\n#@markdown ### Data Preparation Settings\ndata_prep_speed = \"fast\"  #@param [\"fast\", \"balanced\", \"thorough\"]\n#@markdown **Data Prep Quality Modes:**\n#@markdown - `fast`: Skip toxicity filter, basic quality only (~30 min for 12M docs)\n#@markdown - `balanced`: Basic quality + toxicity filter (~2-3 hours)\n#@markdown - `thorough`: All quality filters + toxicity (~6-7 hours)\n\nuse_gpu_data_prep = True  #@param {type:\"boolean\"}\n#@markdown **GPU Data Prep (Recommended for A100/H100):**\n#@markdown - `True`: Use RAPIDS cuDF + NeMo Curator (10-30x faster)\n#@markdown - `False`: Use CPU-only pipeline (slower but more compatible)\n\n# ============================================================\n# MODEL TYPE CONFIGURATIONS\n# ============================================================\n\nMODEL_TYPE_CONFIGS = {\n    \"reasoning_agent\": {\n        \"description\": \"Optimized for math, logic, and tool use\",\n        \"pretraining_datasets\": [\n            \"slimpajama\", \"wikipedia\", \"the-stack-python\", \n            \"openwebtext\", \"arxiv\", \"stackexchange\"\n        ],\n        \"sft_datasets\": [\n            \"gsm8k\", \"orca-math\", \"openorca\", \"cot-collection\", \"metamath\",\n            \"glaive-function-calling\", \"hermes-function-calling\", \"toolbench\",\n            \"logiqa\"\n        ],\n        \"dpo_datasets\": [\"hh-rlhf\", \"ultrafeedback\"],\n        \"data_prep_script\": \"06_prepare_reasoning_data.py\",\n        \"eval_benchmarks\": [\"gsm8k\", \"arc\", \"function_calling\", \"safety\"],\n        \"sft_focus\": \"reasoning\",\n    },\n    \"code_assistant\": {\n        \"description\": \"Optimized for code generation and understanding\",\n        \"pretraining_datasets\": [\n            \"the-stack\", \"slimpajama\", \"wikipedia\", \"arxiv\"\n        ],\n        \"sft_datasets\": [\n            \"code-alpaca\", \"python-code-instructions\", \"evol-instruct-code\",\n            \"glaive-function-calling\", \"oasst1\"\n        ],\n        \"dpo_datasets\": [\"hh-rlhf\"],\n        \"data_prep_script\": \"prepare_lora_data.py\",  # Reuse code prep\n        \"eval_benchmarks\": [\"humaneval\", \"mbpp\", \"safety\"],\n        \"sft_focus\": \"code\",\n    },\n    \"general_assistant\": {\n        \"description\": \"Balanced instruction following\",\n        \"pretraining_datasets\": [\n            \"slimpajama\", \"wikipedia\", \"openwebtext\", \"pg19\"\n        ],\n        \"sft_datasets\": [\n            \"oasst1\", \"dolly-15k\", \"alpaca-cleaned\", \"openorca\"\n        ],\n        \"dpo_datasets\": [\"hh-rlhf\", \"ultrafeedback\"],\n        \"data_prep_script\": \"06_prepare_sft_data.py\",\n        \"eval_benchmarks\": [\"mmlu\", \"hellaswag\", \"safety\"],\n        \"sft_focus\": \"instruction\",\n    },\n    \"chat_model\": {\n        \"description\": \"Conversational and helpful\",\n        \"pretraining_datasets\": [\n            \"slimpajama\", \"wikipedia\", \"openwebtext\"\n        ],\n        \"sft_datasets\": [\n            \"oasst1\", \"dolly-15k\", \"sharegpt\"\n        ],\n        \"dpo_datasets\": [\"hh-rlhf\", \"ultrafeedback\"],\n        \"data_prep_script\": \"06_prepare_sft_data.py\",\n        \"eval_benchmarks\": [\"mt-bench\", \"safety\"],\n        \"sft_focus\": \"chat\",\n    },\n}\n\n# Data prep speed configurations (CPU pipeline)\nDATA_PREP_CONFIGS = {\n    \"fast\": {\n        \"flags\": \"--native-pipeline --fast-quality --no-toxicity --fresh\",\n        \"gpu_flags\": \"--fast-quality --no-toxicity\",\n        \"description\": \"Fastest: GopherQuality only, no toxicity filter\",\n    },\n    \"balanced\": {\n        \"flags\": \"--native-pipeline --fast-quality --fresh\",\n        \"gpu_flags\": \"--fast-quality\",\n        \"description\": \"Balanced: GopherQuality + toxicity filter\",\n    },\n    \"thorough\": {\n        \"flags\": \"--native-pipeline --fresh\",\n        \"gpu_flags\": \"\",\n        \"description\": \"Thorough: All quality filters + toxicity\",\n    },\n}\n\n# Size to training time estimates (A100 80GB)\nSIZE_CONFIGS = {\n    \"125m\": {\"pretrain_hours\": 4, \"sft_hours\": 1, \"dpo_hours\": 0.5, \"batch_size\": 32, \"grad_accum\": 1},\n    \"350m\": {\"pretrain_hours\": 8, \"sft_hours\": 2, \"dpo_hours\": 1, \"batch_size\": 16, \"grad_accum\": 2},\n    \"1b\": {\"pretrain_hours\": 50, \"sft_hours\": 12, \"dpo_hours\": 4, \"batch_size\": 16, \"grad_accum\": 4},\n    \"3b\": {\"pretrain_hours\": 100, \"sft_hours\": 24, \"dpo_hours\": 8, \"batch_size\": 8, \"grad_accum\": 8},\n    \"7b\": {\"pretrain_hours\": 200, \"sft_hours\": 48, \"dpo_hours\": 16, \"batch_size\": 8, \"grad_accum\": 4},\n}\n\n# Build configuration\ntype_config = MODEL_TYPE_CONFIGS[model_type]\nsize_config = SIZE_CONFIGS[model_size]\ndata_prep_config = DATA_PREP_CONFIGS[data_prep_speed]\n\nCONFIG = {\n    'model_type': model_type,\n    'model_size': model_size,\n    'context_length': context_length,\n    'pretrain_tokens_b': pretrain_tokens_billions,\n    'batch_size': size_config['batch_size'],\n    'grad_accum': size_config['grad_accum'],\n    'pretraining_datasets': type_config['pretraining_datasets'],\n    'sft_datasets': type_config['sft_datasets'],\n    'dpo_datasets': type_config['dpo_datasets'],\n    'data_prep_script': type_config['data_prep_script'],\n    'eval_benchmarks': type_config['eval_benchmarks'],\n    'sft_focus': type_config['sft_focus'],\n    'data_prep_flags': data_prep_config['flags'],\n    'gpu_data_prep_flags': data_prep_config['gpu_flags'],\n    'data_prep_speed': data_prep_speed,\n    'use_gpu_data_prep': use_gpu_data_prep,\n}\n\n# Calculate training steps\ntokens_per_step = CONFIG['batch_size'] * CONFIG['grad_accum'] * context_length\ntotal_tokens = int(pretrain_tokens_billions * 1e9)\npretrain_steps = total_tokens // tokens_per_step\nCONFIG['pretrain_steps'] = pretrain_steps\n\n# Estimate time\ntime_scale = pretrain_tokens_billions / 50  # Base estimate is for 50B tokens\npretrain_hours = size_config['pretrain_hours'] * time_scale\ntotal_hours = pretrain_hours + size_config['sft_hours'] + size_config['dpo_hours']\n\n# Drive path based on model type\nDRIVE_BASE = f\"/content/drive/MyDrive/llm-{model_size}-{model_type.replace('_', '-')}\"\n\nprint(\"=\" * 60)\nprint(\"MODEL CONFIGURATION\")\nprint(\"=\" * 60)\nprint(f\"\\nType: {model_type.upper().replace('_', ' ')}\")\nprint(f\"  {type_config['description']}\")\nprint(f\"\\nSize: {model_size.upper()}\")\nprint(f\"  Context: {context_length} tokens\")\nprint(f\"  Batch: {CONFIG['batch_size']} x {CONFIG['grad_accum']} = {CONFIG['batch_size'] * CONFIG['grad_accum']} effective\")\nprint(f\"\\nPretraining:\")\nprint(f\"  Tokens: {pretrain_tokens_billions}B\")\nprint(f\"  Steps: {pretrain_steps:,}\")\nprint(f\"\\nData Preparation: {data_prep_speed.upper()}\")\nprint(f\"  {data_prep_config['description']}\")\nprint(f\"  GPU Acceleration: {'ENABLED (10-30x faster)' if use_gpu_data_prep else 'DISABLED'}\")\nprint(f\"\\nDatasets:\")\nprint(f\"  Pretraining: {', '.join(type_config['pretraining_datasets'][:4])}...\")\nprint(f\"  SFT: {', '.join(type_config['sft_datasets'][:4])}...\")\nprint(f\"\\nEstimated Time (A100):\")\nprint(f\"  Pretraining: ~{pretrain_hours:.0f} hours\")\nprint(f\"  SFT: ~{size_config['sft_hours']} hours\")\nprint(f\"  DPO: ~{size_config['dpo_hours']} hours\")\nprint(f\"  Total: ~{total_hours:.0f} hours ({total_hours/24:.1f} days)\")\nprint(f\"\\nStorage: {DRIVE_BASE}\")\nprint(\"=\" * 60)",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 0.4 Set Up Persistent Storage\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(f\"Setting up storage at: {DRIVE_BASE}\")\n",
    "    \n",
    "    # Create Drive directories\n",
    "    for subdir in ['checkpoints', 'data', 'data/raw', 'data/packed', 'data/sft', 'data/dpo', 'logs', 'evals']:\n",
    "        os.makedirs(os.path.join(DRIVE_BASE, subdir), exist_ok=True)\n",
    "    \n",
    "    # Create symlinks\n",
    "    for dir_name in ['checkpoints', 'data', 'logs', 'evals']:\n",
    "        local_path = os.path.join(PROJECT_ROOT, dir_name)\n",
    "        drive_path = os.path.join(DRIVE_BASE, dir_name)\n",
    "        \n",
    "        if os.path.exists(local_path) and not os.path.islink(local_path):\n",
    "            !cp -r {local_path}/* {drive_path}/ 2>/dev/null || true\n",
    "            !rm -rf {local_path}\n",
    "        elif os.path.islink(local_path):\n",
    "            os.unlink(local_path)\n",
    "        \n",
    "        os.symlink(drive_path, local_path)\n",
    "        print(f\"  {dir_name} -> Drive\")\n",
    "    \n",
    "    print(\"\\nStorage ready!\")\n",
    "else:\n",
    "    for d in ['checkpoints', 'data', 'logs', 'evals']:\n",
    "        os.makedirs(d, exist_ok=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 0.5 Check GPU\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    capability = torch.cuda.get_device_capability()\n",
    "    \n",
    "    CONFIG['use_fp8'] = capability[0] >= 9\n",
    "    \n",
    "    print(f\"GPU: {gpu_name} ({gpu_memory:.0f} GB)\")\n",
    "    print(f\"FP8: {'Available' if CONFIG['use_fp8'] else 'Not available'}\")\n",
    "    \n",
    "    # Memory check for model size\n",
    "    min_memory = {'125m': 8, '350m': 16, '1b': 24, '3b': 40, '7b': 70}\n",
    "    if gpu_memory < min_memory[CONFIG['model_size']]:\n",
    "        print(f\"\\nWARNING: {CONFIG['model_size']} may need {min_memory[CONFIG['model_size']]}+ GB\")\n",
    "else:\n",
    "    print(\"No GPU detected!\")\n",
    "    CONFIG['use_fp8'] = False"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Download Data\n",
    "\n",
    "Downloads datasets matched to your model type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 1.1 Download Pretraining Data\n",
    "#@markdown Downloads pretraining corpora for your model type.\n",
    "\n",
    "print(f\"Downloading pretraining data for: {CONFIG['model_type']}\")\n",
    "print(f\"Datasets: {CONFIG['pretraining_datasets']}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "!python scripts/01_download_data.py --phases pretraining"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 1.2 Download SFT & DPO Data\n",
    "#@markdown Downloads fine-tuning data matched to your model type.\n",
    "\n",
    "print(f\"Downloading SFT data for: {CONFIG['model_type']}\")\n",
    "print(f\"SFT datasets: {CONFIG['sft_datasets']}\")\n",
    "print(f\"DPO datasets: {CONFIG['dpo_datasets']}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Download based on model type\n",
    "if CONFIG['sft_focus'] == 'reasoning':\n",
    "    !python scripts/01_download_data.py --phases reasoning function_calling logic instruction_tuning preference_data\n",
    "elif CONFIG['sft_focus'] == 'code':\n",
    "    !python scripts/01_download_data.py --phases instruction_tuning preference_data\n",
    "    # Code data handled by prepare_lora_data.py\n",
    "else:\n",
    "    !python scripts/01_download_data.py --phases instruction_tuning preference_data"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "#@title ### 2.1 Clean & Tokenize Pretraining Data\n#@markdown Uses the data prep speed setting from configuration above.\n#@markdown GPU mode uses RAPIDS cuDF (150x faster text cleaning) + NeMo Curator (16x faster dedup).\n\nuse_gpu = CONFIG.get('use_gpu_data_prep', False)\ndata_prep_speed = CONFIG['data_prep_speed']\n\nprint(f\"Data prep mode: {data_prep_speed.upper()}\")\nprint(f\"GPU Acceleration: {'ENABLED' if use_gpu else 'DISABLED'}\")\nprint(\"=\" * 50)\n\nif use_gpu:\n    # GPU-accelerated pipeline (10-30x faster on A100)\n    gpu_flags = CONFIG['gpu_data_prep_flags']\n    print(\"Using GPU pipeline (RAPIDS cuDF + NeMo Curator)...\")\n    !python scripts/02_gpu_clean_deduplicate.py --input data/raw --output data/processed {gpu_flags}\nelse:\n    # CPU pipeline (original)\n    data_prep_flags = CONFIG['data_prep_flags']\n    print(f\"Using CPU pipeline with flags: {data_prep_flags}\")\n    !python scripts/02_clean_deduplicate_optimized.py {data_prep_flags}\n\n# Tokenize and pack (same for both)\n!python scripts/03_tokenize_and_pack.py",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 2.2 Prepare SFT Data\n",
    "#@markdown Uses the appropriate script for your model type.\n",
    "\n",
    "prep_script = CONFIG['data_prep_script']\n",
    "print(f\"Using: {prep_script} (optimized for {CONFIG['model_type']})\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "!python scripts/{prep_script}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 2.3 Prepare DPO Data\n",
    "\n",
    "!python scripts/08_prepare_dpo_data.py"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 3.1 Initialize Model\n",
    "\n",
    "model_size = CONFIG['model_size']\n",
    "context = CONFIG['context_length']\n",
    "\n",
    "print(f\"Initializing {model_size.upper()} model with {context} context...\")\n",
    "\n",
    "!python scripts/04_init_model.py --size {model_size} --context-length {context}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 3.2 Verify Setup\n",
    "import os\n",
    "\n",
    "checks = [\n",
    "    ('Model', 'checkpoints/init'),\n",
    "    ('Tokenizer', 'configs/tokenizer'),\n",
    "    ('Pretrain data', 'data/packed'),\n",
    "    ('SFT data', 'data/sft/train'),\n",
    "    ('DPO data', 'data/dpo/train'),\n",
    "]\n",
    "\n",
    "print(\"Setup verification:\")\n",
    "all_ok = True\n",
    "for name, path in checks:\n",
    "    ok = os.path.exists(path)\n",
    "    print(f\"  {name}: {'OK' if ok else 'MISSING'}\")\n",
    "    all_ok = all_ok and ok\n",
    "\n",
    "if all_ok:\n",
    "    print(\"\\nReady for training!\")\n",
    "else:\n",
    "    print(\"\\nFix missing items before proceeding.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 4.1 Start Pretraining\n",
    "\n",
    "steps = CONFIG['pretrain_steps']\n",
    "use_fp8 = CONFIG.get('use_fp8', False)\n",
    "\n",
    "cmd = f\"python scripts/05_pretrain.py --max_steps {steps}\"\n",
    "cmd += \" --use-liger-kernel --use-cce --enable-oom-recovery\"\n",
    "if use_fp8:\n",
    "    cmd += \" --fp8\"\n",
    "\n",
    "print(f\"Model: {CONFIG['model_size'].upper()} {CONFIG['model_type']}\")\n",
    "print(f\"Steps: {steps:,}\")\n",
    "print(f\"Command: {cmd}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "!{cmd}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 4.2 Resume (if interrupted)\n",
    "# CHECKPOINT = \"checkpoints/pretrain/checkpoint-10000\"\n",
    "# !python scripts/05_pretrain.py --resume_from_checkpoint {CHECKPOINT}"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: SFT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 5.1 Start SFT\n",
    "\n",
    "cmd = \"python scripts/07_sft.py --use-liger-kernel --enable-oom-recovery\"\n",
    "if CONFIG.get('use_fp8', False):\n",
    "    cmd += \" --fp8\"\n",
    "\n",
    "print(f\"SFT focus: {CONFIG['sft_focus']}\")\n",
    "print(f\"Datasets: {CONFIG['sft_datasets']}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "!{cmd}"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: DPO Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 6.1 Start DPO\n",
    "\n",
    "cmd = \"python scripts/09_dpo.py --enable-oom-recovery\"\n",
    "if CONFIG.get('use_fp8', False):\n",
    "    cmd += \" --fp8\"\n",
    "\n",
    "!{cmd}"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 7.1 Run Evaluation\n",
    "\n",
    "print(f\"Benchmarks for {CONFIG['model_type']}: {CONFIG['eval_benchmarks']}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "!python scripts/11_evaluate.py checkpoints/dpo_final"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 7.2 Check Gates\n",
    "\n",
    "!python scripts/12_check_gates.py dpo"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 8.1 Load Model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "MODEL_PATH = \"checkpoints/dpo_final\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"configs/tokenizer\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"Model loaded!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 8.2 Generate Response\n",
    "\n",
    "def generate(prompt, max_tokens=512):\n",
    "    formatted = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n\"\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_tokens, \n",
    "                                 temperature=0.7, do_sample=True, top_p=0.9)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test prompts based on model type\n",
    "TEST_PROMPTS = {\n",
    "    'reasoning_agent': \"Solve step by step: If a train travels 60 mph for 2.5 hours, how far does it go?\",\n",
    "    'code_assistant': \"Write a Python function to find the nth Fibonacci number.\",\n",
    "    'general_assistant': \"Explain the difference between machine learning and deep learning.\",\n",
    "    'chat_model': \"Hello! How are you today? What can you help me with?\",\n",
    "}\n",
    "\n",
    "prompt = TEST_PROMPTS.get(CONFIG['model_type'], \"Hello, how are you?\")\n",
    "print(f\"Test for {CONFIG['model_type']}:\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"\\nResponse:\")\n",
    "print(generate(prompt))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title ### 8.3 Custom Prompt\n",
    "\n",
    "CUSTOM_PROMPT = \"\"\"Your prompt here\"\"\"  #@param {type:\"string\"}\n",
    "\n",
    "print(generate(CUSTOM_PROMPT))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Complete!\n",
    "\n",
    "Your model is trained and ready to use. Check the evaluation results above to see how it performs on relevant benchmarks."
   ]
  }
 ]
}