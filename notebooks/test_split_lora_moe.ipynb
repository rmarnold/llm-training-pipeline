{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0000000000000000",
   "metadata": {},
   "source": "# Split LoRA MoE Benchmark: 4-bit QLoRA vs bfloat16 Split LoRA\n\n## Purpose\n\nThis notebook benchmarks two LoRA training strategies for the GPT-OSS 20B MoE model side-by-side:\n\n- **4-bit QLoRA (baseline)**: Loads model in 4-bit NF4 quantization. Standard production pipeline configuration. Lower VRAM footprint for weights, but cannot leverage Unsloth's Faster MoE optimization.\n- **bfloat16 Split LoRA**: Loads model in bfloat16. Enables Unsloth's Faster MoE (Split LoRA), which reorders LoRA matrix operations to only compute on routed token-expert pairs — giving **7-12x speedup** and **35%+ VRAM savings** during the forward/backward pass despite using unquantized weights.\n\n## How Split LoRA Works\n\nIn a standard MoE layer, all tokens are processed by the full LoRA adapter even though each token only routes to top-K experts. Unsloth's Faster MoE reorders the LoRA ops so they execute *after* routing: only the subset of (token, expert) pairs that are actually selected compute LoRA deltas. This eliminates ~(1 - top_k/num_experts) of the LoRA flops per MoE layer.\n\n## Expected Results\n\n| Metric | 4-bit QLoRA | bf16 Split LoRA |\n|--------|------------|------------------|\n| Step time | Baseline | **7-12x faster** |\n| Peak VRAM | Baseline | **35%+ lower** |\n| Expert LoRA | Yes (via fix) | Yes |\n| Weight precision | 4-bit NF4 | bfloat16 |\n\nSplit LoRA should show faster step times and lower peak VRAM despite using bfloat16 base weights, because the VRAM savings from skipping unrouted expert LoRA ops outweigh the cost of unquantized weights.\n\n## Reference\nhttps://unsloth.ai/docs/new/faster-moe"
  },
  {
   "cell_type": "markdown",
   "id": "a1000000000000000",
   "metadata": {},
   "source": "## Step 0: Environment Setup"
  },
  {
   "cell_type": "markdown",
   "id": "a2000000000000000",
   "metadata": {},
   "source": "### 0.1 Mount Google Drive & Install Dependencies"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3000000000000000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab detection and setup\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules or os.path.exists(\"/content\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\", force_remount=False)\n",
    "    # Install dependencies\n",
    "    subprocess.run(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "         \"unsloth\", \"transformers\", \"datasets\", \"trl\", \"peft\",\n",
    "         \"accelerate\", \"ipywidgets\", \"matplotlib\"],\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "# Environment variables - must be set before torch import\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "\n",
    "# Path setup\n",
    "sys.path.insert(0, \"scripts\")\n",
    "if IN_COLAB and os.path.exists(\"/content/drive/MyDrive/llm-training-pipeline\"):\n",
    "    os.chdir(\"/content/drive/MyDrive/llm-training-pipeline\")\n",
    "    sys.path.insert(0, \"scripts\")\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Colab: {IN_COLAB}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4000000000000000",
   "metadata": {},
   "source": "### 0.2 Configure Benchmark"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5000000000000000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# @param forms for Colab UI\n",
    "gpu_tier = \"h100_80gb\" #@param [\"a100_40gb\", \"a100_80gb\", \"h100_80gb\"] {type: \"string\"}\n",
    "benchmark_steps = 50 #@param {type: \"integer\"}\n",
    "lora_rank = 64 #@param {type: \"integer\"}\n",
    "max_seq_length = 4096 #@param {type: \"integer\"}\n",
    "include_training_run = True #@param {type: \"boolean\"}\n",
    "\n",
    "GPU_PROFILES = {\n",
    "    \"a100_40gb\": {\n",
    "        \"moe_backend\": \"unsloth_triton\",\n",
    "        \"baseline_batch_size\": 1,\n",
    "        \"split_batch_size\": 1,\n",
    "        \"expected_speedup\": \"7-10x\",\n",
    "        \"bf16_viable\": False,  # 40GB too small for bf16 20B\n",
    "    },\n",
    "    \"a100_80gb\": {\n",
    "        \"moe_backend\": \"unsloth_triton\",\n",
    "        \"baseline_batch_size\": 2,\n",
    "        \"split_batch_size\": 2,\n",
    "        \"expected_speedup\": \"7-10x\",\n",
    "        \"bf16_viable\": True,\n",
    "    },\n",
    "    \"h100_80gb\": {\n",
    "        \"moe_backend\": \"grouped_mm\",\n",
    "        \"baseline_batch_size\": 2,\n",
    "        \"split_batch_size\": 2,\n",
    "        \"expected_speedup\": \"10-12x\",\n",
    "        \"bf16_viable\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "BENCH_CONFIG = {\n",
    "    \"gpu_tier\": gpu_tier,\n",
    "    \"benchmark_steps\": benchmark_steps,\n",
    "    \"lora_rank\": lora_rank,\n",
    "    \"max_seq_length\": max_seq_length,\n",
    "    \"include_training_run\": include_training_run,\n",
    "    **GPU_PROFILES[gpu_tier],\n",
    "}\n",
    "\n",
    "# Shared LoRA config for fair comparison\n",
    "LORA_CONFIG = {\n",
    "    \"r\": lora_rank,\n",
    "    \"lora_alpha\": lora_rank,  # alpha == rank is standard\n",
    "    \"lora_dropout\": 0,\n",
    "    \"bias\": \"none\",\n",
    "    \"use_gradient_checkpointing\": \"unsloth\",\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "os.makedirs(\"/tmp/split_lora_bench\", exist_ok=True)\n",
    "with open(\"/tmp/split_lora_bench/config.json\", \"w\") as f:\n",
    "    json.dump(BENCH_CONFIG, f, indent=2)\n",
    "\n",
    "print(\"Benchmark Configuration:\")\n",
    "for k, v in BENCH_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6000000000000000",
   "metadata": {},
   "source": "### 0.3 Benchmark Tracker"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7000000000000000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class BenchmarkTracker:\n",
    "    \"\"\"Visual progress tracker for benchmark phases.\"\"\"\n",
    "\n",
    "    PHASES = [\n",
    "        (\"gpu_detect\", \"GPU Detection\"),\n",
    "        (\"synthetic_data\", \"Synthetic Data\"),\n",
    "        (\"baseline_load\", \"Baseline: Load Model\"),\n",
    "        (\"baseline_lora\", \"Baseline: Apply LoRA\"),\n",
    "        (\"baseline_bench\", \"Baseline: Benchmark\"),\n",
    "        (\"cleanup\", \"Cleanup\"),\n",
    "        (\"split_load\", \"Split LoRA: Load Model\"),\n",
    "        (\"split_lora\", \"Split LoRA: Apply LoRA\"),\n",
    "        (\"split_bench\", \"Split LoRA: Benchmark\"),\n",
    "        (\"training_run\", \"Training Validation\"),\n",
    "        (\"comparison\", \"Results Comparison\"),\n",
    "        (\"recommendation\", \"Recommendation\"),\n",
    "    ]\n",
    "\n",
    "    def __init__(self):\n",
    "        self._bars = {}\n",
    "        self._labels = {}\n",
    "        rows = []\n",
    "        for key, name in self.PHASES:\n",
    "            label = widgets.HTML(\n",
    "                value=f\"<span style='color:#888'>&#x25CB; {name}</span>\",\n",
    "                layout=widgets.Layout(width=\"260px\"),\n",
    "            )\n",
    "            bar = widgets.FloatProgress(\n",
    "                value=0,\n",
    "                min=0,\n",
    "                max=1.0,\n",
    "                bar_style=\"info\",\n",
    "                layout=widgets.Layout(width=\"300px\", height=\"18px\"),\n",
    "            )\n",
    "            self._bars[key] = bar\n",
    "            self._labels[key] = label\n",
    "            rows.append(widgets.HBox([label, bar]))\n",
    "        self._container = widgets.VBox(rows)\n",
    "        display(widgets.HTML(\"<b>Benchmark Progress</b>\"))\n",
    "        display(self._container)\n",
    "\n",
    "    def start(self, phase):\n",
    "        self._bars[phase].value = 0.1\n",
    "        self._bars[phase].bar_style = \"info\"\n",
    "        name = dict(self.PHASES)[phase]\n",
    "        self._labels[phase].value = f\"<span style='color:#2196F3'>&#x25B6; {name}</span>\"\n",
    "\n",
    "    def complete(self, phase):\n",
    "        self._bars[phase].value = 1.0\n",
    "        self._bars[phase].bar_style = \"success\"\n",
    "        name = dict(self.PHASES)[phase]\n",
    "        self._labels[phase].value = f\"<span style='color:#4CAF50'>&#x2714; {name}</span>\"\n",
    "\n",
    "    def skip(self, phase):\n",
    "        self._bars[phase].value = 1.0\n",
    "        self._bars[phase].bar_style = \"\"\n",
    "        name = dict(self.PHASES)[phase]\n",
    "        self._labels[phase].value = f\"<span style='color:#888'>&#x23ED; {name} (skipped)</span>\"\n",
    "\n",
    "    def fail(self, phase):\n",
    "        self._bars[phase].value = 1.0\n",
    "        self._bars[phase].bar_style = \"danger\"\n",
    "        name = dict(self.PHASES)[phase]\n",
    "        self._labels[phase].value = f\"<span style='color:#F44336'>&#x2718; {name}</span>\"\n",
    "\n",
    "\n",
    "tracker = BenchmarkTracker()\n",
    "\n",
    "# Results accumulator\n",
    "results = {\n",
    "    \"baseline\": {},\n",
    "    \"split_lora\": {},\n",
    "    \"training_validation\": {},\n",
    "    \"verdict\": None,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8000000000000000",
   "metadata": {},
   "source": "### 0.4 Check GPU & Set MoE Backend"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9000000000000000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tracker.start(\"gpu_detect\")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA not available. This notebook requires a GPU.\")\n",
    "\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "gpu_mem_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "bf16_supported = torch.cuda.is_bf16_supported()\n",
    "\n",
    "print(f\"GPU: {gpu_name}\")\n",
    "print(f\"VRAM: {gpu_mem_gb:.1f} GB\")\n",
    "print(f\"BF16 supported: {bf16_supported}\")\n",
    "\n",
    "# Auto-override tier based on detected GPU\n",
    "if \"H100\" in gpu_name or \"H200\" in gpu_name:\n",
    "    detected_tier = \"h100_80gb\"\n",
    "elif \"A100\" in gpu_name:\n",
    "    detected_tier = \"a100_80gb\" if gpu_mem_gb > 50 else \"a100_40gb\"\n",
    "else:\n",
    "    detected_tier = gpu_tier  # Keep user selection\n",
    "    print(f\"  Unknown GPU, keeping user-selected tier: {gpu_tier}\")\n",
    "\n",
    "if detected_tier != gpu_tier:\n",
    "    print(f\"  Auto-overriding tier: {gpu_tier} -> {detected_tier}\")\n",
    "    gpu_tier = detected_tier\n",
    "    BENCH_CONFIG.update(GPU_PROFILES[detected_tier])\n",
    "    BENCH_CONFIG[\"gpu_tier\"] = detected_tier\n",
    "\n",
    "# CRITICAL: Set MoE backend BEFORE any unsloth import\n",
    "moe_backend = BENCH_CONFIG[\"moe_backend\"]\n",
    "os.environ[\"UNSLOTH_MOE_BACKEND\"] = moe_backend\n",
    "print(f\"MoE backend: {moe_backend}\")\n",
    "\n",
    "if not bf16_supported:\n",
    "    raise RuntimeError(\"BF16 not supported on this GPU. Split LoRA requires BF16.\")\n",
    "\n",
    "if not BENCH_CONFIG[\"bf16_viable\"]:\n",
    "    print(f\"\\nWARNING: {detected_tier} may not have enough VRAM for bf16 GPT-OSS 20B.\")\n",
    "    print(\"Baseline (4-bit) will run but Split LoRA (bf16) may OOM.\")\n",
    "\n",
    "tracker.complete(\"gpu_detect\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10000000000000000",
   "metadata": {},
   "source": "## Step 1: Prepare Synthetic Benchmark Data\n\nWe generate 200 synthetic multi-turn coding agent conversations in Harmony format. These simulate real agent sessions with tool calls, thinking traces, and patch operations — the same distribution used in TUI training.\n\nThe pipeline's `encode_harmony_messages` formatter is used if available in the path. If the import fails (e.g. running in a fresh Colab without the repo), a manual ChatML-style fallback is used instead. Both produce valid training text, but the Harmony formatter adds structured tool call tokens and `<think>` blocks matching the model's training distribution."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11000000000000000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import random\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "tracker.start(\"synthetic_data\")\n",
    "\n",
    "NUM_SAMPLES = 200\n",
    "\n",
    "# Synthetic conversation templates\n",
    "TEMPLATES = [\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"developer\", \"content\": \"You are a coding assistant. Use tools to read and modify files.\"},\n",
    "            {\"role\": \"user\", \"content\": \"There's a bug in {file}. The {feature} isn't working correctly.\"},\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"thinking\": \"Let me read the file to understand the issue.\",\n",
    "                \"tool_calls\": [\n",
    "                    {\"id\": \"1\", \"type\": \"function\", \"function\": {\"name\": \"read_file\", \"arguments\": '{{\"path\": \"{file}\"}}'}}  # noqa: E501\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"tool_call_id\": \"1\", \"content\": \"fn main() {{\\n    let data = vec![1, 2, 3];\\n    println!(\\\"{{:?}}\\\", data);\\n}}\"},\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"thinking\": \"I see the issue. The {feature} needs to be fixed by {fix}.\",\n",
    "                \"content\": \"I found the bug. The {feature} in `{file}` needs {fix}. Let me apply the patch.\",\n",
    "                \"tool_calls\": [\n",
    "                    {\"id\": \"2\", \"type\": \"function\", \"function\": {\"name\": \"write_file\", \"arguments\": '{{\"path\": \"{file}\", \"content\": \"fn main() {{\\\\n    let data = vec![1, 2, 3, 4];\\\\n    println!(\\\\\"{{:?}}\\\\\", data);\\\\n}}\"}}'}}  # noqa: E501\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"tool_call_id\": \"2\", \"content\": \"File written successfully.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"I've fixed the {feature} in `{file}`. The issue was {fix}.\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"developer\", \"content\": \"You are a debugging assistant with access to shell and file tools.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Run the tests for {project} and fix any failures.\"},\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"thinking\": \"I'll run the test suite first to identify failures.\",\n",
    "                \"tool_calls\": [\n",
    "                    {\"id\": \"1\", \"type\": \"function\", \"function\": {\"name\": \"run_command\", \"arguments\": '{\"command\": \"cargo test\"}'}}\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"tool_call_id\": \"1\", \"content\": \"test {test_name} ... FAILED\\n\\nfailures:\\n    {test_name}: assertion failed: expected {expected}, got {actual}\"},\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Test `{test_name}` failed. The assertion expected `{expected}` but got `{actual}`. Let me fix this.\",\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "files = [\"main.rs\", \"lib.rs\", \"utils.rs\", \"handler.rs\", \"config.rs\", \"parser.rs\", \"server.rs\", \"client.rs\"]\n",
    "features = [\"error handling\", \"input validation\", \"data parsing\", \"connection pooling\", \"cache invalidation\", \"rate limiting\"]\n",
    "fixes = [\"adding a bounds check\", \"handling the None case\", \"fixing the off-by-one error\", \"adding timeout logic\", \"correcting the regex pattern\"]\n",
    "projects = [\"web-server\", \"cli-tool\", \"data-pipeline\", \"api-gateway\", \"task-runner\"]\n",
    "test_names = [\"test_parse_input\", \"test_handle_request\", \"test_validate_config\", \"test_connection\", \"test_cache_hit\"]\n",
    "\n",
    "samples = []\n",
    "for i in range(NUM_SAMPLES):\n",
    "    template = random.choice(TEMPLATES)\n",
    "    messages = copy.deepcopy(template[\"messages\"])\n",
    "\n",
    "    # String substitution in all message fields\n",
    "    subs = {\n",
    "        \"file\": random.choice(files),\n",
    "        \"feature\": random.choice(features),\n",
    "        \"fix\": random.choice(fixes),\n",
    "        \"project\": random.choice(projects),\n",
    "        \"test_name\": random.choice(test_names),\n",
    "        \"expected\": str(random.randint(1, 100)),\n",
    "        \"actual\": str(random.randint(1, 100)),\n",
    "    }\n",
    "\n",
    "    for msg in messages:\n",
    "        if \"content\" in msg and msg[\"content\"]:\n",
    "            msg[\"content\"] = msg[\"content\"].format(**subs)\n",
    "        if \"thinking\" in msg and msg[\"thinking\"]:\n",
    "            msg[\"thinking\"] = msg[\"thinking\"].format(**subs)\n",
    "        if \"tool_calls\" in msg:\n",
    "            for tc in msg[\"tool_calls\"]:\n",
    "                tc[\"function\"][\"arguments\"] = tc[\"function\"][\"arguments\"].format(**subs)\n",
    "\n",
    "    # Try Harmony formatter, fall back to manual\n",
    "    try:\n",
    "        from dataset_formatters.harmony import encode_harmony_messages\n",
    "        text = encode_harmony_messages(messages, developer_instructions=None, reasoning_effort=\"medium\")\n",
    "    except ImportError:\n",
    "        # Manual fallback: simple chatml-style\n",
    "        parts = []\n",
    "        for msg in messages:\n",
    "            role = msg[\"role\"]\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            if msg.get(\"thinking\"):\n",
    "                content = f\"<think>{msg['thinking']}</think>\\n{content}\"\n",
    "            if msg.get(\"tool_calls\"):\n",
    "                tc_str = json.dumps(msg[\"tool_calls\"])\n",
    "                content = f\"{content}\\n<tool_calls>{tc_str}</tool_calls>\" if content else f\"<tool_calls>{tc_str}</tool_calls>\"\n",
    "            parts.append(f\"<|im_start|>{role}\\n{content}<|im_end|>\")\n",
    "        text = \"\\n\".join(parts) + \"<|endoftext|>\"\n",
    "\n",
    "    samples.append({\"text\": text})\n",
    "\n",
    "benchmark_dataset = Dataset.from_list(samples)\n",
    "print(f\"Created {len(benchmark_dataset)} synthetic samples\")\n",
    "print(f\"Sample length (chars): {len(samples[0]['text'])}\")\n",
    "\n",
    "tracker.complete(\"synthetic_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12000000000000000",
   "metadata": {},
   "source": "## Step 2: Baseline Measurement (4-bit QLoRA)\n\nThe baseline uses the standard 4-bit NF4 quantized approach with QLoRA, matching the current production pipeline configuration in `configs/gpt_oss_20b.py`. This is the reference point for all speedup and VRAM savings calculations."
  },
  {
   "cell_type": "markdown",
   "id": "a13000000000000000",
   "metadata": {},
   "source": "### 2.0 Benchmark Utilities"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14000000000000000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "\n",
    "\n",
    "def get_vram_gb():\n",
    "    \"\"\"Current VRAM usage in GB.\"\"\"\n",
    "    return torch.cuda.memory_allocated() / (1024**3)\n",
    "\n",
    "\n",
    "def get_vram_peak_gb():\n",
    "    \"\"\"Peak VRAM usage in GB.\"\"\"\n",
    "    return torch.cuda.max_memory_allocated() / (1024**3)\n",
    "\n",
    "\n",
    "def reset_vram_stats():\n",
    "    \"\"\"Reset peak VRAM tracking.\"\"\"\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def cleanup_model(*objects):\n",
    "    \"\"\"Delete model objects and free VRAM.\"\"\"\n",
    "    for obj in objects:\n",
    "        del obj\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"VRAM after cleanup: {get_vram_gb():.2f} GB\")\n",
    "\n",
    "\n",
    "def print_measurement(label, data):\n",
    "    \"\"\"Print a benchmark measurement.\"\"\"\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(f\"  {label}\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "    for k, v in data.items():\n",
    "        if isinstance(v, float):\n",
    "            print(f\"  {k}: {v:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15000000000000000",
   "metadata": {},
   "source": "### 2.1 Load Baseline Model (4-bit QLoRA)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16000000000000000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_lib.unsloth_utils import (\n",
    "    apply_lora_config,\n",
    "    detect_moe_experts,\n",
    "    load_unsloth_model,\n",
    "    print_trainable_params,\n",
    "    verify_expert_lora,\n",
    ")\n",
    "\n",
    "tracker.start(\"baseline_load\")\n",
    "reset_vram_stats()\n",
    "\n",
    "print(\"Loading model with 4-bit quantization (QLoRA baseline)...\")\n",
    "t0 = time.perf_counter()\n",
    "baseline_model, tokenizer = load_unsloth_model(\n",
    "    max_seq_length=BENCH_CONFIG[\"max_seq_length\"],\n",
    "    load_in_4bit=True,\n",
    "    tiled_mlp=True,\n",
    "    offload_embedding=True,\n",
    ")\n",
    "load_time = time.perf_counter() - t0\n",
    "\n",
    "results[\"baseline\"][\"load_time_s\"] = load_time\n",
    "results[\"baseline\"][\"vram_after_load_gb\"] = get_vram_gb()\n",
    "print(f\"Load time: {load_time:.1f}s\")\n",
    "print(f\"VRAM after load: {get_vram_gb():.2f} GB\")\n",
    "\n",
    "# Detect MoE structure\n",
    "moe_info = detect_moe_experts(baseline_model)\n",
    "print(f\"MoE detected: {moe_info['is_moe']}\")\n",
    "if moe_info[\"is_moe\"]:\n",
    "    print(f\"  Experts: {moe_info['num_experts']}\")\n",
    "    print(f\"  Expert params: {moe_info['expert_param_count']:,}\")\n",
    "\n",
    "tracker.complete(\"baseline_load\")\n",
    "\n",
    "# Apply LoRA\n",
    "tracker.start(\"baseline_lora\")\n",
    "print(\"\\nApplying LoRA config...\")\n",
    "baseline_model = apply_lora_config(baseline_model, LORA_CONFIG, auto_detect_moe=True)\n",
    "\n",
    "lora_result = verify_expert_lora(baseline_model)\n",
    "results[\"baseline\"][\"has_expert_lora\"] = lora_result[\"has_expert_lora\"]\n",
    "results[\"baseline\"][\"expert_lora_params\"] = lora_result[\"expert_lora_params\"]\n",
    "results[\"baseline\"][\"attention_lora_params\"] = lora_result[\"attention_lora_params\"]\n",
    "results[\"baseline\"][\"vram_after_lora_gb\"] = get_vram_gb()\n",
    "\n",
    "print(f\"Expert LoRA active: {lora_result['has_expert_lora']}\")\n",
    "print(f\"Expert LoRA params: {lora_result['expert_lora_params']:,}\")\n",
    "print(f\"Attention LoRA params: {lora_result['attention_lora_params']:,}\")\n",
    "print_trainable_params(baseline_model)\n",
    "\n",
    "tracker.complete(\"baseline_lora\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17000000000000000",
   "metadata": {},
   "source": "### 2.2 Benchmark Baseline"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18000000000000000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "tracker.start(\"baseline_bench\")\n",
    "reset_vram_stats()\n",
    "\n",
    "print(f\"Running {BENCH_CONFIG['benchmark_steps']} benchmark steps (4-bit QLoRA)...\")\n",
    "print(\"First few steps may be slower (compilation warmup).\\n\")\n",
    "\n",
    "baseline_training_args = SFTConfig(\n",
    "    output_dir=\"/tmp/split_lora_bench/baseline\",\n",
    "    max_steps=BENCH_CONFIG[\"benchmark_steps\"],\n",
    "    per_device_train_batch_size=BENCH_CONFIG[\"baseline_batch_size\"],\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=5,\n",
    "    logging_steps=5,\n",
    "    bf16=True,\n",
    "    optim=\"adamw_8bit\",\n",
    "    seed=42,\n",
    "    max_seq_length=BENCH_CONFIG[\"max_seq_length\"],\n",
    "    packing=True,\n",
    "    dataset_text_field=\"text\",\n",
    "    report_to=\"none\",\n",
    "    save_strategy=\"no\",\n",
    ")\n",
    "\n",
    "baseline_trainer = SFTTrainer(\n",
    "    model=baseline_model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=benchmark_dataset,\n",
    "    args=baseline_training_args,\n",
    ")\n",
    "\n",
    "# Warmup: 3 steps not timed\n",
    "print(\"Warmup (3 steps)...\")\n",
    "baseline_training_args.max_steps = 3\n",
    "baseline_trainer.train()\n",
    "\n",
    "# Reset for actual benchmark\n",
    "reset_vram_stats()\n",
    "baseline_training_args.max_steps = BENCH_CONFIG[\"benchmark_steps\"]\n",
    "baseline_trainer.args.max_steps = BENCH_CONFIG[\"benchmark_steps\"]\n",
    "\n",
    "print(f\"Benchmarking ({BENCH_CONFIG['benchmark_steps']} steps)...\")\n",
    "t0 = time.perf_counter()\n",
    "baseline_trainer.train()\n",
    "wall_time = time.perf_counter() - t0\n",
    "\n",
    "# Extract metrics\n",
    "log_history = baseline_trainer.state.log_history\n",
    "losses = [e[\"loss\"] for e in log_history if \"loss\" in e]\n",
    "\n",
    "results[\"baseline\"][\"wall_time_s\"] = wall_time\n",
    "results[\"baseline\"][\"avg_step_time_s\"] = wall_time / BENCH_CONFIG[\"benchmark_steps\"]\n",
    "results[\"baseline\"][\"peak_vram_gb\"] = get_vram_peak_gb()\n",
    "results[\"baseline\"][\"loss_start\"] = losses[0] if losses else None\n",
    "results[\"baseline\"][\"loss_end\"] = losses[-1] if losses else None\n",
    "\n",
    "print_measurement(\"Baseline (4-bit QLoRA)\", results[\"baseline\"])\n",
    "\n",
    "tracker.complete(\"baseline_bench\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19000000000000000",
   "metadata": {},
   "source": "### 2.3 Cleanup Baseline Model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20000000000000000",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.start(\"cleanup\")\n",
    "cleanup_model(baseline_trainer, baseline_model)\n",
    "del baseline_trainer, baseline_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"VRAM after full cleanup: {get_vram_gb():.2f} GB\")\n",
    "tracker.complete(\"cleanup\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21000000000000000",
   "metadata": {},
   "source": "## Step 3: Split LoRA Measurement (bfloat16)\n\nThe key difference from the baseline is `load_in_4bit=False` — the model loads in bfloat16 with Split LoRA. Unsloth's Faster MoE reorders LoRA matrix operations to only compute on the routed (token, expert) pairs selected during MoE dispatch, rather than broadcasting across all experts.\n\n**First run triggers Triton kernel autotuning (~2 minutes)**, which is excluded from benchmark timing by running 3 warmup steps before the timed window. Subsequent runs on the same Colab session will skip autotuning."
  },
  {
   "cell_type": "markdown",
   "id": "a22000000000000000",
   "metadata": {},
   "source": "### 3.1 Load Model with bfloat16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23000000000000000",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.start(\"split_load\")\n",
    "reset_vram_stats()\n",
    "\n",
    "print(\"Loading model in bfloat16 (Split LoRA mode)...\")\n",
    "print(\"This uses more VRAM than 4-bit but enables Faster MoE optimization.\\n\")\n",
    "\n",
    "try:\n",
    "    t0 = time.perf_counter()\n",
    "    split_model, tokenizer = load_unsloth_model(\n",
    "        max_seq_length=BENCH_CONFIG[\"max_seq_length\"],\n",
    "        load_in_4bit=False,\n",
    "        dtype=torch.bfloat16,\n",
    "        tiled_mlp=True,\n",
    "        offload_embedding=True,\n",
    "    )\n",
    "    load_time = time.perf_counter() - t0\n",
    "\n",
    "    results[\"split_lora\"][\"load_time_s\"] = load_time\n",
    "    results[\"split_lora\"][\"vram_after_load_gb\"] = get_vram_gb()\n",
    "    print(f\"Load time: {load_time:.1f}s\")\n",
    "    print(f\"VRAM after load: {get_vram_gb():.2f} GB\")\n",
    "\n",
    "    tracker.complete(\"split_load\")\n",
    "    split_load_ok = True\n",
    "\n",
    "except torch.cuda.OutOfMemoryError:\n",
    "    print(\"\\nOOM: Not enough VRAM for bfloat16 model.\")\n",
    "    print(f\"GPU has {gpu_mem_gb:.0f} GB but bf16 GPT-OSS 20B needs ~40 GB for weights alone.\")\n",
    "    print(\"Split LoRA benchmark cannot proceed on this GPU.\")\n",
    "    results[\"split_lora\"][\"error\"] = \"OOM on model load\"\n",
    "    tracker.fail(\"split_load\")\n",
    "    tracker.skip(\"split_lora\")\n",
    "    tracker.skip(\"split_bench\")\n",
    "    split_load_ok = False\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError loading model: {e}\")\n",
    "    results[\"split_lora\"][\"error\"] = str(e)\n",
    "    tracker.fail(\"split_load\")\n",
    "    tracker.skip(\"split_lora\")\n",
    "    tracker.skip(\"split_bench\")\n",
    "    split_load_ok = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24000000000000000",
   "metadata": {},
   "source": "### 3.2 Benchmark Split LoRA"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25000000000000000",
   "metadata": {},
   "outputs": [],
   "source": [
    "if split_load_ok:\n",
    "    # Apply LoRA (same config as baseline for fair comparison)\n",
    "    tracker.start(\"split_lora\")\n",
    "    print(\"Applying LoRA config (same as baseline)...\")\n",
    "    split_model = apply_lora_config(split_model, LORA_CONFIG, auto_detect_moe=True)\n",
    "\n",
    "    lora_result = verify_expert_lora(split_model)\n",
    "    results[\"split_lora\"][\"has_expert_lora\"] = lora_result[\"has_expert_lora\"]\n",
    "    results[\"split_lora\"][\"expert_lora_params\"] = lora_result[\"expert_lora_params\"]\n",
    "    results[\"split_lora\"][\"attention_lora_params\"] = lora_result[\"attention_lora_params\"]\n",
    "    results[\"split_lora\"][\"vram_after_lora_gb\"] = get_vram_gb()\n",
    "\n",
    "    print(f\"Expert LoRA active: {lora_result['has_expert_lora']}\")\n",
    "    print(f\"Expert LoRA params: {lora_result['expert_lora_params']:,}\")\n",
    "    print_trainable_params(split_model)\n",
    "    tracker.complete(\"split_lora\")\n",
    "\n",
    "    # Benchmark\n",
    "    tracker.start(\"split_bench\")\n",
    "    reset_vram_stats()\n",
    "\n",
    "    print(f\"\\nRunning {BENCH_CONFIG['benchmark_steps']} benchmark steps (bfloat16 Split LoRA)...\")\n",
    "    print(\"First run triggers Triton autotuning (~2 min), excluded from timing.\\n\")\n",
    "\n",
    "    split_training_args = SFTConfig(\n",
    "        output_dir=\"/tmp/split_lora_bench/split_lora\",\n",
    "        max_steps=BENCH_CONFIG[\"benchmark_steps\"],\n",
    "        per_device_train_batch_size=BENCH_CONFIG[\"split_batch_size\"],\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_steps=5,\n",
    "        logging_steps=5,\n",
    "        bf16=True,\n",
    "        optim=\"adamw_8bit\",\n",
    "        seed=42,\n",
    "        max_seq_length=BENCH_CONFIG[\"max_seq_length\"],\n",
    "        packing=True,\n",
    "        dataset_text_field=\"text\",\n",
    "        report_to=\"none\",\n",
    "        save_strategy=\"no\",\n",
    "    )\n",
    "\n",
    "    split_trainer = SFTTrainer(\n",
    "        model=split_model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=benchmark_dataset,\n",
    "        args=split_training_args,\n",
    "    )\n",
    "\n",
    "    # Warmup: includes Triton autotune, not timed\n",
    "    print(\"Warmup + Triton autotune (3 steps, not timed)...\")\n",
    "    split_training_args.max_steps = 3\n",
    "    split_trainer.train()\n",
    "\n",
    "    # Reset for actual benchmark\n",
    "    reset_vram_stats()\n",
    "    split_training_args.max_steps = BENCH_CONFIG[\"benchmark_steps\"]\n",
    "    split_trainer.args.max_steps = BENCH_CONFIG[\"benchmark_steps\"]\n",
    "\n",
    "    print(f\"Benchmarking ({BENCH_CONFIG['benchmark_steps']} steps)...\")\n",
    "    t0 = time.perf_counter()\n",
    "    split_trainer.train()\n",
    "    wall_time = time.perf_counter() - t0\n",
    "\n",
    "    log_history = split_trainer.state.log_history\n",
    "    losses = [e[\"loss\"] for e in log_history if \"loss\" in e]\n",
    "\n",
    "    results[\"split_lora\"][\"wall_time_s\"] = wall_time\n",
    "    results[\"split_lora\"][\"avg_step_time_s\"] = wall_time / BENCH_CONFIG[\"benchmark_steps\"]\n",
    "    results[\"split_lora\"][\"peak_vram_gb\"] = get_vram_peak_gb()\n",
    "    results[\"split_lora\"][\"loss_start\"] = losses[0] if losses else None\n",
    "    results[\"split_lora\"][\"loss_end\"] = losses[-1] if losses else None\n",
    "\n",
    "    print_measurement(\"Split LoRA (bfloat16)\", results[\"split_lora\"])\n",
    "    tracker.complete(\"split_bench\")\n",
    "else:\n",
    "    print(\"Skipping Split LoRA benchmark (model load failed).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26000000000000000",
   "metadata": {},
   "source": "## Step 4: Short SFT Training Run (Validation)\n\nRuns 100 steps with `logging_steps=1` to confirm the Split LoRA model converges properly. Three quality gate checks are applied:\n\n1. **No NaN/Inf losses**: Training instability check — any NaN or Inf terminates the gate.\n2. **Loss decreasing**: Compares average of first 10 vs last 10 logged losses. Expected: last_10 < first_10.\n3. **Expert gradient flow**: Verifies that expert layer parameters receive gradients, confirming Split LoRA is training the MoE experts and not just the attention layers."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27000000000000000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "first_10_avg = None\n",
    "last_10_avg = None\n",
    "\n",
    "if split_load_ok and BENCH_CONFIG[\"include_training_run\"]:\n",
    "    tracker.start(\"training_run\")\n",
    "\n",
    "    VALIDATION_STEPS = 100\n",
    "    print(f\"Running {VALIDATION_STEPS}-step training validation...\")\n",
    "\n",
    "    val_args = SFTConfig(\n",
    "        output_dir=\"/tmp/split_lora_bench/validation\",\n",
    "        max_steps=VALIDATION_STEPS,\n",
    "        per_device_train_batch_size=BENCH_CONFIG[\"split_batch_size\"],\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_steps=10,\n",
    "        logging_steps=1,\n",
    "        bf16=True,\n",
    "        optim=\"adamw_8bit\",\n",
    "        seed=42,\n",
    "        max_seq_length=BENCH_CONFIG[\"max_seq_length\"],\n",
    "        packing=True,\n",
    "        dataset_text_field=\"text\",\n",
    "        report_to=\"none\",\n",
    "        save_strategy=\"no\",\n",
    "    )\n",
    "\n",
    "    # Reuse split_model if still loaded, otherwise note skip\n",
    "    val_trainer = SFTTrainer(\n",
    "        model=split_model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=benchmark_dataset,\n",
    "        args=val_args,\n",
    "    )\n",
    "\n",
    "    val_trainer.train()\n",
    "\n",
    "    # Extract losses\n",
    "    val_losses = [e[\"loss\"] for e in val_trainer.state.log_history if \"loss\" in e]\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"  QUALITY GATE: Training Validation\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    checks = {}\n",
    "\n",
    "    # Check 1: No NaN/Inf\n",
    "    nan_count = sum(1 for l in val_losses if math.isnan(l) or math.isinf(l))\n",
    "    checks[\"no_nan_inf\"] = nan_count == 0\n",
    "    print(f\"  {'PASS' if checks['no_nan_inf'] else 'FAIL'} - No NaN/Inf losses ({nan_count} found)\")\n",
    "\n",
    "    # Check 2: Loss decreasing (first 10 avg vs last 10 avg)\n",
    "    if len(val_losses) >= 20:\n",
    "        first_10_avg = sum(val_losses[:10]) / 10\n",
    "        last_10_avg = sum(val_losses[-10:]) / 10\n",
    "        loss_decreased = last_10_avg < first_10_avg\n",
    "        checks[\"loss_decreasing\"] = loss_decreased\n",
    "        improvement_pct = (first_10_avg - last_10_avg) / first_10_avg * 100\n",
    "        print(f\"  {'PASS' if loss_decreased else 'FAIL'} - Loss decreasing: {first_10_avg:.4f} -> {last_10_avg:.4f} ({improvement_pct:+.1f}%)\")\n",
    "    else:\n",
    "        checks[\"loss_decreasing\"] = None\n",
    "        print(f\"  SKIP - Not enough loss entries ({len(val_losses)}) for trend analysis\")\n",
    "\n",
    "    # Check 3: Expert gradient flow\n",
    "    expert_has_grad = False\n",
    "    for name, param in split_model.named_parameters():\n",
    "        if \"expert\" in name.lower() and param.requires_grad and param.grad is not None:\n",
    "            if param.grad.abs().sum() > 0:\n",
    "                expert_has_grad = True\n",
    "                break\n",
    "    checks[\"expert_gradient_flow\"] = expert_has_grad\n",
    "    print(f\"  {'PASS' if expert_has_grad else 'WARN'} - Expert gradient flow {'detected' if expert_has_grad else 'not detected (may be cleared after step)'}\")\n",
    "\n",
    "    # Overall verdict\n",
    "    critical_passed = checks[\"no_nan_inf\"] and (checks[\"loss_decreasing\"] is not False)\n",
    "    results[\"training_validation\"] = {\n",
    "        \"steps\": VALIDATION_STEPS,\n",
    "        \"losses\": val_losses,\n",
    "        \"checks\": checks,\n",
    "        \"passed\": critical_passed,\n",
    "        \"first_10_avg\": first_10_avg,\n",
    "        \"last_10_avg\": last_10_avg,\n",
    "    }\n",
    "\n",
    "    print(f\"\\n  VERDICT: {'PASS' if critical_passed else 'FAIL'}\")\n",
    "\n",
    "    if critical_passed:\n",
    "        tracker.complete(\"training_run\")\n",
    "    else:\n",
    "        tracker.fail(\"training_run\")\n",
    "\n",
    "    # Cleanup validation trainer\n",
    "    del val_trainer\n",
    "\n",
    "elif not BENCH_CONFIG[\"include_training_run\"]:\n",
    "    tracker.skip(\"training_run\")\n",
    "    print(\"Training validation skipped (include_training_run=False)\")\n",
    "else:\n",
    "    tracker.skip(\"training_run\")\n",
    "    print(\"Training validation skipped (Split LoRA model load failed)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28000000000000000",
   "metadata": {},
   "source": "## Step 5: Results Comparison"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29000000000000000",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.start(\"comparison\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"  BENCHMARK RESULTS: 4-bit QLoRA vs bfloat16 Split LoRA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Side-by-side table\n",
    "metrics = [\n",
    "    (\"Peak VRAM (GB)\", \"peak_vram_gb\", \".2f\"),\n",
    "    (\"Avg Step Time (s)\", \"avg_step_time_s\", \".3f\"),\n",
    "    (\"VRAM After Load (GB)\", \"vram_after_load_gb\", \".2f\"),\n",
    "    (\"VRAM After LoRA (GB)\", \"vram_after_lora_gb\", \".2f\"),\n",
    "    (\"Load Time (s)\", \"load_time_s\", \".1f\"),\n",
    "    (\"Expert LoRA Params\", \"expert_lora_params\", \",\"),\n",
    "    (\"Attention LoRA Params\", \"attention_lora_params\", \",\"),\n",
    "    (\"Expert LoRA Active\", \"has_expert_lora\", \"\"),\n",
    "    (\"Loss (start)\", \"loss_start\", \".4f\"),\n",
    "    (\"Loss (end)\", \"loss_end\", \".4f\"),\n",
    "]\n",
    "\n",
    "header = f\"{'Metric':<30} {'4-bit QLoRA':>18} {'bf16 Split LoRA':>18} {'Delta':>12}\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for label, key, fmt in metrics:\n",
    "    b_val = results[\"baseline\"].get(key)\n",
    "    s_val = results[\"split_lora\"].get(key)\n",
    "\n",
    "    if b_val is None:\n",
    "        b_str = \"N/A\"\n",
    "    elif fmt == \",\":\n",
    "        b_str = f\"{b_val:,}\"\n",
    "    elif fmt == \"\":\n",
    "        b_str = str(b_val)\n",
    "    else:\n",
    "        b_str = f\"{b_val:{fmt}}\"\n",
    "\n",
    "    if s_val is None:\n",
    "        s_str = \"N/A\"\n",
    "    elif fmt == \",\":\n",
    "        s_str = f\"{s_val:,}\"\n",
    "    elif fmt == \"\":\n",
    "        s_str = str(s_val)\n",
    "    else:\n",
    "        s_str = f\"{s_val:{fmt}}\"\n",
    "\n",
    "    # Calculate delta for numeric values\n",
    "    delta_str = \"\"\n",
    "    if isinstance(b_val, (int, float)) and isinstance(s_val, (int, float)) and b_val != 0:\n",
    "        if \"time\" in key.lower() or \"vram\" in key.lower():\n",
    "            ratio = s_val / b_val\n",
    "            delta_str = f\"{ratio:.2f}x\"\n",
    "\n",
    "    print(f\"{label:<30} {b_str:>18} {s_str:>18} {delta_str:>12}\")\n",
    "\n",
    "# Speedup headline\n",
    "b_step = results[\"baseline\"].get(\"avg_step_time_s\")\n",
    "s_step = results[\"split_lora\"].get(\"avg_step_time_s\")\n",
    "if b_step and s_step and s_step > 0:\n",
    "    speedup = b_step / s_step\n",
    "    print(f\"\\nSpeedup: {speedup:.1f}x {'faster' if speedup > 1 else 'slower'} with Split LoRA\")\n",
    "    results[\"speedup\"] = speedup\n",
    "\n",
    "b_vram = results[\"baseline\"].get(\"peak_vram_gb\")\n",
    "s_vram = results[\"split_lora\"].get(\"peak_vram_gb\")\n",
    "if b_vram and s_vram:\n",
    "    vram_savings = (b_vram - s_vram) / b_vram * 100\n",
    "    print(f\"VRAM savings: {vram_savings:+.1f}% with Split LoRA\")\n",
    "    results[\"vram_savings_pct\"] = vram_savings\n",
    "\n",
    "# Optional matplotlib chart\n",
    "try:\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Step time comparison\n",
    "    if b_step and s_step:\n",
    "        bars = axes[0].bar([\"4-bit QLoRA\", \"bf16 Split LoRA\"], [b_step, s_step], color=[\"#2196F3\", \"#4CAF50\"])\n",
    "        axes[0].set_ylabel(\"Avg Step Time (s)\")\n",
    "        axes[0].set_title(\"Training Step Time\")\n",
    "        for bar, val in zip(bars, [b_step, s_step]):\n",
    "            axes[0].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01, f\"{val:.3f}s\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    # VRAM comparison\n",
    "    if b_vram and s_vram:\n",
    "        bars = axes[1].bar([\"4-bit QLoRA\", \"bf16 Split LoRA\"], [b_vram, s_vram], color=[\"#2196F3\", \"#4CAF50\"])\n",
    "        axes[1].set_ylabel(\"Peak VRAM (GB)\")\n",
    "        axes[1].set_title(\"Peak VRAM Usage\")\n",
    "        for bar, val in zip(bars, [b_vram, s_vram]):\n",
    "            axes[1].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.2, f\"{val:.1f} GB\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"/tmp/split_lora_bench/comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Chart saved to /tmp/split_lora_bench/comparison.png\")\n",
    "except ImportError:\n",
    "    print(\"(matplotlib not available, skipping chart)\")\n",
    "\n",
    "tracker.complete(\"comparison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30000000000000000",
   "metadata": {},
   "source": "## Step 6: Recommendation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31000000000000000",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.start(\"recommendation\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"  RECOMMENDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Determine viability\n",
    "speedup = results.get(\"speedup\", 0)\n",
    "expert_lora_ok = results.get(\"split_lora\", {}).get(\"has_expert_lora\", False)\n",
    "training_ok = results.get(\"training_validation\", {}).get(\"passed\", None)\n",
    "has_error = \"error\" in results.get(\"split_lora\", {})\n",
    "\n",
    "viable = (\n",
    "    speedup >= 1.5\n",
    "    and expert_lora_ok\n",
    "    and training_ok is not False  # None (skipped) is acceptable\n",
    "    and not has_error\n",
    ")\n",
    "\n",
    "results[\"verdict\"] = \"VIABLE\" if viable else \"NOT VIABLE\"\n",
    "\n",
    "if viable:\n",
    "    print(f\"\\n  VERDICT: VIABLE\")\n",
    "    print(f\"  Split LoRA achieves {speedup:.1f}x speedup with working expert LoRA.\\n\")\n",
    "\n",
    "    print(\"  Suggested pipeline changes:\")\n",
    "    print(\"  \" + \"-\" * 40)\n",
    "    print()\n",
    "    print(\"  1. configs/gpt_oss_20b.py:\")\n",
    "    print(\"     load_in_4bit: False  # Was: True\")\n",
    "    print('     dtype: \"bfloat16\"')\n",
    "    print()\n",
    "    print(\"  2. scripts/pipeline_lib/unsloth_utils.py:\")\n",
    "    print(\"     Add to load_unsloth_model():\")\n",
    "    print(\"       # Auto-detect MoE backend for Split LoRA\")\n",
    "    print('       if not load_in_4bit and os.environ.get(\"UNSLOTH_MOE_BACKEND\") is None:')\n",
    "    print('           gpu_name = torch.cuda.get_device_name(0)')\n",
    "    print('           backend = \"grouped_mm\" if \"H100\" in gpu_name or \"H200\" in gpu_name else \"unsloth_triton\"')\n",
    "    print('           os.environ[\"UNSLOTH_MOE_BACKEND\"] = backend')\n",
    "    print()\n",
    "    print(\"  3. GPU_BASE config updates:\")\n",
    "    print(f'     moe_backend: \"{BENCH_CONFIG[\"moe_backend\"]}\"')\n",
    "    print(f\"     split_lora_speedup: {speedup:.1f}x\")\n",
    "    print(f'     peak_vram_gb: {results[\"split_lora\"].get(\"peak_vram_gb\", \"N/A\")}')\n",
    "\n",
    "    if results.get(\"vram_savings_pct\"):\n",
    "        vram_note = (\n",
    "            f\"saves {abs(results['vram_savings_pct']):.0f}% VRAM\"\n",
    "            if results[\"vram_savings_pct\"] > 0\n",
    "            else f\"uses {abs(results['vram_savings_pct']):.0f}% more VRAM\"\n",
    "        )\n",
    "        print(f\"\\n  Note: bfloat16 Split LoRA {vram_note} vs 4-bit QLoRA\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n  VERDICT: NOT VIABLE\")\n",
    "\n",
    "    reasons = []\n",
    "    if has_error:\n",
    "        reasons.append(f\"Model load error: {results['split_lora'].get('error')}\")\n",
    "    if speedup < 1.5:\n",
    "        reasons.append(f\"Insufficient speedup: {speedup:.1f}x (need >= 1.5x)\")\n",
    "    if not expert_lora_ok:\n",
    "        reasons.append(\"Expert LoRA not working\")\n",
    "    if training_ok is False:\n",
    "        reasons.append(\"Training validation failed\")\n",
    "\n",
    "    print(\"  Reasons:\")\n",
    "    for r in reasons:\n",
    "        print(f\"    - {r}\")\n",
    "\n",
    "    print(\"\\n  Recommendation: Continue using 4-bit QLoRA for production pipeline.\")\n",
    "\n",
    "# Save results\n",
    "with open(\"/tmp/split_lora_bench/results.json\", \"w\") as f:\n",
    "    # Convert non-serializable values\n",
    "    serializable = {}\n",
    "    for k, v in results.items():\n",
    "        if isinstance(v, dict):\n",
    "            serializable[k] = {\n",
    "                sk: sv for sk, sv in v.items()\n",
    "                if not isinstance(sv, list) or len(sv) < 200  # Skip very long loss lists\n",
    "            }\n",
    "        else:\n",
    "            serializable[k] = v\n",
    "    json.dump(serializable, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nResults saved to /tmp/split_lora_bench/results.json\")\n",
    "tracker.complete(\"recommendation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32000000000000000",
   "metadata": {},
   "source": "## Benchmark Complete!\n\n### Next Steps\n\n**If VIABLE:**\n- Update `configs/gpt_oss_20b.py`: set `load_in_4bit=False` and `dtype=\"bfloat16\"`\n- Update `scripts/pipeline_lib/unsloth_utils.py`: add MoE backend auto-detection in `load_unsloth_model()`\n- Re-run `notebooks/train_gpt_oss_coding_tui.ipynb` with Split LoRA enabled\n- Validate on real TUI training data (proxy logs + function-calling datasets)\n- Monitor per-expert gradient norms in wandb to confirm expert utilization\n- Compare final eval metrics (tool call accuracy, agent trajectory quality) against QLoRA baseline\n\n**If NOT VIABLE:**\n- Continue with 4-bit QLoRA as the production configuration\n- Revisit when Unsloth adds Split LoRA support for 4-bit quantized models\n- Check https://unsloth.ai/docs/new/faster-moe for updates on quantization support\n- Consider profiling with `torch.profiler` to identify the actual bottleneck\n\n### Output Artifacts\n- `/tmp/split_lora_bench/results.json` — Full benchmark results (portable)\n- `/tmp/split_lora_bench/comparison.png` — Side-by-side bar charts (if matplotlib available)\n- `/tmp/split_lora_bench/config.json` — Benchmark configuration snapshot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33000000000000000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup\n",
    "try:\n",
    "    if \"split_model\" in dir():\n",
    "        del split_model\n",
    "    if \"split_trainer\" in dir():\n",
    "        del split_trainer\n",
    "    if \"tokenizer\" in dir():\n",
    "        del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Final VRAM: {get_vram_gb():.2f} GB\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Colab: release GPU\n",
    "try:\n",
    "    if IN_COLAB:\n",
    "        from google.colab import runtime\n",
    "        runtime.unassign()\n",
    "        print(\"Colab GPU released.\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"\\nBenchmark complete. Results at /tmp/split_lora_bench/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
