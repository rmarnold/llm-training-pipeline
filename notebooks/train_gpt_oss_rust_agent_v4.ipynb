{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5d6899e",
   "metadata": {},
   "source": [
    "# Train GPT-OSS 20B Rust Agent (v4)\n",
    "\n",
    "**Combined pipeline** — Strandset-first data + mutation enrichment across ALL phases + GRPO RL.\n",
    "\n",
    "**v4 features:**\n",
    "- Widget-based configuration UI (no manual variable editing)\n",
    "- Mutation data enriches **all** training phases (lang_rust, core_agent, IPO)\n",
    "- Pipeline progress dashboard with per-phase tracking\n",
    "- Full pipeline: Strandset → lang_rust → merge → core_agent → IPO → GRPO → eval → export\n",
    "\n",
    "**Base model:** [openai/gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b) (20.9B MoE, 3.6B active)\n",
    "\n",
    "**Data sources:**\n",
    "- [Strandset-Rust-v1](https://huggingface.co/datasets/Fortytwo-Network/Strandset-Rust-v1) (191K examples)\n",
    "- cargo-mutants generated mutations (optional, enriches all phases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ac3a5a",
   "metadata": {},
   "source": [
    "## Step 0: Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c7ebb3",
   "metadata": {},
   "source": [
    "### 0.1 Mount Google Drive & Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe096552",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nIN_COLAB = \"COLAB_GPU\" in os.environ or os.path.exists(\"/content\")\n\nDRIVE_BASE = \"\"\nDRIVE_MODE = \"local\"\n\nif IN_COLAB:\n    from google.colab import drive\n    drive.mount(\"/content/drive\")\n    DRIVE_BASE = \"/content/drive/MyDrive/gpt-oss-20b-rust-agent-v4\"\n    DRIVE_MODE = \"mounted\"\n    os.makedirs(DRIVE_BASE, exist_ok=True)\n\n    # Clone repo if not present\n    if not os.path.exists(\"llm-training-pipeline\"):\n        !git clone https://github.com/rmarnold/llm-training-pipeline.git\n    os.chdir(\"llm-training-pipeline\")\n    !git pull --ff-only\n    print(f\"Working directory: {os.getcwd()}\")\nelse:\n    print(\"Running locally (not in Colab).\")\n    print(f\"Working directory: {os.getcwd()}\")"
  },
  {
   "cell_type": "markdown",
   "id": "fbb6d086",
   "metadata": {},
   "source": [
    "### 0.2 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24311d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "\n",
    "IN_COLAB = \"COLAB_GPU\" in os.environ or os.path.exists(\"/content\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Core + GPT-OSS deps\n",
    "    !pip install -q -e \".[gpt_oss,rust_eval]\"\n",
    "\n",
    "    # Unsloth (Colab optimised)\n",
    "    !pip install -q unsloth\n",
    "\n",
    "    # Rust toolchain (needed for cargo-mutants + eval)\n",
    "    !curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
    "    os.environ[\"PATH\"] = os.path.expanduser(\"~/.cargo/bin\") + \":\" + os.environ[\"PATH\"]\n",
    "    !cargo install cargo-mutants\n",
    "\n",
    "    # vLLM for fast inference in GRPO\n",
    "    !pip install -q vllm\n",
    "\n",
    "    # ipywidgets for config UI\n",
    "    !pip install -q ipywidgets\n",
    "    print(\"\\nDependencies installed.\")\n",
    "else:\n",
    "    print(\"Assuming local dependencies are already installed.\")\n",
    "    print(\"Run: pip install -e '.[gpt_oss,rust_eval]'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509c9ab2",
   "metadata": {},
   "source": "### 0.3 Configure Pipeline\n\nToggle the form view (click the \"...\" menu on this cell) to see the interactive configuration panel.\nAdjust settings, then **run this cell** to apply them."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23228b72",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": "#@title ### Pipeline Configuration { display-mode: \"form\" }\n\n#@markdown ---\n#@markdown #### Core Settings\n\ntraining_scope = \"quick_test\"  #@param [\"full\", \"quick_test\", \"lang_adapter_only\", \"skip_to_rl\"] {type: \"string\"}\ngpu_tier = \"h100_80gb\"  #@param [\"a100_40gb\", \"a100_80gb\", \"h100_80gb\"] {type: \"string\"}\nmax_steps_override = 0  #@param {type: \"integer\"}\n\n#@markdown > *Max Steps Override: 0 = use GPU tier defaults. Set > 0 to cap all stages.*\n\n#@markdown ---\n#@markdown #### Mutation Enrichment\n#@markdown *Requires Rust toolchain (`cargo`, `cargo-mutants`). Mutations run in background during GPU training.*\n\ninclude_mutations = True  #@param {type: \"boolean\"}\nenrich_lang_rust = True  #@param {type: \"boolean\"}\nenrich_ipo = True  #@param {type: \"boolean\"}\n\n#@markdown ---\n#@markdown #### Pipeline Phases\n\ninclude_ipo = True  #@param {type: \"boolean\"}\ninclude_grpo = True  #@param {type: \"boolean\"}\nskip_data_generation = False  #@param {type: \"boolean\"}\n\n#@markdown ---\n#@markdown #### Export\n\nenable_qat_export = False  #@param {type: \"boolean\"}\n\n#@markdown ---\n#@markdown #### Advanced\n\nmax_mutations_per_repo = 50  #@param {type: \"slider\", min: 10, max: 200, step: 10}\nuse_service_account = False  #@param {type: \"boolean\"}\ndrive_folder_id = \"18UpFpUhiNrs2Etha0uFjSGWmj1Ee1SnX\"  #@param {type: \"string\"}\n\n# ======================================================================\n# GPU tier presets (auto-selected based on gpu_tier above)\n# ======================================================================\nimport os, sys, json\n\nGPU_CONFIGS = {\n    \"a100_40gb\": {\n        \"lang_rust_batch\": 2, \"lang_rust_grad_accum\": 16, \"lang_rust_max_steps\": 3000, \"lang_rust_seq_len\": 4096,\n        \"core_agent_batch\": 1, \"core_agent_grad_accum\": 8, \"core_agent_max_steps\": 2000, \"core_agent_seq_len\": 8192,\n        \"ipo_batch\": 1, \"ipo_grad_accum\": 16, \"ipo_max_steps\": 1000, \"ipo_seq_len\": 4096,\n        \"grpo_batch\": 1, \"grpo_grad_accum\": 8, \"grpo_max_steps\": 3000, \"grpo_seq_len\": 16384, \"grpo_num_gen\": 4,\n        \"eval_num_samples\": 100,\n        \"load_mode\": \"4bit\", \"moe_backend\": \"triton\", \"fast_inference\": False,\n    },\n    \"a100_80gb\": {\n        \"lang_rust_batch\": 4, \"lang_rust_grad_accum\": 8, \"lang_rust_max_steps\": 3000, \"lang_rust_seq_len\": 8192,\n        \"core_agent_batch\": 2, \"core_agent_grad_accum\": 4, \"core_agent_max_steps\": 2000, \"core_agent_seq_len\": 16384,\n        \"ipo_batch\": 1, \"ipo_grad_accum\": 16, \"ipo_max_steps\": 1000, \"ipo_seq_len\": 8192,\n        \"grpo_batch\": 1, \"grpo_grad_accum\": 8, \"grpo_max_steps\": 5000, \"grpo_seq_len\": 32768, \"grpo_num_gen\": 4,\n        \"eval_num_samples\": 200,\n        \"load_mode\": \"4bit\", \"moe_backend\": \"triton\", \"fast_inference\": False,\n    },\n    \"h100_80gb\": {\n        \"lang_rust_batch\": 6, \"lang_rust_grad_accum\": 8, \"lang_rust_max_steps\": 3000, \"lang_rust_seq_len\": 8192,\n        \"core_agent_batch\": 6, \"core_agent_grad_accum\": 4, \"core_agent_max_steps\": 2000, \"core_agent_seq_len\": 16384,\n        \"ipo_batch\": 2, \"ipo_grad_accum\": 16, \"ipo_max_steps\": 1000, \"ipo_seq_len\": 8192,\n        \"grpo_batch\": 2, \"grpo_grad_accum\": 8, \"grpo_max_steps\": 5000, \"grpo_seq_len\": 65536, \"grpo_num_gen\": 4,\n        \"eval_num_samples\": 200,\n        \"load_mode\": \"fp8\", \"moe_backend\": \"triton\", \"fast_inference\": True,\n    },\n}\n\ntier = GPU_CONFIGS[gpu_tier]\n\n# ======================================================================\n# Build CONFIG dict from form values\n# ======================================================================\nCONFIG = {\n    \"training_scope\": training_scope,\n    \"gpu_tier\": gpu_tier,\n    **tier,\n    # Mutation enrichment\n    \"include_mutations\": include_mutations,\n    \"enrich_lang_rust\": enrich_lang_rust and include_mutations,\n    \"enrich_ipo\": enrich_ipo and include_mutations,\n    # Pipeline phases\n    \"include_ipo\": include_ipo,\n    \"include_grpo\": include_grpo,\n    \"enable_qat_export\": enable_qat_export,\n    \"skip_data_generation\": skip_data_generation,\n    # Advanced\n    \"max_mutations_per_repo\": max_mutations_per_repo,\n    \"use_service_account\": use_service_account,\n    \"drive_folder_id\": drive_folder_id,\n}\n\n# Apply max_steps override\nif max_steps_override > 0:\n    for key in list(CONFIG.keys()):\n        if key.endswith(\"_max_steps\"):\n            CONFIG[key] = max_steps_override\n\n# Quick test caps\nif CONFIG[\"training_scope\"] == \"quick_test\":\n    for key in list(CONFIG.keys()):\n        if key.endswith(\"_max_steps\"):\n            CONFIG[key] = min(CONFIG[key], 50)\n    CONFIG[\"eval_num_samples\"] = 10\n\n# Auto-detect mutation parallelism\nimport multiprocessing\ncpu_count = multiprocessing.cpu_count()\ncpu_jobs = max(1, cpu_count - 2)\ntry:\n    mem_bytes = os.sysconf(\"SC_PAGE_SIZE\") * os.sysconf(\"SC_PHYS_PAGES\")\n    ram_jobs = max(1, int(mem_bytes / (1024**3) / 4))\nexcept (ValueError, OSError):\n    ram_jobs = cpu_jobs\nCONFIG[\"mutation_jobs\"] = min(cpu_jobs, ram_jobs)\nCONFIG[\"mutation_repo_workers\"] = max(1, CONFIG[\"mutation_jobs\"] // 4)\n\n# Scope-based overrides (can't do reactive UI in Colab Forms, so enforce here)\nif CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n    CONFIG[\"include_ipo\"] = False\n    CONFIG[\"include_grpo\"] = False\nelif CONFIG[\"training_scope\"] == \"skip_to_rl\":\n    CONFIG[\"include_mutations\"] = False\n    CONFIG[\"enrich_lang_rust\"] = False\n    CONFIG[\"enrich_ipo\"] = False\n\n# ======================================================================\n# Set up DriveHelper\n# ======================================================================\nsys.path.insert(0, \"scripts\")\nfrom pipeline_lib.drive_utils import DriveHelper\n\nif \"DRIVE_BASE\" not in dir():\n    DRIVE_BASE = \"\"\nif \"DRIVE_MODE\" not in dir():\n    DRIVE_MODE = \"local\"\n\nif CONFIG[\"use_service_account\"] and CONFIG[\"drive_folder_id\"]:\n    sa_path = \"service_account.json\"\n    try:\n        from google.colab import userdata\n        sa_json = userdata.get(\"SERVICE_ACCOUNT_JSON\")\n        with open(sa_path, \"w\") as f:\n            f.write(sa_json)\n    except Exception:\n        pass\n\n    if os.path.exists(sa_path) and os.path.getsize(sa_path) > 10:\n        try:\n            drive_helper = DriveHelper(\n                mode=\"service_account\",\n                credentials_path=sa_path,\n                folder_id=CONFIG[\"drive_folder_id\"],\n            )\n            DRIVE_MODE = \"service_account\"\n        except Exception as e:\n            print(f\"Service account failed: {e}\")\n            drive_helper = DriveHelper(mode=\"local\")\n            DRIVE_MODE = \"local\"\n    else:\n        drive_helper = DriveHelper(mode=\"local\")\n        DRIVE_MODE = \"local\"\nelif DRIVE_BASE:\n    drive_helper = DriveHelper(mode=\"mounted\", drive_base=DRIVE_BASE)\n    DRIVE_MODE = \"mounted\"\nelse:\n    drive_helper = DriveHelper(mode=\"local\")\n    DRIVE_MODE = \"local\"\n\n# Save for persistence across restarts\nos.makedirs(\"data\", exist_ok=True)\nwith open(\"data/config_v4.json\", \"w\") as f:\n    json.dump(CONFIG, f, indent=2)\n\n# ======================================================================\n# Print summary\n# ======================================================================\nprint(\"=\" * 55)\nprint(\"  PIPELINE CONFIGURATION (v4)\")\nprint(\"=\" * 55)\nprint(f\"  Scope:          {CONFIG['training_scope'].upper()}\")\nprint(f\"  GPU tier:       {CONFIG['gpu_tier']}\")\nprint(f\"  MoE backend:    {CONFIG['moe_backend']}\")\nprint(f\"  Load mode:      {CONFIG['load_mode']}\")\nprint(f\"  Drive mode:     {DRIVE_MODE}\")\nprint()\nprint(f\"  Mutations:      {CONFIG['include_mutations']}\")\nif CONFIG[\"include_mutations\"]:\n    print(f\"    Enrich LR:    {CONFIG['enrich_lang_rust']}\")\n    print(f\"    Enrich IPO:   {CONFIG['enrich_ipo']}\")\n    print(f\"    Mut/repo:     {CONFIG['max_mutations_per_repo']}\")\n    print(f\"    Jobs:         {CONFIG['mutation_jobs']} ({CONFIG['mutation_repo_workers']} repo workers)\")\nprint()\nprint(f\"  IPO:            {CONFIG['include_ipo']}\")\nprint(f\"  GRPO:           {CONFIG['include_grpo']}\")\nprint(f\"  QAT export:     {CONFIG['enable_qat_export']}\")\nif max_steps_override > 0:\n    print(f\"  Max steps:      {max_steps_override} (override)\")\nprint()\nprint(f\"  Lang Adapter:   batch={CONFIG['lang_rust_batch']} x grad_accum={CONFIG['lang_rust_grad_accum']}, seq={CONFIG['lang_rust_seq_len']}, steps={CONFIG['lang_rust_max_steps']}\")\nprint(f\"  Core Agent:     batch={CONFIG['core_agent_batch']} x grad_accum={CONFIG['core_agent_grad_accum']}, seq={CONFIG['core_agent_seq_len']}, steps={CONFIG['core_agent_max_steps']}\")\nif CONFIG[\"include_ipo\"]:\n    print(f\"  IPO:            batch={CONFIG['ipo_batch']} x grad_accum={CONFIG['ipo_grad_accum']}, seq={CONFIG['ipo_seq_len']}, steps={CONFIG['ipo_max_steps']}\")\nif CONFIG[\"include_grpo\"]:\n    print(f\"  GRPO:           batch={CONFIG['grpo_batch']} x grad_accum={CONFIG['grpo_grad_accum']}, seq={CONFIG['grpo_seq_len']}, steps={CONFIG['grpo_max_steps']}\")\nprint(\"=\" * 55)"
  },
  {
   "cell_type": "markdown",
   "id": "607401d4",
   "metadata": {},
   "source": [
    "### 0.4 Pipeline Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ba5276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "class PipelineTracker:\n",
    "    \"\"\"Track pipeline progress with visual indicators.\"\"\"\n",
    "\n",
    "    PHASES = [\n",
    "        (\"strandset\", \"Strandset Data\"),\n",
    "        (\"mutations_bg\", \"Mutations (Background)\"),\n",
    "        (\"lang_rust\", \"Lang Adapter Training\"),\n",
    "        (\"merge\", \"Merge Adapter\"),\n",
    "        (\"enrichment\", \"Mutation Enrichment\"),\n",
    "        (\"core_agent\", \"Core Agent SFT\"),\n",
    "        (\"ipo\", \"IPO Preference\"),\n",
    "        (\"grpo\", \"GRPO RL\"),\n",
    "        (\"eval\", \"Evaluation\"),\n",
    "        (\"export\", \"Export\"),\n",
    "    ]\n",
    "\n",
    "    def __init__(self):\n",
    "        self._bars = {}\n",
    "        self._labels = {}\n",
    "        rows = []\n",
    "        for key, name in self.PHASES:\n",
    "            label = widgets.HTML(\n",
    "                value=f\"<span style='color:#888'>&#x25CB; {name}</span>\",\n",
    "                layout=widgets.Layout(width=\"220px\"),\n",
    "            )\n",
    "            bar = widgets.FloatProgress(\n",
    "                value=0, min=0, max=1.0,\n",
    "                bar_style=\"info\",\n",
    "                layout=widgets.Layout(width=\"300px\", height=\"18px\"),\n",
    "            )\n",
    "            self._bars[key] = bar\n",
    "            self._labels[key] = label\n",
    "            rows.append(widgets.HBox([label, bar]))\n",
    "        self._container = widgets.VBox(rows)\n",
    "        display(widgets.HTML(\"<b>Pipeline Progress</b>\"))\n",
    "        display(self._container)\n",
    "\n",
    "    def start(self, phase):\n",
    "        self._labels[phase].value = (\n",
    "            f\"<span style='color:#2196F3'>&#x25B6; {dict(self.PHASES)[phase]}</span>\"\n",
    "        )\n",
    "        self._bars[phase].value = 0.1\n",
    "        self._bars[phase].bar_style = \"info\"\n",
    "\n",
    "    def complete(self, phase):\n",
    "        self._labels[phase].value = (\n",
    "            f\"<span style='color:#4CAF50'>&#x2714; {dict(self.PHASES)[phase]}</span>\"\n",
    "        )\n",
    "        self._bars[phase].value = 1.0\n",
    "        self._bars[phase].bar_style = \"success\"\n",
    "\n",
    "    def skip(self, phase):\n",
    "        self._labels[phase].value = (\n",
    "            f\"<span style='color:#9E9E9E'>&#x2014; {dict(self.PHASES)[phase]} (skipped)</span>\"\n",
    "        )\n",
    "        self._bars[phase].value = 1.0\n",
    "        self._bars[phase].bar_style = \"\"\n",
    "\n",
    "    def fail(self, phase):\n",
    "        self._labels[phase].value = (\n",
    "            f\"<span style='color:#F44336'>&#x2718; {dict(self.PHASES)[phase]}</span>\"\n",
    "        )\n",
    "        self._bars[phase].bar_style = \"danger\"\n",
    "\n",
    "tracker = PipelineTracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74ceb36",
   "metadata": {},
   "source": [
    "### 0.5 Set Up Persistent Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e170f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DRIVE_SUBDIRS = [\n",
    "    \"data/rust/strandset\",\n",
    "    \"data/rust/mutations\",\n",
    "    \"data/rust/lang_rust\",\n",
    "    \"data/rust/core_agent\",\n",
    "    \"data/rust/ipo\",\n",
    "    \"data/rust/grpo\",\n",
    "    \"data/rust/eval\",\n",
    "    \"checkpoints/lang_rust\",\n",
    "    \"checkpoints/gpt-oss-20b-rust-merged\",\n",
    "    \"checkpoints/core_agent\",\n",
    "    \"checkpoints/core_agent_ipo\",\n",
    "    \"checkpoints/core_agent_grpo\",\n",
    "    \"evals\",\n",
    "]\n",
    "\n",
    "if DRIVE_MODE == \"mounted\":\n",
    "    for subdir in DRIVE_SUBDIRS:\n",
    "        drive_path = os.path.join(DRIVE_BASE, subdir)\n",
    "        os.makedirs(drive_path, exist_ok=True)\n",
    "        local_path = subdir\n",
    "        if not os.path.exists(local_path):\n",
    "            os.makedirs(os.path.dirname(local_path) or \".\", exist_ok=True)\n",
    "            os.symlink(drive_path, local_path)\n",
    "            print(f\"  Linked: {local_path} -> {drive_path}\")\n",
    "        else:\n",
    "            print(f\"  Exists: {local_path}\")\n",
    "    print(f\"\\nDrive base: {DRIVE_BASE}\")\n",
    "elif DRIVE_MODE == \"service_account\":\n",
    "    for subdir in DRIVE_SUBDIRS:\n",
    "        os.makedirs(subdir, exist_ok=True)\n",
    "        drive_helper.ensure_dir(subdir)\n",
    "    print(\"Drive directories created (service account mode).\")\n",
    "else:\n",
    "    for subdir in DRIVE_SUBDIRS:\n",
    "        os.makedirs(subdir, exist_ok=True)\n",
    "    print(\"Local directories created (no Drive backup).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a145f2de",
   "metadata": {},
   "source": [
    "### 0.6 Check GPU & Configure MoE Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305e96d0",
   "metadata": {},
   "outputs": [],
   "source": "import torch, os\n\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n\n    print(f\"GPU: {gpu_name}\")\n    print(f\"VRAM: {gpu_mem:.1f} GB\")\n\n    # Auto-detect GPU tier override\n    detected_tier = None\n    if \"H100\" in gpu_name or \"H200\" in gpu_name:\n        detected_tier = \"h100_80gb\"\n    elif \"A100\" in gpu_name:\n        detected_tier = \"a100_80gb\" if gpu_mem > 45 else \"a100_40gb\"\n\n    if detected_tier and detected_tier != CONFIG[\"gpu_tier\"]:\n        print(f\"\\n  Auto-override: {CONFIG['gpu_tier']} -> {detected_tier}\")\n        old_tier = CONFIG[\"gpu_tier\"]\n        CONFIG[\"gpu_tier\"] = detected_tier\n        tier = GPU_CONFIGS[detected_tier]\n        for k, v in tier.items():\n            CONFIG[k] = v\n        print(f\"  Updated CONFIG with {detected_tier} presets.\")\n\n    # Set MoE backend\n    os.environ[\"UNSLOTH_MOE_BACKEND\"] = CONFIG.get(\"moe_backend\", \"triton\")\n    print(f\"\\n  MoE backend: {os.environ['UNSLOTH_MOE_BACKEND']}\")\n    print(f\"  Load mode: {CONFIG['load_mode']}\")\n    print(f\"  Fast inference: {CONFIG.get('fast_inference', False)}\")\n\n    # FP8 detection\n    if CONFIG[\"load_mode\"] == \"fp8\":\n        try:\n            import transformer_engine\n            print(\"  FP8: transformer-engine available\")\n        except ImportError:\n            print(\"  FP8: transformer-engine not found, falling back to 4bit\")\n            CONFIG[\"load_mode\"] = \"4bit\"\nelse:\n    print(\"No GPU detected! Training will fail.\")\n    print(\"Enable GPU: Runtime -> Change runtime type -> GPU\")\n\nprint(f\"\\nFinal config: scope={CONFIG['training_scope']}, tier={CONFIG['gpu_tier']}\")"
  },
  {
   "cell_type": "markdown",
   "id": "a1563122",
   "metadata": {},
   "source": [
    "## Step 1: Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0385585b",
   "metadata": {},
   "source": [
    "### 1.1 Download & Format Strandset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd3f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"skip_data_generation\"]:\n",
    "    print(\"Skipping data generation (skip_data_generation=True)\")\n",
    "    tracker.skip(\"strandset\")\n",
    "else:\n",
    "    tracker.start(\"strandset\")\n",
    "\n",
    "    cmd = \"python scripts/20_prepare_strandset.py\"\n",
    "\n",
    "    if CONFIG[\"training_scope\"] == \"quick_test\":\n",
    "        cmd += \" --max_samples 500\"\n",
    "\n",
    "    print(\"Downloading and formatting Strandset-Rust-v1...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    # Copy Strandset outputs to training dirs\n",
    "    import shutil\n",
    "    from pathlib import Path\n",
    "\n",
    "    mappings = [\n",
    "        (\"data/rust/strandset/lang_rust/train\", \"data/rust/lang_rust/train\"),\n",
    "        (\"data/rust/strandset/core_agent/train\", \"data/rust/core_agent/train\"),\n",
    "        (\"data/rust/strandset/ipo/train\", \"data/rust/ipo/train\"),\n",
    "    ]\n",
    "    for src, dst in mappings:\n",
    "        if os.path.exists(src):\n",
    "            if os.path.islink(dst):\n",
    "                # Symlinked to Drive — copy into the link target\n",
    "                shutil.copytree(src, dst, dirs_exist_ok=True)\n",
    "            elif not os.path.exists(dst) or len(os.listdir(dst)) == 0:\n",
    "                shutil.copytree(src, dst, dirs_exist_ok=True)\n",
    "            print(f\"  {src} -> {dst}\")\n",
    "\n",
    "    drive_helper.backup(\"data/rust/strandset\", \"data/rust/strandset\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nStrandset backed up to Drive.\")\n",
    "\n",
    "    tracker.complete(\"strandset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e725f3",
   "metadata": {},
   "source": [
    "### 1.2 Start Background Mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00476f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "mutation_proc = None\n",
    "\n",
    "if not CONFIG[\"include_mutations\"]:\n",
    "    print(\"Mutations disabled — skipping.\")\n",
    "    tracker.skip(\"mutations_bg\")\n",
    "elif CONFIG[\"skip_data_generation\"]:\n",
    "    print(\"Skipping data generation (skip_data_generation=True)\")\n",
    "    tracker.skip(\"mutations_bg\")\n",
    "else:\n",
    "    tracker.start(\"mutations_bg\")\n",
    "\n",
    "    max_muts = CONFIG[\"max_mutations_per_repo\"]\n",
    "    backup_dir = \"data/rust/mutations/backup\"\n",
    "\n",
    "    cmd = [\n",
    "        \"python\", \"scripts/16_generate_mutations.py\",\n",
    "        \"--max_mutations_per_repo\", str(max_muts),\n",
    "        \"--backup-dir\", backup_dir,\n",
    "    ]\n",
    "\n",
    "    if CONFIG[\"training_scope\"] == \"quick_test\":\n",
    "        cmd.extend([\"--repos\", \"ripgrep\"])\n",
    "\n",
    "    print(f\"Starting mutations in background (max {max_muts}/repo)...\")\n",
    "    print(f\"  Command: {' '.join(cmd)}\")\n",
    "    mutation_proc = subprocess.Popen(\n",
    "        cmd,\n",
    "        stdout=open(\"data/rust/mutations/stdout.log\", \"w\"),\n",
    "        stderr=subprocess.STDOUT,\n",
    "    )\n",
    "    print(f\"  PID: {mutation_proc.pid}\")\n",
    "    print(\"  Mutations will run during lang_rust training (~60 min).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8713bca7",
   "metadata": {},
   "source": [
    "### 1.3 Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550d2985",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_checks = [\n",
    "    (\"Strandset stats\", \"data/rust/strandset/stats.json\"),\n",
    "    (\"Lang Rust train\", \"data/rust/lang_rust/train\"),\n",
    "    (\"Core Agent train\", \"data/rust/core_agent/train\"),\n",
    "    (\"IPO train\", \"data/rust/ipo/train\"),\n",
    "    (\"Mutations dir\", \"data/rust/mutations\"),\n",
    "]\n",
    "\n",
    "print(\"Data Verification:\")\n",
    "print(\"=\" * 60)\n",
    "for name, path in data_checks:\n",
    "    exists = os.path.exists(path)\n",
    "    if exists and os.path.isdir(path):\n",
    "        items = os.listdir(path)\n",
    "        print(f\"  \\u2713 {name}: {path} ({len(items)} items)\")\n",
    "    elif exists:\n",
    "        size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "        print(f\"  \\u2713 {name}: {path} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 {name}: not found at {path}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df473c2",
   "metadata": {},
   "source": [
    "## Step 2: Lang Adapter Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048f19ca",
   "metadata": {},
   "source": [
    "### 2.0 Pre-enrich lang_rust (Cached Mutations)\n",
    "\n",
    "If a previous run generated mutation-based debug pairs for lang_rust, merge them\n",
    "into the training data now. Uses a `.enriched_v4` marker file to avoid duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5680f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "    print(\"Skipping — scope is skip_to_rl\")\n",
    "elif not CONFIG[\"enrich_lang_rust\"]:\n",
    "    print(\"Skipping — enrich_lang_rust is disabled\")\n",
    "else:\n",
    "    cache_dir = \"data/rust/mutations/lang_rust_enrichment\"\n",
    "    train_dir = \"data/rust/lang_rust/train\"\n",
    "    marker = os.path.join(train_dir, \".enriched_v4\")\n",
    "\n",
    "    if os.path.exists(cache_dir) and not os.path.exists(marker):\n",
    "        from datasets import load_from_disk, concatenate_datasets\n",
    "\n",
    "        try:\n",
    "            cached_ds = load_from_disk(cache_dir)\n",
    "            base_ds = load_from_disk(train_dir)\n",
    "            print(f\"Pre-enriching lang_rust:\")\n",
    "            print(f\"  Base dataset: {len(base_ds):,} examples\")\n",
    "            print(f\"  Cached mutations: {len(cached_ds):,} debug pairs\")\n",
    "            merged = concatenate_datasets([base_ds, cached_ds])\n",
    "            merged.save_to_disk(train_dir)\n",
    "\n",
    "            # Write marker to prevent re-merging\n",
    "            with open(marker, \"w\") as f:\n",
    "                f.write(f\"enriched with {len(cached_ds)} cached examples\\n\")\n",
    "\n",
    "            print(f\"  Merged: {len(merged):,} total -> {train_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: could not load cached enrichment: {e}\")\n",
    "    elif os.path.exists(marker):\n",
    "        print(\"Already enriched (marker found). Skipping.\")\n",
    "    else:\n",
    "        print(\"No cached mutation enrichment found. Will generate for next run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aa88e9",
   "metadata": {},
   "source": [
    "### 2.1 Train lang_rust Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd383a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "    print(\"Skipping — scope is skip_to_rl\")\n",
    "    tracker.skip(\"lang_rust\")\n",
    "else:\n",
    "    tracker.start(\"lang_rust\")\n",
    "\n",
    "    batch = CONFIG[\"lang_rust_batch\"]\n",
    "    grad_accum = CONFIG[\"lang_rust_grad_accum\"]\n",
    "    max_steps = CONFIG[\"lang_rust_max_steps\"]\n",
    "    seq_len = CONFIG[\"lang_rust_seq_len\"]\n",
    "\n",
    "    cmd = f\"python scripts/13_train_lang_adapter.py\"\n",
    "    cmd += f\" --train_data_path data/rust/lang_rust/train\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    print(f\"Training lang_rust adapter...\")\n",
    "    print(f\"  Data: data/rust/lang_rust/train\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Seq length: {seq_len} (from config)\")\n",
    "    print(f\"  LoRA rank: 64\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/lang_rust\", \"checkpoints/lang_rust\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")\n",
    "\n",
    "    tracker.complete(\"lang_rust\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b56fd5",
   "metadata": {},
   "source": [
    "### 2.2 Merge lang_rust into Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e86946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "    print(\"Skipping — scope is skip_to_rl\")\n",
    "    tracker.skip(\"merge\")\n",
    "else:\n",
    "    tracker.start(\"merge\")\n",
    "\n",
    "    print(\"Merging lang_rust adapter into base model...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/19_merge_adapter.py\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/gpt-oss-20b-rust-merged\", \"checkpoints/gpt-oss-20b-rust-merged\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nMerged model backed up to Drive.\")\n",
    "\n",
    "    tracker.complete(\"merge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd162294",
   "metadata": {},
   "source": [
    "### 2.3 Verify Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab78cedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "    print(\"Skipping — scope is skip_to_rl\")\n",
    "else:\n",
    "    merged_path = \"checkpoints/gpt-oss-20b-rust-merged\"\n",
    "\n",
    "    print(\"Merge Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(merged_path):\n",
    "        files = os.listdir(merged_path)\n",
    "        total_size = sum(\n",
    "            os.path.getsize(os.path.join(merged_path, f))\n",
    "            for f in files if os.path.isfile(os.path.join(merged_path, f))\n",
    "        )\n",
    "        print(f\"  \\u2713 Merged model: {merged_path}\")\n",
    "        print(f\"    Files: {len(files)}\")\n",
    "        print(f\"    Total size: {total_size / (1024**3):.1f} GB\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 Merged model not found at {merged_path}\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d4e80a",
   "metadata": {},
   "source": [
    "## Step 2.5: Mutation Enrichment\n",
    "\n",
    "Wait for background mutations to complete, then enrich **all** training phases:\n",
    "- **(a)** Save debug pairs for lang_rust enrichment cache (for future runs)\n",
    "- **(b)** Generate trajectories and merge into core_agent data\n",
    "- **(c)** Format preference pairs and merge into IPO data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e1333e",
   "metadata": {},
   "source": [
    "### 2.5 Wait for Mutations & Enrich All Phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda6e390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, json\n",
    "sys.path.insert(0, \"scripts\")\n",
    "\n",
    "if not CONFIG[\"include_mutations\"] or mutation_proc is None:\n",
    "    print(\"Mutations not running — skipping enrichment.\")\n",
    "    tracker.skip(\"enrichment\")\n",
    "elif CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "    print(\"Skipping — scope is skip_to_rl\")\n",
    "    tracker.skip(\"enrichment\")\n",
    "else:\n",
    "    tracker.start(\"enrichment\")\n",
    "\n",
    "    # --- Wait for mutation process ---\n",
    "    print(\"Waiting for background mutations to complete...\")\n",
    "    returncode = mutation_proc.wait()\n",
    "    if returncode != 0:\n",
    "        print(f\"  WARNING: mutations exited with code {returncode}\")\n",
    "        print(f\"  Check log: data/rust/mutations/stdout.log\")\n",
    "    else:\n",
    "        tracker.complete(\"mutations_bg\")\n",
    "\n",
    "    mutations_path = \"data/rust/mutations/mutations.jsonl\"\n",
    "    if not os.path.exists(mutations_path):\n",
    "        print(f\"  No mutations file found at {mutations_path}\")\n",
    "        tracker.fail(\"enrichment\")\n",
    "    else:\n",
    "        # Count mutations\n",
    "        with open(mutations_path) as f:\n",
    "            mutations = [json.loads(line) for line in f if line.strip()]\n",
    "        print(f\"  Loaded {len(mutations):,} mutations from {mutations_path}\")\n",
    "\n",
    "        # ============================================================\n",
    "        # (a) Save debug pairs for lang_rust enrichment cache\n",
    "        # ============================================================\n",
    "        print(f\"\\n--- (a) Caching lang_rust enrichment ---\")\n",
    "        from dataset_formatters.harmony import format_harmony_debug\n",
    "\n",
    "        debug_examples = []\n",
    "        for m in mutations:\n",
    "            result = format_harmony_debug({\n",
    "                \"buggy_code\": m.get(\"mutant_code\", \"\"),\n",
    "                \"error_message\": m.get(\"compiler_error\", m.get(\"test_error\", \"\")),\n",
    "                \"fixed_code\": m.get(\"original_code\", \"\"),\n",
    "            })\n",
    "            if result.get(\"text\"):\n",
    "                debug_examples.append(result)\n",
    "\n",
    "        if debug_examples:\n",
    "            from datasets import Dataset\n",
    "            cache_dir = \"data/rust/mutations/lang_rust_enrichment\"\n",
    "            debug_ds = Dataset.from_list(debug_examples)\n",
    "            debug_ds.save_to_disk(cache_dir)\n",
    "            print(f\"  Saved {len(debug_ds):,} debug pairs -> {cache_dir}\")\n",
    "            print(f\"  (Will be used on next run for lang_rust pre-enrichment)\")\n",
    "        else:\n",
    "            print(\"  No valid debug pairs generated.\")\n",
    "\n",
    "        # ============================================================\n",
    "        # (b) Generate trajectories -> merge into core_agent\n",
    "        # ============================================================\n",
    "        print(f\"\\n--- (b) Generating trajectories for core_agent ---\")\n",
    "\n",
    "        traj_cmd = f\"python scripts/15_generate_trajectories.py\"\n",
    "        traj_cmd += f\" --mutations_path {mutations_path}\"\n",
    "        traj_cmd += f\" --output_dir data/rust/core_agent/train_traj\"\n",
    "        traj_cmd += f\" --no-strandset\"\n",
    "\n",
    "        if CONFIG[\"training_scope\"] == \"quick_test\":\n",
    "            traj_cmd += \" --max_samples 100\"\n",
    "\n",
    "        !{traj_cmd}\n",
    "\n",
    "        # Merge trajectory data into core_agent\n",
    "        traj_path = \"data/rust/core_agent/train_traj\"\n",
    "        core_path = \"data/rust/core_agent/train\"\n",
    "        if os.path.exists(traj_path) and os.path.exists(core_path):\n",
    "            from datasets import load_from_disk, concatenate_datasets\n",
    "\n",
    "            traj_ds = load_from_disk(traj_path)\n",
    "            core_ds = load_from_disk(core_path)\n",
    "            print(f\"  Enriching core_agent: {len(core_ds):,} Strandset + {len(traj_ds):,} trajectories\")\n",
    "            merged = concatenate_datasets([core_ds, traj_ds])\n",
    "            merged.save_to_disk(core_path)\n",
    "            print(f\"  Saved enriched dataset: {len(merged):,} total -> {core_path}\")\n",
    "\n",
    "        # ============================================================\n",
    "        # (c) Format preference pairs -> merge into IPO\n",
    "        # ============================================================\n",
    "        if CONFIG[\"enrich_ipo\"]:\n",
    "            print(f\"\\n--- (c) Generating preference pairs for IPO ---\")\n",
    "            from dataset_formatters.harmony import format_harmony_preference\n",
    "\n",
    "            pref_examples = []\n",
    "            for m in mutations:\n",
    "                buggy = m.get(\"mutant_code\", \"\")\n",
    "                fixed = m.get(\"original_code\", \"\")\n",
    "                error = m.get(\"compiler_error\", m.get(\"test_error\", \"\"))\n",
    "                if not buggy or not fixed:\n",
    "                    continue\n",
    "\n",
    "                prompt = f\"Fix the bug in this Rust code\"\n",
    "                if error:\n",
    "                    prompt += f\":\\n\\nError: {error}\"\n",
    "                prompt += f\"\\n\\n```rust\\n{buggy}\\n```\"\n",
    "\n",
    "                result = format_harmony_preference({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"chosen\": f\"```rust\\n{fixed}\\n```\",\n",
    "                    \"rejected\": f\"```rust\\n{buggy}\\n```\",\n",
    "                })\n",
    "                if result.get(\"text\"):\n",
    "                    pref_examples.append(result)\n",
    "\n",
    "            if pref_examples:\n",
    "                from datasets import Dataset, load_from_disk, concatenate_datasets\n",
    "\n",
    "                ipo_path = \"data/rust/ipo/train\"\n",
    "                pref_ds = Dataset.from_list(pref_examples)\n",
    "\n",
    "                if os.path.exists(ipo_path):\n",
    "                    base_ipo = load_from_disk(ipo_path)\n",
    "                    print(f\"  Enriching IPO: {len(base_ipo):,} Strandset + {len(pref_ds):,} mutation pairs\")\n",
    "                    merged_ipo = concatenate_datasets([base_ipo, pref_ds])\n",
    "                    merged_ipo.save_to_disk(ipo_path)\n",
    "                    print(f\"  Saved enriched IPO: {len(merged_ipo):,} total -> {ipo_path}\")\n",
    "                else:\n",
    "                    pref_ds.save_to_disk(ipo_path)\n",
    "                    print(f\"  Saved IPO: {len(pref_ds):,} pairs -> {ipo_path}\")\n",
    "            else:\n",
    "                print(\"  No valid preference pairs generated.\")\n",
    "        else:\n",
    "            print(\"\\n--- (c) IPO enrichment disabled ---\")\n",
    "\n",
    "        # Backup enriched data\n",
    "        drive_helper.backup(\"data/rust/core_agent\", \"data/rust/core_agent\")\n",
    "        drive_helper.backup(\"data/rust/ipo\", \"data/rust/ipo\")\n",
    "        drive_helper.backup(\"data/rust/mutations\", \"data/rust/mutations\")\n",
    "        if DRIVE_MODE != \"local\":\n",
    "            print(\"\\nEnriched data backed up to Drive.\")\n",
    "\n",
    "        tracker.complete(\"enrichment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e865c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "enriched_checks = [\n",
    "    (\"Lang Rust (training)\", \"data/rust/lang_rust/train\"),\n",
    "    (\"Core Agent (enriched)\", \"data/rust/core_agent/train\"),\n",
    "    (\"IPO (enriched)\", \"data/rust/ipo/train\"),\n",
    "]\n",
    "\n",
    "print(\"Enriched Data Summary:\")\n",
    "print(\"=\" * 60)\n",
    "for name, path in enriched_checks:\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            ds = load_from_disk(path)\n",
    "            print(f\"  \\u2713 {name}: {len(ds):,} examples\")\n",
    "        except Exception as e:\n",
    "            print(f\"  \\u2717 {name}: failed to load ({e})\")\n",
    "    else:\n",
    "        print(f\"  \\u2014 {name}: not found\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efa219a",
   "metadata": {},
   "source": [
    "## Step 3: Core Agent SFT\n",
    "\n",
    "Train a higher-rank LoRA adapter (rank 128) on agent trajectories with tool use.\n",
    "Uses the merged lang_rust model as the base.\n",
    "\n",
    "**v4:** Data enriched with mutation-based trajectories (Step 2.5b)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a271aa",
   "metadata": {},
   "source": [
    "### 3.1 Train core_agent Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67b2a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] in (\"lang_adapter_only\", \"skip_to_rl\"):\n",
    "    print(f\"Skipping \\u2014 scope is {CONFIG['training_scope']}\")\n",
    "    tracker.skip(\"core_agent\")\n",
    "else:\n",
    "    tracker.start(\"core_agent\")\n",
    "\n",
    "    batch = CONFIG[\"core_agent_batch\"]\n",
    "    grad_accum = CONFIG[\"core_agent_grad_accum\"]\n",
    "    max_steps = CONFIG[\"core_agent_max_steps\"]\n",
    "    seq_len = CONFIG[\"core_agent_seq_len\"]\n",
    "\n",
    "    cmd = f\"python scripts/14_train_core_agent.py\"\n",
    "    cmd += f\" --train_data_path data/rust/core_agent/train\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    print(f\"Training core_agent adapter...\")\n",
    "    print(f\"  Data: data/rust/core_agent/train\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Seq length: {seq_len} (from config)\")\n",
    "    print(f\"  LoRA rank: 128\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(f\"  Auto packing: enabled (uncontaminated)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/core_agent\", \"checkpoints/core_agent\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")\n",
    "\n",
    "    tracker.complete(\"core_agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e2369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] in (\"lang_adapter_only\", \"skip_to_rl\"):\n",
    "    print(f\"Skipping \\u2014 scope is {CONFIG['training_scope']}\")\n",
    "else:\n",
    "    ckpt_path = \"checkpoints/core_agent/final\"\n",
    "\n",
    "    print(\"Core Agent Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(ckpt_path):\n",
    "        files = os.listdir(ckpt_path)\n",
    "        print(f\"  \\u2713 Checkpoint: {ckpt_path} ({len(files)} files)\")\n",
    "\n",
    "        adapter_config = os.path.join(ckpt_path, \"adapter_config.json\")\n",
    "        if os.path.exists(adapter_config):\n",
    "            import json\n",
    "            with open(adapter_config) as f:\n",
    "                cfg = json.load(f)\n",
    "            print(f\"    LoRA rank: {cfg.get('r', '?')}\")\n",
    "            print(f\"    Alpha: {cfg.get('lora_alpha', '?')}\")\n",
    "            print(f\"    Target modules: {cfg.get('target_modules', '?')}\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 Checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cba4689",
   "metadata": {},
   "source": [
    "## Step 4: IPO Preference Training\n",
    "\n",
    "Train with Identity Preference Optimisation on preference pairs.\n",
    "Very low learning rate (5e-7), 1 epoch only to avoid collapse.\n",
    "\n",
    "**v4:** IPO data enriched with mutation-based preference pairs (Step 2.5c)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edffaa80",
   "metadata": {},
   "source": [
    "### 4.1 Train with IPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e5a869",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "    tracker.skip(\"ipo\")\n",
    "elif not CONFIG[\"include_ipo\"]:\n",
    "    print(\"Skipping \\u2014 IPO disabled (include_ipo=False)\")\n",
    "    tracker.skip(\"ipo\")\n",
    "else:\n",
    "    tracker.start(\"ipo\")\n",
    "\n",
    "    batch = CONFIG[\"ipo_batch\"]\n",
    "    grad_accum = CONFIG[\"ipo_grad_accum\"]\n",
    "    max_steps = CONFIG[\"ipo_max_steps\"]\n",
    "\n",
    "    if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "        ipo_checkpoint = \"checkpoints/core_agent/final\"\n",
    "        print(\"Using existing core_agent checkpoint (skip_to_rl mode)\")\n",
    "    else:\n",
    "        ipo_checkpoint = \"checkpoints/core_agent/final\"\n",
    "\n",
    "    cmd = f\"python scripts/17_ipo_preference.py\"\n",
    "    cmd += f\" --checkpoint {ipo_checkpoint}\"\n",
    "    cmd += f\" --train_data_path data/rust/ipo/train\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    print(f\"Training with IPO (enriched preferences)...\")\n",
    "    print(f\"  Checkpoint: {ipo_checkpoint}\")\n",
    "    print(f\"  Data: data/rust/ipo/train\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Loss: IPO (beta=0.1)\")\n",
    "    print(f\"  Load mode: {CONFIG['load_mode']}\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/core_agent_ipo\", \"checkpoints/core_agent_ipo\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")\n",
    "\n",
    "    tracker.complete(\"ipo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9372daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "elif not CONFIG[\"include_ipo\"]:\n",
    "    print(\"Skipping \\u2014 IPO disabled\")\n",
    "else:\n",
    "    ckpt_path = \"checkpoints/core_agent_ipo/final\"\n",
    "\n",
    "    print(\"IPO Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(ckpt_path):\n",
    "        files = os.listdir(ckpt_path)\n",
    "        print(f\"  \\u2713 IPO checkpoint: {ckpt_path} ({len(files)} files)\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 IPO checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "    tb_dir = \"checkpoints/core_agent_ipo\"\n",
    "    tb_files = []\n",
    "    if os.path.exists(tb_dir):\n",
    "        for root, dirs, fnames in os.walk(tb_dir):\n",
    "            for fn in fnames:\n",
    "                if fn.startswith(\"events.out.tfevents\"):\n",
    "                    tb_files.append(os.path.join(root, fn))\n",
    "    if tb_files:\n",
    "        print(f\"  \\u2713 TensorBoard logs found ({len(tb_files)} event files)\")\n",
    "        print(f\"    Monitor KL divergence: warn >0.3, abort >0.5\")\n",
    "    else:\n",
    "        print(f\"  \\u2014 No TensorBoard logs found\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153541d7",
   "metadata": {},
   "source": [
    "## Step 5: GRPO RL\n",
    "\n",
    "Group Relative Policy Optimisation with execution-based rewards.\n",
    "Generates N completions per prompt, runs `cargo check/test/clippy`, computes group-relative advantages.\n",
    "\n",
    "**Optimisations:**\n",
    "- FP8 RL with vLLM inference on H100 (1.6x throughput)\n",
    "- Chunked batching for longer context\n",
    "- Extended curriculum: 65K context on H100\n",
    "- Harmony format compliance reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2cf3a9",
   "metadata": {},
   "source": [
    "### 5.1 Train with GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a170c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "    tracker.skip(\"grpo\")\n",
    "elif not CONFIG[\"include_grpo\"]:\n",
    "    print(\"Skipping \\u2014 GRPO disabled (include_grpo=False)\")\n",
    "    tracker.skip(\"grpo\")\n",
    "else:\n",
    "    tracker.start(\"grpo\")\n",
    "\n",
    "    batch = CONFIG[\"grpo_batch\"]\n",
    "    grad_accum = CONFIG[\"grpo_grad_accum\"]\n",
    "    max_steps = CONFIG[\"grpo_max_steps\"]\n",
    "    max_seq = CONFIG[\"grpo_seq_len\"]\n",
    "\n",
    "    grpo_checkpoint = \"checkpoints/core_agent_ipo/final\"\n",
    "    if not os.path.exists(grpo_checkpoint):\n",
    "        grpo_checkpoint = \"checkpoints/core_agent/final\"\n",
    "        print(f\"  IPO checkpoint not found, using: {grpo_checkpoint}\")\n",
    "\n",
    "    cmd = f\"python scripts/18_grpo_rl.py\"\n",
    "    cmd += f\" --checkpoint {grpo_checkpoint}\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    v4_features = []\n",
    "    v4_features.append(f\"Split LoRA ({CONFIG['moe_backend']})\")\n",
    "    if CONFIG[\"load_mode\"] == \"fp8\":\n",
    "        v4_features.append(\"FP8 weights\")\n",
    "    if CONFIG.get(\"fast_inference\"):\n",
    "        v4_features.append(\"vLLM inference\")\n",
    "    v4_features.append(\"Chunked batching (auto)\")\n",
    "    v4_features.append(\"Auto packing\")\n",
    "\n",
    "    if CONFIG[\"gpu_tier\"] == \"a100_40gb\":\n",
    "        print(\"NOTE: 40GB GPU \\u2014 GRPO sequence length capped at 16384\")\n",
    "\n",
    "    print(f\"Training with GRPO...\")\n",
    "    print(f\"  Checkpoint: {grpo_checkpoint}\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Max seq length: {max_seq}\")\n",
    "    print(f\"  Generations per prompt: {CONFIG['grpo_num_gen']}\")\n",
    "    print(f\"\\n  Features active:\")\n",
    "    for feat in v4_features:\n",
    "        print(f\"    \\u2713 {feat}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/core_agent_grpo\", \"checkpoints/core_agent_grpo\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")\n",
    "\n",
    "    tracker.complete(\"grpo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77039e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "elif not CONFIG[\"include_grpo\"]:\n",
    "    print(\"Skipping \\u2014 GRPO disabled\")\n",
    "else:\n",
    "    ckpt_path = \"checkpoints/core_agent_grpo/final\"\n",
    "\n",
    "    print(\"GRPO Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(ckpt_path):\n",
    "        files = os.listdir(ckpt_path)\n",
    "        print(f\"  \\u2713 GRPO checkpoint: {ckpt_path} ({len(files)} files)\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 GRPO checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6dccbf",
   "metadata": {},
   "source": [
    "## Step 6: Evaluation\n",
    "\n",
    "Evaluate the best checkpoint on held-out Rust tasks using execution-based metrics\n",
    "(cargo check, cargo test, clippy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedee434",
   "metadata": {},
   "source": [
    "### 6.1 Run Rust Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17010594",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "    tracker.skip(\"eval\")\n",
    "else:\n",
    "    tracker.start(\"eval\")\n",
    "\n",
    "    # Determine best checkpoint\n",
    "    if CONFIG[\"include_grpo\"] and os.path.exists(\"checkpoints/core_agent_grpo/final\"):\n",
    "        eval_checkpoint = \"checkpoints/core_agent_grpo/final\"\n",
    "    elif CONFIG[\"include_ipo\"] and os.path.exists(\"checkpoints/core_agent_ipo/final\"):\n",
    "        eval_checkpoint = \"checkpoints/core_agent_ipo/final\"\n",
    "    elif os.path.exists(\"checkpoints/core_agent/final\"):\n",
    "        eval_checkpoint = \"checkpoints/core_agent/final\"\n",
    "    else:\n",
    "        eval_checkpoint = \"checkpoints/lang_rust/final\"\n",
    "\n",
    "    num_samples = CONFIG[\"eval_num_samples\"]\n",
    "\n",
    "    print(f\"Evaluating checkpoint: {eval_checkpoint}\")\n",
    "    print(f\"Samples: {num_samples}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/eval_rust_agent.py \\\n",
    "        --checkpoint {eval_checkpoint} \\\n",
    "        --num_samples {num_samples}\n",
    "\n",
    "    drive_helper.backup(\"evals/rust_agent\", \"evals/rust_agent\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nResults backed up to Drive.\")\n",
    "\n",
    "    tracker.complete(\"eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c709ae2e",
   "metadata": {},
   "source": [
    "### 6.2 Check Promotion Gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb0d11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    print(\"Checking promotion gates...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/12_check_gates.py rust_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1078137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    import json\n",
    "\n",
    "    metrics_path = \"evals/rust_agent/metrics.json\"\n",
    "\n",
    "    if os.path.exists(metrics_path):\n",
    "        with open(metrics_path) as f:\n",
    "            metrics = json.load(f)\n",
    "\n",
    "        targets = {\n",
    "            \"cargo_check_pass_rate\": (0.85, \"higher\"),\n",
    "            \"cargo_test_pass_rate\": (0.70, \"higher\"),\n",
    "            \"clippy_clean_rate\": (0.80, \"higher\"),\n",
    "            \"iterations_to_green_median\": (3, \"lower\"),\n",
    "            \"diff_size_median\": (50, \"lower\"),\n",
    "            \"tool_call_format_accuracy\": (0.99, \"higher\"),\n",
    "            \"hallucinated_api_rate\": (0.05, \"lower\"),\n",
    "        }\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "        print(\"EVALUATION RESULTS\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"{'Metric':<32} {'Value':>8} {'Target':>8} {'Status':>8}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        for key, (target, direction) in targets.items():\n",
    "            value = metrics.get(key)\n",
    "            if value is None:\n",
    "                print(f\"{key:<32} {'N/A':>8} {target:>8} {'\\u2014':>8}\")\n",
    "                continue\n",
    "\n",
    "            if direction == \"higher\":\n",
    "                passed = value >= target\n",
    "            else:\n",
    "                passed = value <= target\n",
    "\n",
    "            status = \"\\u2713 PASS\" if passed else \"\\u2717 FAIL\"\n",
    "            fmt_val = f\"{value:.2%}\" if isinstance(value, float) and value <= 1 else f\"{value}\"\n",
    "            fmt_tgt = f\"{target:.0%}\" if isinstance(target, float) and target <= 1 else f\"{target}\"\n",
    "            print(f\"{key:<32} {fmt_val:>8} {fmt_tgt:>8} {status:>8}\")\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "    else:\n",
    "        print(f\"\\u2717 Metrics file not found at {metrics_path}\")\n",
    "        print(\"Run evaluation (6.1) first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00c8685",
   "metadata": {},
   "source": [
    "## Step 7: Test Model\n",
    "\n",
    "Load the trained model and generate Rust code interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a094bdb",
   "metadata": {},
   "source": [
    "### 7.1 Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a7979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "CHECKPOINT_PRIORITY = [\n",
    "    \"checkpoints/core_agent_grpo/final\",\n",
    "    \"checkpoints/core_agent_ipo/final\",\n",
    "    \"checkpoints/core_agent/final\",\n",
    "    \"checkpoints/lang_rust/final\",\n",
    "]\n",
    "\n",
    "# Merged model can be loaded directly (no adapter needed)\n",
    "MERGED_PATH = \"checkpoints/gpt-oss-20b-rust-merged\"\n",
    "\n",
    "MODEL_PATH = None\n",
    "is_adapter = False\n",
    "for path in CHECKPOINT_PRIORITY:\n",
    "    if os.path.exists(path) and os.path.exists(os.path.join(path, \"adapter_config.json\")):\n",
    "        MODEL_PATH = path\n",
    "        is_adapter = True\n",
    "        break\n",
    "\n",
    "if MODEL_PATH is None and os.path.exists(MERGED_PATH):\n",
    "    MODEL_PATH = MERGED_PATH\n",
    "    is_adapter = False\n",
    "\n",
    "if MODEL_PATH is None:\n",
    "    print(\"\\u2717 No checkpoint found. Train the model first.\")\n",
    "else:\n",
    "    print(f\"Loading model from: {MODEL_PATH}\")\n",
    "    print(f\"  Type: {'LoRA adapter' if is_adapter else 'merged model'}\")\n",
    "\n",
    "    # GPT-OSS fused MoE experts (GptOssExperts) don't support BNB 4-bit\n",
    "    # quantization at load time (no `down_projs` sub-module for traversal).\n",
    "    # See: https://github.com/unslothai/unsloth/issues/3775\n",
    "    #\n",
    "    # Strategy: use Unsloth's pre-quantized BNB model (already 4-bit, no\n",
    "    # quantizer runs at load). Falls back to bfloat16 if unavailable.\n",
    "    model = None\n",
    "    base_name = \"openai/gpt-oss-20b\"\n",
    "\n",
    "    # Try 1: Pre-quantized BNB 4-bit\n",
    "    try:\n",
    "        print(\"  Loading pre-quantized BNB 4-bit model...\")\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\",\n",
    "            max_seq_length=4096,\n",
    "            dtype=None,\n",
    "            load_in_4bit=False,  # Already quantized\n",
    "        )\n",
    "        print(\"  Mode: BNB 4-bit (pre-quantized)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Pre-quantized BNB failed: {e}\")\n",
    "\n",
    "    # Try 2: bfloat16 without quantization (~40GB, fits H100 80GB)\n",
    "    if model is None:\n",
    "        print(\"  Loading in bfloat16 (no quantization)...\")\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            base_name,\n",
    "            max_seq_length=4096,\n",
    "            dtype=torch.bfloat16,\n",
    "            load_in_4bit=False,\n",
    "        )\n",
    "        print(\"  Mode: bfloat16 (no quantization)\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if is_adapter:\n",
    "        print(f\"  Applying LoRA adapter from {MODEL_PATH}...\")\n",
    "        model = PeftModel.from_pretrained(model, MODEL_PATH)\n",
    "\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    print(\"\\u2713 Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f561ea",
   "metadata": {},
   "source": [
    "### 7.2 Generate Rust Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cb6f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"scripts\")\n",
    "from dataset_formatters.harmony import encode_harmony_messages\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    \"Write a Rust function `fn merge_sorted(a: &[i32], b: &[i32]) -> Vec<i32>` that merges two sorted slices into a single sorted vector.\",\n",
    "    \"This Rust code fails the borrow checker. Fix it:\\n```rust\\nfn main() {\\n    let mut v = vec![1, 2, 3];\\n    let first = &v[0];\\n    v.push(4);\\n    println!(\\\"{}\\\", first);\\n}\\n```\",\n",
    "    \"Write an async Rust function using tokio that fetches a URL with reqwest, retries up to 3 times on failure, and returns the response body as a String.\",\n",
    "]\n",
    "\n",
    "def generate_rust(prompt, max_tokens=1024):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted = encode_harmony_messages(\n",
    "        messages,\n",
    "        developer_instructions=\"You are a Rust programming expert. Write correct, idiomatic code.\",\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.3,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "for i, prompt in enumerate(TEST_PROMPTS, 1):\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Test {i}: {prompt[:80]}...\")\n",
    "    print(\"=\" * 60)\n",
    "    response = generate_rust(prompt)\n",
    "    print(response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ba99a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_PROMPT = \"Write a Rust function that reads a CSV file and returns the sum of a specified column.\"\n",
    "\n",
    "print(f\"Prompt: {CUSTOM_PROMPT}\")\n",
    "print(\"=\" * 60)\n",
    "print(generate_rust(CUSTOM_PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7422b1db",
   "metadata": {},
   "source": [
    "## Step 8: Export\n",
    "\n",
    "Merge the final adapter and export to HuggingFace + GGUF formats.\n",
    "\n",
    "**v4:** Optional QAT export for 97-100% MXFP4 quality retention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5ba5df",
   "metadata": {},
   "source": [
    "### 8.1 Export to GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e357abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.start(\"export\")\n",
    "\n",
    "ADAPTER_PRIORITY = [\n",
    "    \"checkpoints/core_agent_grpo/final\",\n",
    "    \"checkpoints/core_agent_ipo/final\",\n",
    "    \"checkpoints/core_agent/final\",\n",
    "    \"checkpoints/lang_rust/final\",\n",
    "]\n",
    "\n",
    "adapter_path = None\n",
    "for path in ADAPTER_PRIORITY:\n",
    "    if os.path.exists(path):\n",
    "        adapter_path = path\n",
    "        break\n",
    "\n",
    "if adapter_path is None:\n",
    "    print(\"\\u2717 No adapter checkpoint found.\")\n",
    "    tracker.fail(\"export\")\n",
    "else:\n",
    "    export_dir = \"checkpoints/gpt-oss-20b-rust-export-v4\"\n",
    "    print(f\"Exporting adapter: {adapter_path}\")\n",
    "    print(f\"Output: {export_dir}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/19_merge_adapter.py \\\n",
    "        --adapter_path {adapter_path} \\\n",
    "        --output_dir {export_dir} \\\n",
    "        --export_formats hf gguf_q4\n",
    "\n",
    "    drive_helper.backup(export_dir, \"checkpoints/gpt-oss-20b-rust-export-v4\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nExport backed up to Drive.\")\n",
    "\n",
    "    tracker.complete(\"export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65140ee6",
   "metadata": {},
   "source": [
    "### 8.2 QAT Export (Optional)\n",
    "\n",
    "Quantisation-Aware Training for MXFP4 deployment.\n",
    "Recovers 97-100% quality vs 59-89% with post-training quantisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171b5e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CONFIG.get(\"enable_qat_export\"):\n",
    "    print(\"QAT export disabled. Enable via widget toggle in Step 0.3.\")\n",
    "    print(\"\\nQAT recovers 97-100% quality when deploying to MXFP4,\")\n",
    "    print(\"vs 59-89% with standard post-training quantisation (PTQ).\")\n",
    "else:\n",
    "    export_dir = \"checkpoints/gpt-oss-20b-rust-export-v4\"\n",
    "    qat_dir = \"checkpoints/gpt-oss-20b-rust-qat\"\n",
    "\n",
    "    if not os.path.exists(export_dir):\n",
    "        print(\"\\u2717 Run standard export (8.1) first.\")\n",
    "    else:\n",
    "        print(\"Running QAT pass on merged model...\")\n",
    "        print(\"  This fine-tunes with MXFP4-aware quantisation at reduced LR (1e-5).\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        try:\n",
    "            import modelopt.torch.quantization as mtq\n",
    "            print(\"\\u2713 nvidia-modelopt available\")\n",
    "\n",
    "            print(\"\\nQAT pipeline (manual steps):\")\n",
    "            print(f\"  1. Load merged BF16 model from {export_dir}\")\n",
    "            print(f\"  2. mtq.quantize(model, config=mtq.MXFP4_DEFAULT_CFG)\")\n",
    "            print(f\"  3. Fine-tune for ~100 steps at LR 1e-5\")\n",
    "            print(f\"  4. Export to {qat_dir}\")\n",
    "        except ImportError:\n",
    "            print(\"\\u2717 nvidia-modelopt not installed.\")\n",
    "            print(\"  Install: pip install nvidia-modelopt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d62b51",
   "metadata": {},
   "source": [
    "### 8.3 Download GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede9bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    import glob\n",
    "\n",
    "    export_dir = \"checkpoints/gpt-oss-20b-rust-export-v4\"\n",
    "    gguf_files = glob.glob(os.path.join(export_dir, \"**/*.gguf\"), recursive=True)\n",
    "\n",
    "    if gguf_files:\n",
    "        gguf_path = gguf_files[0]\n",
    "        size_gb = os.path.getsize(gguf_path) / (1024**3)\n",
    "        print(f\"Downloading: {os.path.basename(gguf_path)} ({size_gb:.1f} GB)\")\n",
    "        files.download(gguf_path)\n",
    "    else:\n",
    "        print(\"\\u2717 No GGUF file found. Run export (8.1) first.\")\n",
    "else:\n",
    "    print(\"Download not available outside Colab.\")\n",
    "    print(\"GGUF file is at: checkpoints/gpt-oss-20b-rust-export-v4/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59ec414",
   "metadata": {},
   "source": [
    "### 8.4 Upload to HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20db3e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "HF_REPO_ID = \"\"  # e.g. \"your-username/gpt-oss-20b-rust-agent-v4\"\n",
    "HF_PRIVATE = True\n",
    "\n",
    "assert HF_REPO_ID, \"Set HF_REPO_ID above before running this cell.\"\n",
    "\n",
    "import glob\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Authenticate: try Colab Secrets first, then interactive login\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    hf_token = userdata.get(\"HF_TOKEN\")\n",
    "    print(\"Using HF_TOKEN from Colab Secrets.\")\n",
    "except Exception:\n",
    "    from huggingface_hub import login\n",
    "    login()\n",
    "    hf_token = None  # login() stores token globally\n",
    "\n",
    "api = HfApi(token=hf_token)\n",
    "\n",
    "# Create repo (no-op if it already exists)\n",
    "api.create_repo(repo_id=HF_REPO_ID, private=HF_PRIVATE, exist_ok=True)\n",
    "print(f\"Repo ready: https://huggingface.co/{HF_REPO_ID}\")\n",
    "\n",
    "# --- Model card ---\n",
    "export_dir = \"checkpoints/gpt-oss-20b-rust-export-v4\"\n",
    "hf_dir = os.path.join(export_dir, \"hf\")\n",
    "\n",
    "model_card = \"\"\"\\\n",
    "---\n",
    "base_model: openai/gpt-oss-20b\n",
    "tags:\n",
    "  - rust\n",
    "  - code-agent\n",
    "  - gpt-oss\n",
    "  - qlora\n",
    "  - unsloth\n",
    "  - grpo\n",
    "license: apache-2.0\n",
    "pipeline_tag: text-generation\n",
    "---\n",
    "\n",
    "# GPT-OSS 20B Rust Agent (v4)\n",
    "\n",
    "Fine-tuned from [openai/gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b) using\n",
    "Strandset-Rust-v1 + mutation enrichment with execution-grounded GRPO reinforcement learning.\n",
    "\n",
    "## Training Pipeline\n",
    "\n",
    "1. **Strandset Data** \\u2014 191K verified Rust examples (code gen, debug, review, preferences)\n",
    "2. **Mutation Generation** \\u2014 cargo-mutants based code mutations\n",
    "3. **Lang Adapter** \\u2014 Rust domain specialisation (enriched with cached mutation debug pairs)\n",
    "4. **Core Agent SFT** \\u2014 Debug/review training (enriched with mutation trajectories)\n",
    "5. **IPO** \\u2014 Preference optimisation (enriched with mutation preference pairs)\n",
    "6. **GRPO RL** \\u2014 Execution-grounded reinforcement learning\n",
    "\n",
    "Trained with [Unsloth](https://github.com/unslothai/unsloth) QLoRA on NVIDIA H100 80GB.\n",
    "\n",
    "## v4 Features\n",
    "\n",
    "- Widget-based configuration UI\n",
    "- Mutation enrichment across ALL training phases\n",
    "- Pipeline progress dashboard\n",
    "- Split LoRA + FP8 RL on H100\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"{repo_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{repo_id}\")\n",
    "```\n",
    "\n",
    "## GGUF\n",
    "\n",
    "A quantised GGUF file is included for use with llama.cpp or Ollama.\n",
    "\"\"\".format(repo_id=HF_REPO_ID)\n",
    "\n",
    "readme_path = os.path.join(hf_dir, \"README.md\")\n",
    "os.makedirs(hf_dir, exist_ok=True)\n",
    "with open(readme_path, \"w\") as f:\n",
    "    f.write(model_card)\n",
    "print(f\"Wrote model card to {readme_path}\")\n",
    "\n",
    "# --- Upload HF safetensors model ---\n",
    "assert os.path.isdir(hf_dir), f\"HF export dir not found: {hf_dir}. Run export (8.1) first.\"\n",
    "print(f\"Uploading HF model from {hf_dir} ...\")\n",
    "api.upload_folder(\n",
    "    folder_path=hf_dir,\n",
    "    repo_id=HF_REPO_ID,\n",
    "    commit_message=\"Upload merged HF model (v4 \\u2014 Strandset + mutation enrichment pipeline)\",\n",
    "    token=hf_token,\n",
    ")\n",
    "print(\"HF model uploaded.\")\n",
    "\n",
    "# --- Upload GGUF file ---\n",
    "gguf_files = glob.glob(os.path.join(export_dir, \"**/*.gguf\"), recursive=True)\n",
    "if gguf_files:\n",
    "    gguf_path = gguf_files[0]\n",
    "    gguf_name = os.path.basename(gguf_path)\n",
    "    size_gb = os.path.getsize(gguf_path) / (1024**3)\n",
    "    print(f\"Uploading GGUF: {gguf_name} ({size_gb:.1f} GB) ...\")\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=gguf_path,\n",
    "        path_in_repo=gguf_name,\n",
    "        repo_id=HF_REPO_ID,\n",
    "        commit_message=f\"Upload GGUF quantisation ({gguf_name})\",\n",
    "        token=hf_token,\n",
    "    )\n",
    "    print(\"GGUF uploaded.\")\n",
    "else:\n",
    "    print(\"No GGUF file found \\u2014 skipping. Run export (8.1) to generate one.\")\n",
    "\n",
    "print(f\"\\nDone! View your model at: https://huggingface.co/{HF_REPO_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a82159",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Complete!\n",
    "\n",
    "Your GPT-OSS 20B Rust coding agent (v4) is trained and ready to use.\n",
    "\n",
    "**v4 Pipeline:**\n",
    "1. Strandset-Rust-v1: 191K verified examples across all phases\n",
    "2. Mutation Enrichment: cargo-mutants data enriches lang_rust (cached), core_agent (trajectories), and IPO (preference pairs)\n",
    "3. Lang Adapter: Rust domain specialisation (LoRA rank 64)\n",
    "4. Core Agent SFT: Debug and review training (LoRA rank 128)\n",
    "5. IPO: Preference optimisation with enriched pairs\n",
    "6. GRPO RL: Execution-grounded reinforcement learning\n",
    "\n",
    "**Outputs:**\n",
    "- Checkpoints: `checkpoints/core_agent_{ipo,grpo}/final`\n",
    "- Evaluation: `evals/rust_agent/metrics.json`\n",
    "- Exported model: `checkpoints/gpt-oss-20b-rust-export-v4/`\n",
    "- All backed up to Google Drive: `gpt-oss-20b-rust-agent-v4/`\n",
    "\n",
    "**Next steps:**\n",
    "- Review evaluation metrics in Step 6\n",
    "- Test interactively in Step 7\n",
    "- Deploy the GGUF file with llama.cpp or Ollama\n",
    "- For MXFP4 deployment, enable QAT export in Step 8.2"
   ]
  },
  {
   "cell_type": "code",
   "id": "6nveei7arye",
   "source": "# Disconnect and release GPU runtime to stop billing\nfrom google.colab import runtime\nruntime.unassign()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}