{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4e5f6a7b8",
   "metadata": {},
   "source": [
    "# Train GPT-OSS 20B → Rust Coding Agent (v3 — Strandset)\n",
    "\n",
    "Simplified pipeline using [Strandset-Rust-v1](https://huggingface.co/datasets/Fortytwo-Network/Strandset-Rust-v1) (191K verified Rust examples, Apache 2.0) as the sole data source.\n",
    "\n",
    "**Key differences from v2:**\n",
    "- **No Rust toolchain** — no `rustup`, `cargo-mutants`, or compilation needed\n",
    "- **No mutation/trajectory generation** — data comes entirely from Strandset\n",
    "- **No GRPO** — no execution-based rewards without cargo\n",
    "- **IPO from synthetic preferences** — bug_detection pairs (fixed=chosen, buggy=rejected)\n",
    "\n",
    "**3-Phase Pipeline:**\n",
    "1. **Lang Adapter** — Rust domain specialisation via QLoRA (script 13 + 19)\n",
    "2. **Core Agent SFT** — Debug/review training from Strandset (script 14)\n",
    "3. **IPO Preference** — Synthetic preference pairs from bug_detection (script 17)\n",
    "\n",
    "**Requirements:**\n",
    "- **GPU**: A100 40GB+ (H100 80GB recommended for FP8)\n",
    "- **Storage**: Google Drive for persistent checkpoints\n",
    "- **No Rust toolchain required**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5f6a7b8c9",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6a7b8c9d0",
   "metadata": {},
   "source": [
    "### 0.1 Mount Google Drive & Clone Repository\n",
    "\n",
    "**PyCharm / headless users:** If `drive.mount()` doesn't work, set `use_service_account = True`\n",
    "and provide your service-account JSON key in Step 0.3."
   ]
  },
  {
   "cell_type": "code",
   "id": "d4e5f6a7b8c9d0e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T09:10:53.488435Z",
     "start_time": "2026-02-14T09:10:53.023896Z"
    }
   },
   "source": "import os\nimport sys\n\ntry:\n    import google.colab\n    IN_COLAB = True\nexcept ImportError:\n    IN_COLAB = False\n\nuse_service_account = False  # Use drive.mount() in Colab UI\n\nDRIVE_MOUNTED = False\n\nif IN_COLAB and not use_service_account:\n    try:\n        from google.colab import drive\n        drive.mount('/content/drive')\n        DRIVE_MOUNTED = True\n        print(\"Google Drive mounted\")\n    except Exception as e:\n        print(f\"drive.mount() failed: {e}\")\n        print(\"Falling back to local-only mode.\")\n        print(\"Tip: set use_service_account=True and provide a JSON key in Step 0.3.\")\nelif IN_COLAB and use_service_account:\n    print(\"Service-account mode selected \\u2014 skipping drive.mount()\")\n    print(\"Configure credentials in Step 0.3.\")\nelse:\n    print(\"Running locally\")\n\nREPO_URL = \"https://github.com/rmarnold/llm-training-pipeline.git\"\nBRANCH = \"main\"\n\nREPO_DIR = \"/content/llm-training-pipeline\"\n\nif IN_COLAB:\n    if os.path.exists(REPO_DIR):\n        %cd {REPO_DIR}\n        !git pull origin {BRANCH}\n    else:\n        !git clone -b {BRANCH} {REPO_URL} {REPO_DIR}\n        %cd {REPO_DIR}\n\n    PROJECT_ROOT = REPO_DIR\nelse:\n    PROJECT_ROOT = os.getcwd()\n\nos.chdir(PROJECT_ROOT)\nprint(f\"\\nProject root: {PROJECT_ROOT}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8c9d0e1f2",
   "metadata": {},
   "source": [
    "### 0.2 Install Dependencies\n",
    "\n",
    "Installs pipeline deps and latest Unsloth. **No Rust toolchain needed** — all training data\n",
    "comes from Strandset.\n",
    "\n",
    "**Note:** flash-attn is intentionally NOT installed. FA3 is incompatible with GPT-OSS\n",
    "backward passes. Unsloth's Flex Attention replaces it automatically."
   ]
  },
  {
   "cell_type": "code",
   "id": "f6a7b8c9d0e1f2a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T07:58:01.031009Z",
     "start_time": "2026-02-14T07:54:39.733340Z"
    }
   },
   "source": [
    "if IN_COLAB:\n",
    "    print(\"Installing Python dependencies...\")\n",
    "    print(\"=\" * 60)\n",
    "    !pip install -q -e \".[gpt_oss,colab]\"\n",
    "\n",
    "    # Fix pyarrow binary incompatibility with datasets 4.x on Colab\n",
    "    !pip install -q --force-reinstall pyarrow\n",
    "\n",
    "    # Force latest Unsloth with Split LoRA + FP8 RL\n",
    "    print(\"\\nInstalling latest Unsloth (Split LoRA + Flex Attention)...\")\n",
    "    !pip install -q --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo\n",
    "    !pip install -q \"unsloth[colab-new]\"\n",
    "\n",
    "    # vLLM for FP8 inference (H100 only, optional)\n",
    "    !pip install -q vllm>=0.12.0 2>/dev/null || true\n",
    "\n",
    "    # Verification\n",
    "    from importlib.metadata import version, PackageNotFoundError\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Dependency Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for pkg in [\"unsloth\", \"trl\", \"peft\", \"datasets\", \"tiktoken\", \"vllm\"]:\n",
    "        try:\n",
    "            ver = version(pkg)\n",
    "            print(f\"\\u2713 {pkg}: {ver}\")\n",
    "        except PackageNotFoundError:\n",
    "            if pkg == \"vllm\":\n",
    "                print(f\"\\u2014 {pkg}: not installed (optional, H100 FP8 only)\")\n",
    "            else:\n",
    "                print(f\"\\u2717 {pkg}: not installed\")\n",
    "\n",
    "    print(\"\\nNote: No Rust toolchain needed for v3 (Strandset-only pipeline)\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"Running locally \\u2014 ensure deps are installed:\")\n",
    "    print(\"  pip install -e '.[gpt_oss]'\")\n",
    "    print(\"  pip install --upgrade unsloth unsloth_zoo\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing Python dependencies...\n",
      "============================================================\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m141.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.3/432.3 kB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m173.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.5/376.5 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m915.7/915.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m118.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.9/837.9 kB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m116.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building editable for llm-training-pipeline (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for pyfastcopy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.9.0+cu128 requires torch==2.9.0, but you have torch 2.10.0 which is incompatible.\n",
      "fastai 2.8.6 requires torch<2.10,>=1.10, but you have torch 2.10.0 which is incompatible.\n",
      "cuda-python 12.9.5 requires cuda-bindings~=12.9.5, but you have cuda-bindings 12.9.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "Installing latest Unsloth (Split LoRA + Flex Attention)...\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.3/432.3 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.5/376.5 kB\u001b[0m \u001b[31m536.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: unsloth 2026.2.1 does not provide the extra 'triton'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "============================================================\n",
      "Dependency Verification:\n",
      "============================================================\n",
      "✓ unsloth: 2026.2.1\n",
      "✓ trl: 0.24.0\n",
      "✓ peft: 0.18.1\n",
      "✓ datasets: 4.3.0\n",
      "✓ tiktoken: 0.12.0\n",
      "✓ vllm: 0.15.1\n",
      "\n",
      "Note: No Rust toolchain needed for v3 (Strandset-only pipeline)\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0e1f2a3b4",
   "metadata": {},
   "source": [
    "### 0.3 Configure Pipeline\n",
    "\n",
    "**Training Scope** (`training_scope`):\n",
    "- `full` \\u2014 All 3 phases (Lang Adapter + Core Agent + IPO)\n",
    "- `quick_test` \\u2014 Short runs (100 steps each) to verify setup\n",
    "- `lang_adapter_only` \\u2014 Only train lang_rust adapter + merge\n",
    "\n",
    "**Service Account Setup** (for Drive backup):\n",
    "1. Set `use_service_account = True` in cell 0.1\n",
    "2. Run cell 0.3 \\u2014 it will try Colab Secrets, then file, then paste prompt\n",
    "3. Set `DRIVE_FOLDER_ID` in Colab Secrets, or set `drive_folder_id` below"
   ]
  },
  {
   "cell_type": "code",
   "id": "b8c9d0e1f2a3b4c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T07:58:52.855286Z",
     "start_time": "2026-02-14T07:58:01.091339Z"
    }
   },
   "source": [
    "import json\n",
    "training_scope = \"quick_test\"  # \"full\", \"quick_test\", \"lang_adapter_only\"\n",
    "\n",
    "gpu_tier = \"h100_80gb\"  # \"a100_40gb\", \"a100_80gb\", \"h100_80gb\"\n",
    "\n",
    "max_steps_override = 0  # Set >0 to cap all stages (0 = use defaults)\n",
    "\n",
    "include_ipo = True  # False to skip IPO preference training\n",
    "\n",
    "enable_qat_export = False  # True for MXFP4 QAT export\n",
    "\n",
    "include_mutations = False  # True to run cargo-mutants in background (requires Rust toolchain)\n",
    "\n",
    "# ============================================================\n",
    "# SERVICE ACCOUNT CREDENTIALS\n",
    "# ============================================================\n",
    "# Priority order:\n",
    "#   1. Existing VALID file at /content/service_account.json (instant, no timeout)\n",
    "#   2. Colab Secrets (only if no valid file — may timeout outside browser UI)\n",
    "#   3. Paste JSON key via input() prompt\n",
    "#   4. Fall back to local mode (no Drive backup)\n",
    "\n",
    "drive_folder_id = \"18UpFpUhiNrs2Etha0uFjSGWmj1Ee1SnX\"  # Google Drive folder ID\n",
    "\n",
    "_SA_VM_PATH = \"/content/service_account.json\"\n",
    "_FOLDER_ID_PATH = \"/content/drive_folder_id.txt\"\n",
    "service_account_key = \"\"\n",
    "\n",
    "def _is_json(s):\n",
    "    \"\"\"Check if string looks like JSON (not a folder ID).\"\"\"\n",
    "    return s.strip().startswith(\"{\")\n",
    "\n",
    "def _validate_sa_file(path):\n",
    "    \"\"\"Check that a service account JSON file exists and contains valid JSON.\"\"\"\n",
    "    try:\n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "        return isinstance(data, dict) and \"type\" in data\n",
    "    except (json.JSONDecodeError, OSError, ValueError):\n",
    "        return False\n",
    "\n",
    "if use_service_account and IN_COLAB:\n",
    "    # 1. Check for existing VALID file first (avoids Colab Secrets timeout on re-runs)\n",
    "    if os.path.exists(_SA_VM_PATH) and _validate_sa_file(_SA_VM_PATH):\n",
    "        service_account_key = _SA_VM_PATH\n",
    "        print(f\"Using existing key file: {_SA_VM_PATH}\")\n",
    "    else:\n",
    "        if os.path.exists(_SA_VM_PATH):\n",
    "            os.remove(_SA_VM_PATH)\n",
    "            print(f\"Removed invalid/empty key file: {_SA_VM_PATH}\")\n",
    "\n",
    "        # 2. Try Colab Secrets (may timeout if not running in browser UI)\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            _key_json = userdata.get(\"SERVICE_ACCOUNT_KEY\")\n",
    "            if _key_json:\n",
    "                # Validate before saving\n",
    "                json.loads(_key_json)\n",
    "                with open(_SA_VM_PATH, \"w\") as _f:\n",
    "                    _f.write(_key_json)\n",
    "                service_account_key = _SA_VM_PATH\n",
    "                print(\"Service account key loaded from Colab Secrets.\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"  Colab Secret SERVICE_ACCOUNT_KEY contains invalid JSON.\")\n",
    "        except Exception as _e:\n",
    "            print(f\"  Colab Secrets lookup failed: {type(_e).__name__}: {_e}\")\n",
    "\n",
    "        # 3. Fall back to paste prompt\n",
    "        if not service_account_key:\n",
    "            try:\n",
    "                print(\"No service account key found.\")\n",
    "                _key_text = input(\"Paste service account JSON (entire content in one go): \")\n",
    "                _key_text = _key_text.strip()\n",
    "                if _key_text:\n",
    "                    json.loads(_key_text)\n",
    "                    with open(_SA_VM_PATH, \"w\") as _f:\n",
    "                        _f.write(_key_text)\n",
    "                    service_account_key = _SA_VM_PATH\n",
    "                    print(f\"Saved to {_SA_VM_PATH}\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"  Invalid JSON — key not saved.\")\n",
    "            except EOFError:\n",
    "                pass\n",
    "\n",
    "    # Resolve drive_folder_id: saved file > Colab Secrets > input prompt\n",
    "    if not drive_folder_id and os.path.exists(_FOLDER_ID_PATH):\n",
    "        with open(_FOLDER_ID_PATH) as _f:\n",
    "            drive_folder_id = _f.read().strip()\n",
    "        if drive_folder_id:\n",
    "            print(f\"Using saved folder ID from {_FOLDER_ID_PATH}\")\n",
    "\n",
    "    if not drive_folder_id:\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            _fid = userdata.get(\"DRIVE_FOLDER_ID\") or \"\"\n",
    "            if _fid and not _is_json(_fid):\n",
    "                drive_folder_id = _fid\n",
    "                print(f\"Drive folder ID loaded from Colab Secrets.\")\n",
    "            elif _fid:\n",
    "                print(\"WARNING: DRIVE_FOLDER_ID Colab Secret contains JSON, not a folder ID. Ignoring.\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if not drive_folder_id and service_account_key:\n",
    "        _fid = input(\"Enter Google Drive folder ID (from URL): \").strip()\n",
    "        if _fid and not _is_json(_fid):\n",
    "            drive_folder_id = _fid\n",
    "            # Persist so we don't have to re-enter on re-runs\n",
    "            with open(_FOLDER_ID_PATH, \"w\") as _f:\n",
    "                _f.write(drive_folder_id)\n",
    "            print(f\"Saved folder ID to {_FOLDER_ID_PATH}\")\n",
    "        elif _fid:\n",
    "            print(\"ERROR: That looks like JSON, not a folder ID.\")\n",
    "            print(\"The folder ID is the part after /folders/ in the Google Drive URL.\")\n",
    "\n",
    "    if not service_account_key:\n",
    "        print(\"No service account key — Drive backup disabled.\")\n",
    "\n",
    "elif use_service_account:\n",
    "    for _path in [_SA_VM_PATH, \"service_account.json\"]:\n",
    "        if os.path.exists(_path):\n",
    "            service_account_key = _path\n",
    "            print(f\"Using key file: {_path}\")\n",
    "            break\n",
    "    if not service_account_key:\n",
    "        print(\"Running locally — set service_account_key to your JSON key path.\")\n",
    "\n",
    "# ============================================================\n",
    "# DRIVE MODE\n",
    "# ============================================================\n",
    "import importlib\n",
    "import scripts.pipeline_lib.drive_utils as _du_mod\n",
    "importlib.reload(_du_mod)  # pick up any fixes from git pull\n",
    "from scripts.pipeline_lib.drive_utils import DriveHelper\n",
    "\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/gpt-oss-20b-rust-agent-v3\"\n",
    "\n",
    "if DRIVE_MOUNTED:\n",
    "    DRIVE_MODE = \"mounted\"\n",
    "elif use_service_account and service_account_key and drive_folder_id:\n",
    "    DRIVE_MODE = \"service_account\"\n",
    "else:\n",
    "    DRIVE_MODE = \"local\"\n",
    "\n",
    "drive_helper = DriveHelper(\n",
    "    mode=DRIVE_MODE,\n",
    "    drive_base=DRIVE_BASE,\n",
    "    credentials_path=service_account_key or None,\n",
    "    folder_id=drive_folder_id or None,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# GPU TIER CONFIGS\n",
    "# ============================================================\n",
    "\n",
    "GPU_CONFIGS = {\n",
    "    \"a100_40gb\": {\n",
    "        \"moe_backend\": \"unsloth_triton\",\n",
    "        \"load_mode\": \"4bit\",\n",
    "        \"fast_inference\": False,\n",
    "        \"lang_rust\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 8192, \"max_steps\": 3000},\n",
    "        \"core_agent\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 12288, \"max_steps\": 2000},\n",
    "        \"ipo\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 12288, \"max_steps\": 1000},\n",
    "    },\n",
    "    \"a100_80gb\": {\n",
    "        \"moe_backend\": \"unsloth_triton\",\n",
    "        \"load_mode\": \"4bit\",\n",
    "        \"fast_inference\": False,\n",
    "        \"lang_rust\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 8192, \"max_steps\": 5000},\n",
    "        \"core_agent\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 16384, \"max_steps\": 3000},\n",
    "        \"ipo\": {\"batch\": 1, \"grad_accum\": 16, \"seq_len\": 16384, \"max_steps\": 2000},\n",
    "    },\n",
    "    \"h100_80gb\": {\n",
    "        \"moe_backend\": \"grouped_mm\",\n",
    "        \"load_mode\": \"fp8\",\n",
    "        \"fast_inference\": True,\n",
    "        \"lang_rust\": {\"batch\": 4, \"grad_accum\": 2, \"seq_len\": 8192, \"max_steps\": 5000},\n",
    "        \"core_agent\": {\"batch\": 2, \"grad_accum\": 2, \"seq_len\": 16384, \"max_steps\": 3000},\n",
    "        \"ipo\": {\"batch\": 2, \"grad_accum\": 4, \"seq_len\": 16384, \"max_steps\": 2000},\n",
    "    },\n",
    "}\n",
    "\n",
    "if training_scope == \"quick_test\":\n",
    "    max_steps_override = 100\n",
    "\n",
    "gpu_cfg = GPU_CONFIGS[gpu_tier]\n",
    "\n",
    "# Mutation generation (optional, requires Rust toolchain)\n",
    "if include_mutations:\n",
    "    import multiprocessing\n",
    "    import os as _os\n",
    "    cpu_count = multiprocessing.cpu_count()\n",
    "    try:\n",
    "        _mem_bytes = _os.sysconf(\"SC_PAGE_SIZE\") * _os.sysconf(\"SC_PHYS_PAGES\")\n",
    "        total_ram_gb = _mem_bytes / (1024**3)\n",
    "        ram_based_jobs = max(1, int(total_ram_gb / 4))\n",
    "    except (ValueError, OSError):\n",
    "        total_ram_gb = 0\n",
    "        ram_based_jobs = cpu_count\n",
    "    cpu_based_jobs = max(1, cpu_count - 2)\n",
    "    mutation_jobs = min(cpu_based_jobs, ram_based_jobs)\n",
    "    mutation_repo_workers = max(1, mutation_jobs // 4)\n",
    "else:\n",
    "    mutation_jobs = 0\n",
    "    mutation_repo_workers = 0\n",
    "\n",
    "CONFIG = {\n",
    "    \"training_scope\": training_scope,\n",
    "    \"gpu_tier\": gpu_tier,\n",
    "    \"include_ipo\": include_ipo,\n",
    "    \"enable_qat_export\": enable_qat_export,\n",
    "    \"moe_backend\": gpu_cfg[\"moe_backend\"],\n",
    "    \"load_mode\": gpu_cfg[\"load_mode\"],\n",
    "    \"fast_inference\": gpu_cfg[\"fast_inference\"],\n",
    "    # Lang adapter\n",
    "    \"lang_rust_batch\": gpu_cfg[\"lang_rust\"][\"batch\"],\n",
    "    \"lang_rust_grad_accum\": gpu_cfg[\"lang_rust\"][\"grad_accum\"],\n",
    "    \"lang_rust_seq_len\": gpu_cfg[\"lang_rust\"][\"seq_len\"],\n",
    "    \"lang_rust_max_steps\": max_steps_override or gpu_cfg[\"lang_rust\"][\"max_steps\"],\n",
    "    # Core agent\n",
    "    \"core_agent_batch\": gpu_cfg[\"core_agent\"][\"batch\"],\n",
    "    \"core_agent_grad_accum\": gpu_cfg[\"core_agent\"][\"grad_accum\"],\n",
    "    \"core_agent_seq_len\": gpu_cfg[\"core_agent\"][\"seq_len\"],\n",
    "    \"core_agent_max_steps\": max_steps_override or gpu_cfg[\"core_agent\"][\"max_steps\"],\n",
    "    # IPO\n",
    "    \"ipo_batch\": gpu_cfg[\"ipo\"][\"batch\"],\n",
    "    \"ipo_grad_accum\": gpu_cfg[\"ipo\"][\"grad_accum\"],\n",
    "    \"ipo_seq_len\": gpu_cfg[\"ipo\"][\"seq_len\"],\n",
    "    \"ipo_max_steps\": max_steps_override or gpu_cfg[\"ipo\"][\"max_steps\"],\n",
    "    # Eval\n",
    "    \"eval_num_samples\": 10 if training_scope == \"quick_test\" else 50,\n",
    "    # Mutation generation (optional)\n",
    "    \"include_mutations\": include_mutations,\n",
    "    \"max_mutations_per_repo\": 50,\n",
    "    \"mutation_jobs\": mutation_jobs,\n",
    "    \"mutation_repo_workers\": mutation_repo_workers,\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PIPELINE CONFIGURATION (v3 \\u2014 Strandset)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nScope: {training_scope.upper()}\")\n",
    "print(f\"GPU tier: {gpu_tier}\")\n",
    "print(f\"MoE backend: {CONFIG['moe_backend']}\")\n",
    "print(f\"Load mode: {CONFIG['load_mode']}\")\n",
    "print(f\"Fast inference (vLLM): {CONFIG['fast_inference']}\")\n",
    "print(f\"Include IPO: {include_ipo}\")\n",
    "print(f\"Include mutations (background): {include_mutations}\")\n",
    "print(f\"QAT export: {enable_qat_export}\")\n",
    "print(f\"Drive mode: {DRIVE_MODE}\")\n",
    "if max_steps_override:\n",
    "    print(f\"Max steps override: {max_steps_override}\")\n",
    "print(f\"\\nLang Adapter:  batch={CONFIG['lang_rust_batch']} x grad_accum={CONFIG['lang_rust_grad_accum']}, seq={CONFIG['lang_rust_seq_len']}, steps={CONFIG['lang_rust_max_steps']}\")\n",
    "print(f\"Core Agent:    batch={CONFIG['core_agent_batch']} x grad_accum={CONFIG['core_agent_grad_accum']}, seq={CONFIG['core_agent_seq_len']}, steps={CONFIG['core_agent_max_steps']}\")\n",
    "if include_ipo:\n",
    "    print(f\"IPO:           batch={CONFIG['ipo_batch']} x grad_accum={CONFIG['ipo_grad_accum']}, seq={CONFIG['ipo_seq_len']}, steps={CONFIG['ipo_max_steps']}\")\n",
    "print(\"=\" * 60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Colab Secrets lookup failed: TimeoutException: Requesting secret SERVICE_ACCOUNT_KEY timed out. Secrets can only be fetched when running from the Colab UI.\n",
      "No service account key found.\n",
      "Saved to /content/service_account.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:scripts.pipeline_lib.drive_utils:DriveHelper: folder is on personal My Drive, not a Shared Drive. Service accounts cannot CREATE new files here (no storage quota). Uploads of new files will be skipped. To fix: create a Shared Drive (requires Google Workspace) and move your folder there.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PIPELINE CONFIGURATION (v3 — Strandset)\n",
      "============================================================\n",
      "\n",
      "Scope: QUICK_TEST\n",
      "GPU tier: h100_80gb\n",
      "MoE backend: grouped_mm\n",
      "Load mode: fp8\n",
      "Fast inference (vLLM): True\n",
      "Include IPO: True\n",
      "QAT export: False\n",
      "Drive mode: service_account\n",
      "Max steps override: 100\n",
      "\n",
      "Lang Adapter:  batch=2 x grad_accum=4, seq=8192, steps=100\n",
      "Core Agent:    batch=1 x grad_accum=4, seq=16384, steps=100\n",
      "IPO:           batch=1 x grad_accum=16, seq=16384, steps=100\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2a3b4c5d6",
   "metadata": {},
   "source": [
    "### 0.4 Set Up Persistent Storage"
   ]
  },
  {
   "cell_type": "code",
   "id": "d0e1f2a3b4c5d6e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T07:58:57.054342Z",
     "start_time": "2026-02-14T07:58:52.906843Z"
    }
   },
   "source": [
    "DRIVE_SUBDIRS = [\n",
    "    \"checkpoints/lang_rust\",\n",
    "    \"checkpoints/core_agent\",\n",
    "    \"checkpoints/core_agent_ipo\",\n",
    "    \"checkpoints/gpt-oss-20b-rust-merged\",\n",
    "    \"data/rust/strandset\",\n",
    "    \"logs\",\n",
    "]\n",
    "\n",
    "if DRIVE_MODE == \"mounted\":\n",
    "    print(f\"Setting up storage at: {DRIVE_BASE}\")\n",
    "    for subdir in DRIVE_SUBDIRS:\n",
    "        os.makedirs(os.path.join(DRIVE_BASE, subdir), exist_ok=True)\n",
    "\n",
    "    for dir_name in [\"checkpoints\", \"data\", \"logs\"]:\n",
    "        local_path = os.path.join(PROJECT_ROOT, dir_name)\n",
    "        drive_path = os.path.join(DRIVE_BASE, dir_name)\n",
    "\n",
    "        if os.path.exists(local_path) and not os.path.islink(local_path):\n",
    "            !cp -r {local_path}/* {drive_path}/ 2>/dev/null || true\n",
    "            !rm -rf {local_path}\n",
    "        elif os.path.islink(local_path):\n",
    "            os.unlink(local_path)\n",
    "\n",
    "        os.symlink(drive_path, local_path)\n",
    "        print(f\"  {dir_name} -> Drive (mounted)\")\n",
    "\n",
    "elif DRIVE_MODE == \"service_account\":\n",
    "    print(\"Setting up local storage + Drive API restore...\")\n",
    "    for subdir in DRIVE_SUBDIRS:\n",
    "        os.makedirs(os.path.join(PROJECT_ROOT, subdir), exist_ok=True)\n",
    "        drive_helper.ensure_dir(subdir)\n",
    "\n",
    "    for dir_name in [\"checkpoints\", \"data\", \"logs\"]:\n",
    "        local_path = os.path.join(PROJECT_ROOT, dir_name)\n",
    "        if os.path.islink(local_path):\n",
    "            os.unlink(local_path)\n",
    "            os.makedirs(local_path, exist_ok=True)\n",
    "        print(f\"  {dir_name} -> local (backed up via Drive API)\")\n",
    "\n",
    "    print(\"\\nRestoring existing data from Drive...\")\n",
    "    for subdir in DRIVE_SUBDIRS:\n",
    "        local_target = os.path.join(PROJECT_ROOT, subdir)\n",
    "        drive_helper.restore(subdir, local_target)\n",
    "    print(\"Restore complete.\")\n",
    "\n",
    "else:\n",
    "    for d in [\"checkpoints\", \"data/rust\", \"logs\"]:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "    print(\"Local directories created (no Drive backup).\")\n",
    "\n",
    "print(\"\\nStorage ready!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up local storage + Drive API restore...\n",
      "  checkpoints -> local (backed up via Drive API)\n",
      "  data -> local (backed up via Drive API)\n",
      "  logs -> local (backed up via Drive API)\n",
      "\n",
      "Restoring existing data from Drive...\n",
      "Restore complete.\n",
      "\n",
      "Storage ready!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4c5d6e7f8",
   "metadata": {},
   "source": [
    "### 0.5 Check GPU & Configure MoE Backend"
   ]
  },
  {
   "cell_type": "code",
   "id": "f2a3b4c5d6e7f8a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T07:58:57.438465Z",
     "start_time": "2026-02-14T07:58:57.104217Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    capability = torch.cuda.get_device_capability()\n",
    "    is_h100 = \"H100\" in gpu_name or \"H200\" in gpu_name or \"B200\" in gpu_name\n",
    "\n",
    "    CONFIG[\"use_fp8\"] = capability[0] >= 9 and is_h100\n",
    "\n",
    "    if is_h100:\n",
    "        detected_tier = \"h100_80gb\"\n",
    "    elif gpu_memory >= 70:\n",
    "        detected_tier = \"a100_80gb\"\n",
    "    else:\n",
    "        detected_tier = \"a100_40gb\"\n",
    "\n",
    "    if detected_tier != CONFIG[\"gpu_tier\"]:\n",
    "        print(f\"NOTE: Auto-detected {detected_tier}, overriding configured {CONFIG['gpu_tier']}\")\n",
    "        CONFIG[\"gpu_tier\"] = detected_tier\n",
    "        gpu_cfg = GPU_CONFIGS[detected_tier]\n",
    "        CONFIG[\"moe_backend\"] = gpu_cfg[\"moe_backend\"]\n",
    "        CONFIG[\"load_mode\"] = gpu_cfg[\"load_mode\"]\n",
    "        CONFIG[\"fast_inference\"] = gpu_cfg[\"fast_inference\"]\n",
    "\n",
    "    os.environ[\"UNSLOTH_MOE_BACKEND\"] = CONFIG[\"moe_backend\"]\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"GPU: {gpu_name} ({gpu_memory:.0f} GB)\")\n",
    "    print(f\"Compute capability: {capability[0]}.{capability[1]}\")\n",
    "    print(f\"Tier: {CONFIG['gpu_tier']}\")\n",
    "    print(f\"\\nSplit LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(f\"Load mode: {CONFIG['load_mode']}\")\n",
    "    print(f\"FP8 available: {CONFIG['use_fp8']}\")\n",
    "    print(f\"Fast inference (vLLM): {CONFIG['fast_inference']}\")\n",
    "\n",
    "    if gpu_memory < 40:\n",
    "        print(\"\\nWARNING: <40 GB VRAM. Long-context training (16K+) may OOM.\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"No GPU detected!\")\n",
    "    CONFIG[\"use_fp8\"] = False\n",
    "    os.environ[\"UNSLOTH_MOE_BACKEND\"] = \"native_torch\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GPU: NVIDIA H100 80GB HBM3 (79 GB)\n",
      "Compute capability: 9.0\n",
      "Tier: h100_80gb\n",
      "\n",
      "Split LoRA backend: grouped_mm\n",
      "Load mode: fp8\n",
      "FP8 available: True\n",
      "Fast inference (vLLM): True\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6e7f8a9b0",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Data Preparation\n",
    "\n",
    "Downloads Strandset-Rust-v1 from HuggingFace, parses the 15 task categories,\n",
    "and formats everything in Harmony for each training stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5d6e7f8a9b0c1",
   "metadata": {},
   "source": [
    "### 1.1 Download & Format Strandset"
   ]
  },
  {
   "cell_type": "code",
   "id": "c5d6e7f8a9b0c1d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T07:59:07.928397Z",
     "start_time": "2026-02-14T07:58:57.445534Z"
    }
   },
   "source": [
    "max_samples = 500 if CONFIG[\"training_scope\"] == \"quick_test\" else 0  # 0 = all\n",
    "\n",
    "print(\"Downloading & formatting Strandset-Rust-v1...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cmd = \"python scripts/20_prepare_strandset.py\"\n",
    "if max_samples:\n",
    "    cmd += f\" --max_samples {max_samples}\"\n",
    "if not CONFIG[\"include_ipo\"]:\n",
    "    cmd += \" --no-preferences\"\n",
    "\n",
    "!{cmd}\n",
    "\n",
    "drive_helper.backup(\"data/rust/strandset\", \"data/rust/strandset\")\n",
    "if DRIVE_MODE != \"local\":\n",
    "    print(\"\\nBacked up Strandset data to Drive.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading & formatting Strandset-Rust-v1...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Preparing Strandset-Rust-v1\n",
      "============================================================\n",
      "  Dataset: Fortytwo-Network/Strandset-Rust-v1\n",
      "  Output: data/rust/strandset\n",
      "  Max samples: 500\n",
      "\n",
      "Loading dataset...\n",
      "README.md: 6.93kB [00:00, 35.5MB/s]\n",
      "data/train-00000-of-00001.parquet: 100% 109M/109M [00:01<00:00, 94.0MB/s] \n",
      "data/test-00000-of-00001.parquet: 100% 359k/359k [00:00<00:00, 1.14MB/s]\n",
      "Generating train split: 100% 191008/191008 [00:00<00:00, 575187.72 examples/s]\n",
      "Generating test split: 100% 225/225 [00:00<00:00, 66968.38 examples/s]\n",
      "  Train split: 191,008 examples\n",
      "  Test split: 225 examples\n",
      "\n",
      "Processing 500 examples...\n",
      "\n",
      "Saving datasets...\n",
      "Saving the dataset (1/1 shards): 100% 419/419 [00:00<00:00, 276453.26 examples/s]\n",
      "  lang_rust/train: 419 examples -> data/rust/strandset/lang_rust/train\n",
      "Saving the dataset (1/1 shards): 100% 48/48 [00:00<00:00, 36242.41 examples/s]\n",
      "  core_agent/train: 48 examples -> data/rust/strandset/core_agent/train\n",
      "Saving the dataset (1/1 shards): 100% 3/3 [00:00<00:00, 1730.32 examples/s]\n",
      "  ipo/train: 3 preference pairs -> data/rust/strandset/ipo/train\n",
      "Saving the dataset (1/1 shards): 100% 180/180 [00:00<00:00, 137268.13 examples/s]\n",
      "  eval/test: 180 examples -> data/rust/strandset/eval/test\n",
      "\n",
      "============================================================\n",
      "Strandset preparation complete!\n",
      "============================================================\n",
      "  Total processed: 500\n",
      "  Lang adapter:    419\n",
      "  Core agent:      48 (debug=3, review=45)\n",
      "  IPO pairs:       3\n",
      "  Skipped (empty): 33\n",
      "  Skipped (parse): 0\n",
      "\n",
      "  Category breakdown:\n",
      "    code_generation: 92\n",
      "    docstring_generation: 70\n",
      "    comment_generation: 65\n",
      "    code_summarization: 60\n",
      "    code_explanation: 48\n",
      "    variable_naming: 47\n",
      "    function_naming: 46\n",
      "    code_refactoring: 41\n",
      "    code_review: 17\n",
      "    code_optimization: 11\n",
      "    bug_detection: 3\n",
      "\n",
      "  Stats: data/rust/strandset/stats.json\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"storageQuotaExceeded\"\n",
      "WARNING:scripts.pipeline_lib.drive_utils:Drive backup skipped — service account has no storage quota on personal My Drive. To enable backups, move your folder to a Shared Drive (requires Google Workspace). Training data is safe on the local VM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backed up Strandset data to Drive.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "dpx9uwuvg86",
   "source": "### 1.1b Generate Mutations (Optional, Background)\n\nIf `include_mutations=True`, starts `cargo-mutants` in the background during training.\nMutations are used to generate trajectories that enrich core_agent training data.\nRequires Rust toolchain (`cargo`, `cargo-mutants`).\n\nMonitor progress: `!tail -f logs/mutation_gen.log`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "1mfhgpiaebaj",
   "source": "import subprocess\n\nmutation_proc = None  # Will hold the background process\n\nif not CONFIG[\"include_mutations\"]:\n    print(\"Mutation generation disabled (include_mutations=False)\")\n    print(\"Set include_mutations=True in Step 0.3 to enrich core_agent data with trajectories.\")\nelse:\n    max_muts = CONFIG[\"max_mutations_per_repo\"]\n    jobs = CONFIG[\"mutation_jobs\"]\n    repo_workers = CONFIG[\"mutation_repo_workers\"]\n\n    # Determine backup dir for incremental per-repo backup to Drive\n    backup_dir = None\n    if DRIVE_MODE == \"mounted\":\n        backup_dir = os.path.join(DRIVE_BASE, \"data/rust/mutations\")\n        os.makedirs(backup_dir, exist_ok=True)\n\n    print(f\"Starting mutation generation in BACKGROUND...\")\n    print(f\"  Max {max_muts}/repo, {jobs} jobs, {repo_workers} repo workers\")\n    if backup_dir:\n        print(f\"  Per-repo backup: {backup_dir}\")\n    print(\"=\" * 60)\n\n    os.makedirs(\"logs\", exist_ok=True)\n\n    mutation_cmd = [\n        \"python\", \"scripts/16_generate_mutations.py\",\n        \"--max_mutations_per_repo\", str(max_muts),\n        \"--jobs\", str(jobs),\n        \"--repo-workers\", str(repo_workers),\n    ]\n    if backup_dir:\n        mutation_cmd += [\"--backup-dir\", backup_dir]\n\n    mutation_proc = subprocess.Popen(\n        mutation_cmd,\n        stdout=open(\"logs/mutation_gen.log\", \"w\"),\n        stderr=subprocess.STDOUT,\n    )\n    print(f\"Mutation generation started in background (PID: {mutation_proc.pid})\")\n    print(f\"Monitor: !tail -f logs/mutation_gen.log\")\n    print(f\"\\nProceeding to training...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f8a9b0c1d2e3",
   "metadata": {},
   "source": [
    "### 1.2 Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "e7f8a9b0c1d2e3f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T07:59:08.040123Z",
     "start_time": "2026-02-14T07:59:07.978329Z"
    }
   },
   "source": [
    "data_checks = [\n",
    "    (\"Strandset lang_rust\", \"data/rust/strandset/lang_rust/train\"),\n",
    "    (\"Strandset core_agent\", \"data/rust/strandset/core_agent/train\"),\n",
    "    (\"Strandset IPO\", \"data/rust/strandset/ipo/train\"),\n",
    "    (\"Strandset eval\", \"data/rust/strandset/eval/test\"),\n",
    "    (\"Stats\", \"data/rust/strandset/stats.json\"),\n",
    "]\n",
    "\n",
    "print(\"Data Verification:\")\n",
    "print(\"=\" * 60)\n",
    "for name, path in data_checks:\n",
    "    exists = os.path.exists(path)\n",
    "    if exists and os.path.isdir(path):\n",
    "        items = os.listdir(path)\n",
    "        print(f\"  \\u2713 {name}: {path} ({len(items)} items)\")\n",
    "    elif exists:\n",
    "        size_kb = os.path.getsize(path) / 1024\n",
    "        print(f\"  \\u2713 {name}: {path} ({size_kb:.1f} KB)\")\n",
    "    else:\n",
    "        needed = True\n",
    "        if not CONFIG[\"include_ipo\"] and \"IPO\" in name:\n",
    "            needed = False\n",
    "        if CONFIG[\"training_scope\"] == \"lang_adapter_only\" and name in (\"Strandset core_agent\", \"Strandset IPO\"):\n",
    "            needed = False\n",
    "        sym = \"\\u2717\" if needed else \"\\u2014\"\n",
    "        label = \"MISSING\" if needed else \"not needed\"\n",
    "        print(f\"  {sym} {name}: {label}\")\n",
    "\n",
    "# Show stats if available\n",
    "stats_path = \"data/rust/strandset/stats.json\"\n",
    "if os.path.exists(stats_path):\n",
    "    import json\n",
    "    with open(stats_path) as f:\n",
    "        stats = json.load(f)\n",
    "    print(f\"\\n  Total processed: {stats.get('total_processed', '?'):,}\")\n",
    "    print(f\"  Lang adapter: {stats.get('lang_rust', '?'):,}\")\n",
    "    print(f\"  Core agent: {stats.get('core_agent_debug', 0) + stats.get('core_agent_review', 0):,}\")\n",
    "    print(f\"  IPO pairs: {stats.get('ipo', '?'):,}\")\n",
    "print(\"=\" * 60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Verification:\n",
      "============================================================\n",
      "  ✓ Strandset lang_rust: data/rust/strandset/lang_rust/train (3 items)\n",
      "  ✓ Strandset core_agent: data/rust/strandset/core_agent/train (3 items)\n",
      "  ✓ Strandset IPO: data/rust/strandset/ipo/train (3 items)\n",
      "  ✓ Strandset eval: data/rust/strandset/eval/test (3 items)\n",
      "  ✓ Stats: data/rust/strandset/stats.json (0.5 KB)\n",
      "\n",
      "  Total processed: 500\n",
      "  Lang adapter: 419\n",
      "  Core agent: 48\n",
      "  IPO pairs: 3\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "f8a9b0c1d2e3f4a5",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Lang Adapter Training\n",
    "\n",
    "Train a QLoRA adapter (rank 64) to specialise GPT-OSS 20B on Rust syntax, stdlib, and idioms.\n",
    "Uses Strandset's code_generation, code_completion, docstring, comment, and naming examples.\n",
    "Then merge the adapter into the base weights for downstream training.\n",
    "\n",
    "**Split LoRA** backend auto-enabled for 7-12x faster MoE training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0c1d2e3f4a5b6",
   "metadata": {},
   "source": [
    "### 2.1 Train lang_rust Adapter"
   ]
  },
  {
   "cell_type": "code",
   "id": "b0c1d2e3f4a5b6c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T08:16:31.231909Z",
     "start_time": "2026-02-14T07:59:08.044625Z"
    }
   },
   "source": [
    "batch = CONFIG[\"lang_rust_batch\"]\n",
    "grad_accum = CONFIG[\"lang_rust_grad_accum\"]\n",
    "max_steps = CONFIG[\"lang_rust_max_steps\"]\n",
    "seq_len = CONFIG[\"lang_rust_seq_len\"]\n",
    "\n",
    "cmd = f\"python scripts/13_train_lang_adapter.py\"\n",
    "cmd += f\" --train_data_path data/rust/strandset/lang_rust/train\"\n",
    "cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "print(f\"Training lang_rust adapter...\")\n",
    "print(f\"  Data: data/rust/strandset/lang_rust/train\")\n",
    "print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "print(f\"  Max steps: {max_steps}\")\n",
    "print(f\"  Seq length: {seq_len} (from config)\")\n",
    "print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "!{cmd}\n",
    "\n",
    "drive_helper.backup(\"checkpoints/lang_rust\", \"checkpoints/lang_rust\")\n",
    "if DRIVE_MODE != \"local\":\n",
    "    print(\"\\nCheckpoint backed up to Drive.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lang_rust adapter...\n",
      "  Data: data/rust/strandset/lang_rust/train\n",
      "  Batch: 2 x 4 = 8\n",
      "  Max steps: 100\n",
      "  Seq length: 8192 (from config)\n",
      "  Split LoRA backend: grouped_mm\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Training Language Adapter: gpt-oss-20b-lang-rust-v1\n",
      "============================================================\n",
      "\n",
      "Loading model: openai/gpt-oss-20b\n",
      "/content/llm-training-pipeline/scripts/pipeline_lib/unsloth_utils.py:38: UserWarning: WARNING: Unsloth should be imported before [transformers] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n",
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "2026-02-14 07:59:13.293267: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-14 07:59:13.307227: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1771055953.323841    2512 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1771055953.328957    2512 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1771055953.342064    2512 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771055953.342087    2512 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771055953.342091    2512 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771055953.342093    2512 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-02-14 07:59:13.345426: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "⚙️  Running in WANDB offline mode\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2026.2.1: Fast Gpt_Oss patching. Transformers: 4.57.6. vLLM: 0.15.1.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.179 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "model.safetensors.index.json: 1.19MB [00:00, 234MB/s]\n",
      "Fetching 4 files:   0% 0/4 [00:00<?, ?it/s]\n",
      "model-00001-of-00004.safetensors:   0% 0.00/4.00G [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   0% 0.00/3.37G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:   0% 0.00/4.00G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:   0% 0.00/1.16G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:   0% 76.7k/1.16G [00:00<2:54:58, 110kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   0% 903k/4.00G [00:00<1:05:54, 1.01MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:   3% 35.0M/1.16G [00:00<00:21, 53.3MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   2% 66.8M/4.00G [00:00<00:42, 91.7MB/s] \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  19% 219M/1.16G [00:00<00:02, 356MB/s]  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   6% 235M/4.00G [00:01<00:10, 353MB/s]  \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  36% 421M/1.16G [00:01<00:01, 668MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:   0% 1.79M/4.00G [00:01<43:14, 1.54MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  10% 412M/4.00G [00:01<00:05, 619MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  54% 622M/1.16G [00:01<00:00, 917MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:   2% 75.5M/4.00G [00:01<00:48, 81.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  15% 582M/4.00G [00:01<00:04, 848MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  71% 823M/1.16G [00:01<00:00, 1.14GB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:   5% 193M/4.00G [00:01<00:16, 224MB/s]  \u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  19% 742M/4.00G [00:01<00:03, 1.02GB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  88% 1.02G/1.16G [00:01<00:00, 1.30GB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:   9% 375M/4.00G [00:01<00:07, 474MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00004-of-00004.safetensors: 100% 1.16G/1.16G [00:01<00:00, 757MB/s] [A\n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:   0% 75.3k/3.37G [00:01<19:51:11, 47.2kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  13% 528M/4.00G [00:01<00:05, 656MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  26% 1.06G/4.00G [00:01<00:02, 1.19GB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   2% 53.7M/3.37G [00:01<01:16, 43.2MB/s]   \u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  30% 1.22G/4.00G [00:01<00:02, 1.28GB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  16% 651M/4.00G [00:01<00:04, 711MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   4% 127M/3.37G [00:01<00:28, 113MB/s]  \u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  34% 1.38G/4.00G [00:01<00:01, 1.36GB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  19% 765M/4.00G [00:01<00:04, 751MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  39% 1.54G/4.00G [00:01<00:01, 1.43GB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   5% 184M/3.37G [00:01<00:18, 169MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   7% 248M/3.37G [00:02<00:13, 233MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  22% 870M/4.00G [00:02<00:04, 681MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  42% 1.70G/4.00G [00:02<00:02, 1.11GB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   9% 298M/3.37G [00:02<00:11, 270MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  24% 958M/4.00G [00:02<00:05, 600MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  11% 385M/3.37G [00:02<00:08, 361MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  26% 1.05G/4.00G [00:02<00:04, 613MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  13% 445M/3.37G [00:02<00:08, 363MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  47% 1.86G/4.00G [00:02<00:03, 709MB/s] \u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  15% 493M/3.37G [00:02<00:08, 353MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  28% 1.13G/4.00G [00:02<00:05, 485MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  51% 2.04G/4.00G [00:02<00:02, 870MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  18% 602M/3.37G [00:02<00:05, 507MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  30% 1.20G/4.00G [00:02<00:05, 518MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  33% 1.31G/4.00G [00:02<00:04, 640MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  20% 669M/3.37G [00:03<00:08, 305MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  35% 1.39G/4.00G [00:03<00:06, 410MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  21% 721M/3.37G [00:03<00:12, 210MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  36% 1.45G/4.00G [00:03<00:09, 267MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  54% 2.16G/4.00G [00:03<00:05, 329MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  23% 759M/3.37G [00:03<00:12, 201MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  38% 1.51G/4.00G [00:04<00:11, 224MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  24% 797M/3.37G [00:04<00:16, 159MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  39% 1.55G/4.00G [00:04<00:11, 221MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  56% 2.25G/4.00G [00:05<00:10, 168MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  25% 828M/3.37G [00:05<00:30, 83.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  40% 1.58G/4.00G [00:05<00:24, 99.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  25% 849M/3.37G [00:05<00:27, 90.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  26% 873M/3.37G [00:05<00:24, 104MB/s] \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  40% 1.61G/4.00G [00:05<00:22, 108MB/s] \u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  58% 2.32G/4.00G [00:05<00:09, 170MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  27% 899M/3.37G [00:05<00:24, 101MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  61% 2.44G/4.00G [00:05<00:07, 223MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  41% 1.63G/4.00G [00:05<00:22, 105MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  29% 966M/3.37G [00:06<00:15, 158MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  42% 1.67G/4.00G [00:06<00:18, 123MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  43% 1.71G/4.00G [00:06<00:15, 152MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  29% 993M/3.37G [00:06<00:16, 143MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  45% 1.79G/4.00G [00:06<00:09, 237MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  66% 2.63G/4.00G [00:06<00:05, 248MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  67% 2.69G/4.00G [00:09<00:13, 93.7MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  30% 1.01G/3.37G [00:09<01:18, 30.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  46% 1.82G/4.00G [00:09<00:49, 43.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  71% 2.85G/4.00G [00:09<00:07, 144MB/s] \u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  33% 1.11G/3.37G [00:09<00:35, 63.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  49% 1.95G/4.00G [00:09<00:22, 92.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  75% 2.98G/4.00G [00:09<00:05, 198MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  35% 1.19G/3.37G [00:09<00:21, 102MB/s] \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  52% 2.07G/4.00G [00:09<00:13, 148MB/s] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  38% 1.27G/3.37G [00:09<00:14, 147MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  55% 2.19G/4.00G [00:09<00:08, 210MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  79% 3.14G/4.00G [00:09<00:03, 244MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  39% 1.33G/3.37G [00:10<00:14, 146MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  81% 3.22G/4.00G [00:10<00:03, 248MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  41% 1.38G/3.37G [00:10<00:14, 141MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  56% 2.26G/4.00G [00:10<00:11, 157MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  42% 1.42G/3.37G [00:10<00:13, 145MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  44% 1.49G/3.37G [00:10<00:10, 173MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  83% 3.33G/4.00G [00:11<00:03, 196MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  45% 1.51G/3.37G [00:11<00:10, 182MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  58% 2.31G/4.00G [00:11<00:13, 129MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  84% 3.38G/4.00G [00:11<00:03, 195MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  46% 1.54G/3.37G [00:12<00:25, 71.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  59% 2.35G/4.00G [00:13<00:26, 61.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  46% 1.56G/3.37G [00:13<00:35, 50.6MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  85% 3.42G/4.00G [00:13<00:07, 80.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  89% 3.56G/4.00G [00:13<00:03, 137MB/s] \u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  62% 2.49G/4.00G [00:13<00:13, 112MB/s] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  49% 1.66G/3.37G [00:13<00:16, 102MB/s] \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  65% 2.61G/4.00G [00:13<00:08, 168MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  52% 1.76G/3.37G [00:13<00:09, 169MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  92% 3.68G/4.00G [00:13<00:01, 181MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  94% 3.75G/4.00G [00:13<00:01, 204MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  54% 1.82G/3.37G [00:14<00:11, 135MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  67% 2.69G/4.00G [00:14<00:08, 152MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  55% 1.86G/3.37G [00:14<00:10, 139MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  69% 2.75G/4.00G [00:14<00:08, 150MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  56% 1.90G/3.37G [00:14<00:09, 153MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  97% 3.87G/4.00G [00:14<00:00, 178MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  70% 2.79G/4.00G [00:14<00:07, 155MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  57% 1.93G/3.37G [00:14<00:09, 147MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  58% 1.96G/3.37G [00:15<00:09, 157MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  59% 1.99G/3.37G [00:15<00:08, 165MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  60% 2.02G/3.37G [00:15<00:07, 176MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  61% 2.04G/3.37G [00:17<00:37, 36.0MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  99% 3.95G/4.00G [00:17<00:00, 71.9MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors: 100% 4.00G/4.00G [00:17<00:00, 223MB/s] \u001b[A\u001b[A\u001b[A\n",
      "Fetching 4 files:  25% 1/4 [00:18<00:54, 18.08s/it]\n",
      "\n",
      "model-00003-of-00004.safetensors:  66% 2.22G/3.37G [00:17<00:09, 116MB/s] \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  77% 3.06G/4.00G [00:17<00:07, 123MB/s] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  70% 2.35G/3.37G [00:18<00:05, 189MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  80% 3.21G/4.00G [00:18<00:04, 189MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  83% 3.33G/4.00G [00:18<00:03, 212MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  74% 2.49G/3.37G [00:18<00:03, 235MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  85% 3.41G/4.00G [00:18<00:02, 247MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  76% 2.57G/3.37G [00:18<00:02, 269MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  87% 3.48G/4.00G [00:18<00:02, 250MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  78% 2.63G/3.37G [00:19<00:03, 234MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  88% 3.54G/4.00G [00:19<00:01, 239MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  80% 2.69G/3.37G [00:19<00:03, 206MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  90% 3.59G/4.00G [00:19<00:01, 230MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  81% 2.73G/3.37G [00:19<00:03, 204MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  91% 3.63G/4.00G [00:19<00:01, 220MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  82% 2.76G/3.37G [00:19<00:02, 208MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  92% 3.67G/4.00G [00:19<00:01, 202MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  83% 2.80G/3.37G [00:22<00:10, 53.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  92% 3.70G/4.00G [00:22<00:05, 53.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  88% 2.96G/3.37G [00:22<00:03, 125MB/s] \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  94% 3.75G/4.00G [00:22<00:03, 77.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  90% 3.03G/3.37G [00:22<00:02, 156MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors:  98% 3.93G/4.00G [00:22<00:00, 175MB/s] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors: 100% 3.37G/3.37G [00:22<00:00, 150MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00002-of-00004.safetensors: 100% 4.00G/4.00G [00:22<00:00, 176MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Fetching 4 files: 100% 4/4 [00:22<00:00,  5.71s/it]\n",
      "Loading checkpoint shards: 100% 4/4 [00:03<00:00,  1.12it/s]\n",
      "generation_config.json: 100% 165/165 [00:00<00:00, 1.67MB/s]\n",
      "tokenizer_config.json: 22.8kB [00:00, 459kB/s]\n",
      "tokenizer.json: 100% 27.9M/27.9M [00:00<00:00, 60.0MB/s]\n",
      "special_tokens_map.json: 100% 446/446 [00:00<00:00, 3.92MB/s]\n",
      "chat_template.jinja: 15.1kB [00:00, 52.4MB/s]\n",
      "\n",
      "Applying LoRA configuration...\n",
      "Unsloth: Detected MoE model with num_experts = 32 and target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']. Enabling LoRA on MoE parameters: ['mlp.experts.gate_up_proj', 'mlp.experts.down_proj']\n",
      "Unsloth: PEFT set target_parameters but found no matching parameters.\n",
      "This is expected for MoE models - Unsloth handles MoE expert LoRA targeting separately.\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "Trainable parameters: 31,850,496 / 11,072,953,920 (0.29%)\n",
      "\n",
      "Loading training data: data/rust/strandset/lang_rust/train\n",
      "  Training examples: 419\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=30): 100% 419/419 [00:08<00:00, 50.77 examples/s]\n",
      "\n",
      "Starting training...\n",
      "  Output: checkpoints/lang_rust\n",
      "  Epochs: 1\n",
      "  Max steps: 100\n",
      "  LR: 2e-05\n",
      "  Effective batch: 8\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 419 | Num Epochs = 2 | Total steps = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 31,850,496 of 20,946,607,680 (0.15% trained)\n",
      "  0% 0/100 [00:00<?, ?it/s]Unsloth: Will smartly offload gradients to save VRAM!\n",
      "{'loss': 2.4716, 'grad_norm': 5.419440269470215, 'learning_rate': 1.9912640693269754e-05, 'epoch': 0.19}\n",
      "{'loss': 1.4908, 'grad_norm': 2.7143044471740723, 'learning_rate': 1.8947293298207637e-05, 'epoch': 0.38}\n",
      "{'loss': 1.2029, 'grad_norm': 3.37758207321167, 'learning_rate': 1.7012367842724887e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0308, 'grad_norm': 3.5340704917907715, 'learning_rate': 1.4317543523384928e-05, 'epoch': 0.76}\n",
      "{'loss': 0.9142, 'grad_norm': 2.1238529682159424, 'learning_rate': 1.1154846369695864e-05, 'epoch': 0.95}\n",
      "{'loss': 0.8595, 'grad_norm': 1.790487289428711, 'learning_rate': 7.867003692562533e-06, 'epoch': 1.13}\n",
      "{'loss': 0.8263, 'grad_norm': 3.0231640338897705, 'learning_rate': 4.8103042621878515e-06, 'epoch': 1.32}\n",
      "{'loss': 0.7808, 'grad_norm': 2.443430185317993, 'learning_rate': 2.315988891431412e-06, 'epoch': 1.51}\n",
      "{'loss': 0.7721, 'grad_norm': 3.1463735103607178, 'learning_rate': 6.543553540053926e-07, 'epoch': 1.7}\n",
      "{'loss': 0.7692, 'grad_norm': 2.1450600624084473, 'learning_rate': 5.467426590739511e-09, 'epoch': 1.9}\n",
      "{'train_runtime': 958.1572, 'train_samples_per_second': 0.835, 'train_steps_per_second': 0.104, 'train_loss': 1.1118215751647949, 'epoch': 1.9}\n",
      "100% 100/100 [15:58<00:00,  9.58s/it]\n",
      "Saved lora to checkpoints/lang_rust/final\n",
      "\n",
      "Training complete!\n",
      "  Adapter saved to: checkpoints/lang_rust/final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"storageQuotaExceeded\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint backed up to Drive.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4a5b6c7d8",
   "metadata": {},
   "source": [
    "### 2.2 Merge lang_rust into Base"
   ]
  },
  {
   "cell_type": "code",
   "id": "d2e3f4a5b6c7d8e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T08:17:07.125914Z",
     "start_time": "2026-02-14T08:16:31.282288Z"
    }
   },
   "source": [
    "print(\"Merging lang_rust adapter into base model...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "!python scripts/19_merge_adapter.py \\\n",
    "    --adapter_path checkpoints/lang_rust/final \\\n",
    "    --output_dir checkpoints/gpt-oss-20b-rust-merged \\\n",
    "    --export_formats hf\n",
    "\n",
    "drive_helper.backup(\"checkpoints/gpt-oss-20b-rust-merged\", \"checkpoints/gpt-oss-20b-rust-merged\")\n",
    "if DRIVE_MODE != \"local\":\n",
    "    print(\"\\nMerged model backed up to Drive.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging lang_rust adapter into base model...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Merging Adapter: merge-lang-rust\n",
      "============================================================\n",
      "  Base model: openai/gpt-oss-20b\n",
      "  Adapter: checkpoints/lang_rust/final\n",
      "  Output: checkpoints/gpt-oss-20b-rust-merged\n",
      "  Formats: ['hf']\n",
      "/content/llm-training-pipeline/scripts/pipeline_lib/unsloth_utils.py:138: UserWarning: WARNING: Unsloth should be imported before [transformers] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n",
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "2026-02-14 08:16:35.068071: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-14 08:16:35.081088: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1771056995.096548    9068 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1771056995.101605    9068 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1771056995.114147    9068 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771056995.114173    9068 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771056995.114176    9068 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771056995.114178    9068 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-02-14 08:16:35.117729: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2026.2.1: Fast Gpt_Oss patching. Transformers: 4.57.6. vLLM: 0.15.1.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.179 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Loading checkpoint shards: 100% 4/4 [00:03<00:00,  1.07it/s]\n",
      "Unsloth: PEFT set target_parameters but found no matching parameters.\n",
      "This is expected for MoE models - Unsloth handles MoE expert LoRA targeting separately.\n",
      "/usr/local/lib/python3.12/dist-packages/unsloth_zoo/saving_utils.py:1678: UserWarning: Model is not a PeftModel (no Lora adapters detected). Skipping Merge. Please use save_pretrained() or push_to_hub() instead!\n",
      "  warnings.warn(\"Model is not a PeftModel (no Lora adapters detected). Skipping Merge. Please use save_pretrained() or push_to_hub() instead!\")\n",
      "Exported HuggingFace format to checkpoints/gpt-oss-20b-rust-merged/hf\n",
      "\n",
      "Running smoke test...\n",
      "  Smoke test FAILED: Unsloth: No config file found - are you sure the `model_name` is correct?\n",
      "If you're using a model on your local device, confirm if the folder location exists.\n",
      "If you're using a HuggingFace online model, check if it exists.\n",
      "\n",
      "============================================================\n",
      "Merge complete!\n",
      "  Output: checkpoints/gpt-oss-20b-rust-merged\n",
      "============================================================\n",
      "\n",
      "Merged model backed up to Drive.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "e3f4a5b6c7d8e9f0",
   "metadata": {},
   "source": [
    "### 2.3 Verify Merge"
   ]
  },
  {
   "cell_type": "code",
   "id": "f4a5b6c7d8e9f0a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T08:17:07.237200Z",
     "start_time": "2026-02-14T08:17:07.178932Z"
    }
   },
   "source": [
    "merged_path = \"checkpoints/gpt-oss-20b-rust-merged\"\n",
    "adapter_path = \"checkpoints/lang_rust/final\"\n",
    "\n",
    "print(\"Merge Verification:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if os.path.exists(merged_path):\n",
    "    files = os.listdir(merged_path)\n",
    "    safetensors = [f for f in files if f.endswith(\".safetensors\")]\n",
    "    print(f\"  \\u2713 Merged model: {merged_path}\")\n",
    "    print(f\"    {len(safetensors)} safetensors shard(s), {len(files)} total files\")\n",
    "else:\n",
    "    print(f\"  \\u2717 Merged model not found at {merged_path}\")\n",
    "\n",
    "if os.path.exists(adapter_path):\n",
    "    adapter_files = os.listdir(adapter_path)\n",
    "    print(f\"  \\u2713 Adapter: {adapter_path} ({len(adapter_files)} files)\")\n",
    "else:\n",
    "    print(f\"  \\u2717 Adapter not found at {adapter_path}\")\n",
    "\n",
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"\\n\\u2713 lang_adapter_only scope complete. Stopping here.\")\n",
    "\n",
    "print(\"=\" * 60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge Verification:\n",
      "============================================================\n",
      "  ✓ Merged model: checkpoints/gpt-oss-20b-rust-merged\n",
      "    0 safetensors shard(s), 0 total files\n",
      "  ✓ Adapter: checkpoints/lang_rust/final (7 files)\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "uny2mawqiof",
   "source": "### 2.4 Wait for Mutations + Generate Trajectories (Optional)\n\nIf `include_mutations=True`, waits for background mutation generation to finish,\nthen generates agent trajectories to enrich core_agent training data.\n\nSkipped if mutations are disabled.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ewaf78f24s7",
   "source": "if not CONFIG[\"include_mutations\"]:\n    print(\"Mutation generation disabled — skipping.\")\nelif CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n    print(f\"Skipping — scope is {CONFIG['training_scope']}\")\nelse:\n    # --- Wait for background mutation generation ---\n    if mutation_proc is not None:\n        if mutation_proc.poll() is None:\n            print(\"Waiting for background mutation generation to finish...\")\n            print(\"(Monitor: !tail -f logs/mutation_gen.log)\")\n            mutation_proc.wait()\n\n        rc = mutation_proc.returncode\n        if rc == 0:\n            print(f\"Mutation generation completed successfully.\")\n        else:\n            print(f\"WARNING: Mutation generation failed (rc={rc})\")\n            print(\"Check logs/mutation_gen.log for details.\")\n            print(\"Continuing — trajectories will use whatever data is available.\")\n    else:\n        print(\"No background mutation process found.\")\n\n    # Backup mutation results to Drive\n    drive_helper.backup(\"data/rust/mutations\", \"data/rust/mutations\")\n    if DRIVE_MODE != \"local\":\n        print(\"Backed up mutations to Drive.\")\n\n    # --- Generate trajectories to enrich core_agent data ---\n    max_samples = 500 if CONFIG[\"training_scope\"] == \"quick_test\" else 5000\n\n    print(f\"\\nGenerating trajectories (max {max_samples} per source)...\")\n    print(\"=\" * 60)\n\n    cmd = f\"python scripts/15_generate_trajectories.py --max_samples {max_samples}\"\n\n    mutations_path = \"data/rust/mutations/mutations.jsonl\"\n    if os.path.exists(mutations_path):\n        cmd += f\" --mutations_path {mutations_path}\"\n\n    !{cmd}\n\n    # Merge trajectory data into Strandset core_agent dataset\n    from datasets import load_from_disk, concatenate_datasets\n\n    traj_path = \"data/rust/core_agent/train\"\n    strandset_core_path = \"data/rust/strandset/core_agent/train\"\n\n    if os.path.exists(traj_path) and os.path.exists(strandset_core_path):\n        traj_ds = load_from_disk(traj_path)\n        strandset_ds = load_from_disk(strandset_core_path)\n        print(f\"\\n  Enriching core_agent: {len(strandset_ds):,} Strandset + {len(traj_ds):,} trajectories\")\n        merged = concatenate_datasets([strandset_ds, traj_ds])\n        merged.save_to_disk(traj_path)\n        print(f\"  Saved enriched dataset: {len(merged):,} total -> {traj_path}\")\n\n    drive_helper.backup(\"data/rust/core_agent\", \"data/rust/core_agent\")\n    if DRIVE_MODE != \"local\":\n        print(\"\\nBacked up trajectories to Drive.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a5b6c7d8e9f0a1b2",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Core Agent SFT\n",
    "\n",
    "Train a higher-rank LoRA adapter (rank 128) on Strandset's debug/review examples.\n",
    "Uses the merged lang_rust model as the base.\n",
    "\n",
    "**Split LoRA** + **Auto packing** (3x faster, zero-config)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c7d8e9f0a1b2c3",
   "metadata": {},
   "source": [
    "### 3.1 Train core_agent Adapter"
   ]
  },
  {
   "cell_type": "code",
   "id": "c7d8e9f0a1b2c3d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T08:32:22.911784Z",
     "start_time": "2026-02-14T08:17:07.238065Z"
    }
   },
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    batch = CONFIG[\"core_agent_batch\"]\n",
    "    grad_accum = CONFIG[\"core_agent_grad_accum\"]\n",
    "    max_steps = CONFIG[\"core_agent_max_steps\"]\n",
    "    seq_len = CONFIG[\"core_agent_seq_len\"]\n",
    "\n",
    "    cmd = f\"python scripts/14_train_core_agent.py\"\n",
    "    cmd += f\" --train_data_path data/rust/strandset/core_agent/train\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    print(f\"Training core_agent adapter...\")\n",
    "    print(f\"  Data: data/rust/strandset/core_agent/train\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Seq length: {seq_len} (from config)\")\n",
    "    print(f\"  LoRA rank: 128\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(f\"  Auto packing: enabled (uncontaminated)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/core_agent\", \"checkpoints/core_agent\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training core_agent adapter...\n",
      "  Data: data/rust/strandset/core_agent/train\n",
      "  Batch: 1 x 4 = 4\n",
      "  Max steps: 100\n",
      "  Seq length: 16384 (from config)\n",
      "  LoRA rank: 128\n",
      "  Split LoRA backend: grouped_mm\n",
      "  Auto packing: enabled (uncontaminated)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Training Core Agent: gpt-oss-20b-core-agent-v1\n",
      "============================================================\n",
      "\n",
      "Merging lang_rust adapter from checkpoints/lang_rust...\n",
      "  (Run scripts/19_merge_adapter.py first, or use --no-merge-lang-adapter)\n",
      "  Falling back to base model: openai/gpt-oss-20b\n",
      "\n",
      "Loading model: openai/gpt-oss-20b\n",
      "/content/llm-training-pipeline/scripts/pipeline_lib/unsloth_utils.py:38: UserWarning: WARNING: Unsloth should be imported before [transformers] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n",
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "2026-02-14 08:17:10.964966: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-14 08:17:10.977326: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1771057030.992719    9505 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1771057030.997831    9505 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1771057031.009881    9505 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771057031.009906    9505 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771057031.009909    9505 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771057031.009912    9505 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-02-14 08:17:11.013261: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "⚙️  Running in WANDB offline mode\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2026.2.1: Fast Gpt_Oss patching. Transformers: 4.57.6. vLLM: 0.15.1.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.179 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Loading checkpoint shards: 100% 4/4 [00:03<00:00,  1.15it/s]\n",
      "\n",
      "Applying LoRA (rank=128)...\n",
      "Unsloth: Detected MoE model with num_experts = 32 and target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']. Enabling LoRA on MoE parameters: ['mlp.experts.gate_up_proj', 'mlp.experts.down_proj']\n",
      "Unsloth: PEFT set target_parameters but found no matching parameters.\n",
      "This is expected for MoE models - Unsloth handles MoE expert LoRA targeting separately.\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "Trainable parameters: 63,700,992 / 11,104,804,416 (0.57%)\n",
      "\n",
      "Loading training data: data/rust/strandset/core_agent/train\n",
      "  Training examples: 48\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=30): 100% 48/48 [00:08<00:00,  5.90 examples/s]\n",
      "\n",
      "Starting training...\n",
      "  Output: checkpoints/core_agent\n",
      "  Epochs: 2\n",
      "  LR: 3e-05\n",
      "  Max seq length: 16384\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 48 | Num Epochs = 9 | Total steps = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 63,700,992 of 20,978,458,176 (0.30% trained)\n",
      "  1% 1/100 [00:50<1:22:40, 50.10s/it]Unsloth: Will smartly offload gradients to save VRAM!\n",
      "{'loss': 2.5653, 'grad_norm': 14.15890884399414, 'learning_rate': 2.4e-05, 'epoch': 0.42}\n",
      "{'loss': 1.6604, 'grad_norm': 6.342757225036621, 'learning_rate': 2.9868961039904628e-05, 'epoch': 0.83}\n",
      "{'loss': 1.4094, 'grad_norm': 4.834097862243652, 'learning_rate': 2.9340536723015367e-05, 'epoch': 1.25}\n",
      "{'loss': 1.0121, 'grad_norm': 4.858771800994873, 'learning_rate': 2.8420939947311454e-05, 'epoch': 1.67}\n",
      "{'loss': 1.0485, 'grad_norm': 4.522945404052734, 'learning_rate': 2.7135254915624213e-05, 'epoch': 2.08}\n",
      "{'loss': 0.811, 'grad_norm': 2.930119752883911, 'learning_rate': 2.5518551764087326e-05, 'epoch': 2.5}\n",
      "{'loss': 0.7733, 'grad_norm': 3.283590316772461, 'learning_rate': 2.3614929940244155e-05, 'epoch': 2.92}\n",
      "{'loss': 0.6535, 'grad_norm': 3.0885634422302246, 'learning_rate': 2.1476315285077393e-05, 'epoch': 3.33}\n",
      "{'loss': 0.6858, 'grad_norm': 5.5626678466796875, 'learning_rate': 1.916104363142767e-05, 'epoch': 3.75}\n",
      "{'loss': 0.6276, 'grad_norm': 3.560779333114624, 'learning_rate': 1.6732269554543794e-05, 'epoch': 4.17}\n",
      "{'loss': 0.5946, 'grad_norm': 3.6728506088256836, 'learning_rate': 1.4256243679901665e-05, 'epoch': 4.58}\n",
      "{'loss': 0.5651, 'grad_norm': 6.244950771331787, 'learning_rate': 1.18005055388438e-05, 'epoch': 5.0}\n",
      "{'loss': 0.4629, 'grad_norm': 3.6533596515655518, 'learning_rate': 9.432041266226686e-06, 'epoch': 5.42}\n",
      "{'loss': 0.5365, 'grad_norm': 6.166356563568115, 'learning_rate': 7.215456393281777e-06, 'epoch': 5.83}\n",
      "{'loss': 0.4513, 'grad_norm': 4.327205657958984, 'learning_rate': 5.21121357713747e-06, 'epoch': 6.25}\n",
      "{'loss': 0.4077, 'grad_norm': 6.393224716186523, 'learning_rate': 3.473983337147118e-06, 'epoch': 6.67}\n",
      "{'loss': 0.4487, 'grad_norm': 4.601622104644775, 'learning_rate': 2.0511527856363916e-06, 'epoch': 7.08}\n",
      "{'loss': 0.4399, 'grad_norm': 3.8935844898223877, 'learning_rate': 9.815330310080889e-07, 'epoch': 7.5}\n",
      "{'loss': 0.3881, 'grad_norm': 4.2835917472839355, 'learning_rate': 2.943005118778597e-07, 'epoch': 7.92}\n",
      "{'loss': 0.3773, 'grad_norm': 3.815138816833496, 'learning_rate': 8.201139886109265e-09, 'epoch': 8.33}\n",
      "{'train_runtime': 861.3955, 'train_samples_per_second': 0.464, 'train_steps_per_second': 0.116, 'train_loss': 0.7959598779678345, 'epoch': 8.33}\n",
      "100% 100/100 [14:21<00:00,  8.61s/it]\n",
      "Saved lora to checkpoints/core_agent/final\n",
      "\n",
      "Training complete!\n",
      "  Core agent adapter saved to: checkpoints/core_agent/final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"storageQuotaExceeded\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint backed up to Drive.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "d8e9f0a1b2c3d4e5",
   "metadata": {},
   "source": [
    "### 3.2 Verify core_agent"
   ]
  },
  {
   "cell_type": "code",
   "id": "e9f0a1b2c3d4e5f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T08:32:23.021562Z",
     "start_time": "2026-02-14T08:32:22.963732Z"
    }
   },
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    ckpt_path = \"checkpoints/core_agent/final\"\n",
    "\n",
    "    print(\"Core Agent Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(ckpt_path):\n",
    "        files = os.listdir(ckpt_path)\n",
    "        print(f\"  \\u2713 Checkpoint: {ckpt_path} ({len(files)} files)\")\n",
    "\n",
    "        adapter_config = os.path.join(ckpt_path, \"adapter_config.json\")\n",
    "        if os.path.exists(adapter_config):\n",
    "            import json\n",
    "            with open(adapter_config) as f:\n",
    "                cfg = json.load(f)\n",
    "            print(f\"    LoRA rank: {cfg.get('r', '?')}\")\n",
    "            print(f\"    Alpha: {cfg.get('lora_alpha', '?')}\")\n",
    "            print(f\"    Target modules: {cfg.get('target_modules', '?')}\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 Checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core Agent Verification:\n",
      "============================================================\n",
      "  ✓ Checkpoint: checkpoints/core_agent/final (7 files)\n",
      "    LoRA rank: 128\n",
      "    Alpha: 256\n",
      "    Target modules: ['v_proj', 'q_proj', 'up_proj', 'gate_proj', 'o_proj', 'k_proj', 'down_proj']\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "f0a1b2c3d4e5f6a7",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: IPO Preference Training (Optional)\n",
    "\n",
    "Train with Identity Preference Optimisation on synthetic preference pairs\n",
    "from Strandset's bug_detection category (fixed=chosen, buggy=rejected).\n",
    "\n",
    "Very low learning rate (5e-7), 1 epoch only to avoid collapse.\n",
    "\n",
    "Set `include_ipo=False` in Step 0.3 to skip."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4e5f6a7b9",
   "metadata": {},
   "source": [
    "### 4.1 Train with IPO"
   ]
  },
  {
   "cell_type": "code",
   "id": "b2c3d4e5f6a7b9c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T08:49:14.926510Z",
     "start_time": "2026-02-14T08:32:23.022857Z"
    }
   },
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "elif not CONFIG[\"include_ipo\"]:\n",
    "    print(\"Skipping \\u2014 IPO disabled (include_ipo=False)\")\n",
    "else:\n",
    "    batch = CONFIG[\"ipo_batch\"]\n",
    "    grad_accum = CONFIG[\"ipo_grad_accum\"]\n",
    "    max_steps = CONFIG[\"ipo_max_steps\"]\n",
    "\n",
    "    ipo_checkpoint = \"checkpoints/core_agent/final\"\n",
    "\n",
    "    cmd = f\"python scripts/17_ipo_preference.py\"\n",
    "    cmd += f\" --checkpoint {ipo_checkpoint}\"\n",
    "    cmd += f\" --train_data_path data/rust/strandset/ipo/train\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    print(f\"Training with IPO (synthetic preferences)...\")\n",
    "    print(f\"  Checkpoint: {ipo_checkpoint}\")\n",
    "    print(f\"  Data: data/rust/strandset/ipo/train\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Loss: IPO (beta=0.1)\")\n",
    "    print(f\"  Load mode: {CONFIG['load_mode']}\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/core_agent_ipo\", \"checkpoints/core_agent_ipo\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with IPO (synthetic preferences)...\n",
      "  Checkpoint: checkpoints/core_agent/final\n",
      "  Data: data/rust/strandset/ipo/train\n",
      "  Batch: 1 x 16 = 16\n",
      "  Max steps: 100\n",
      "  Loss: IPO (beta=0.1)\n",
      "  Load mode: fp8\n",
      "  Split LoRA backend: grouped_mm\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "IPO Preference Training: gpt-oss-20b-ipo-v1\n",
      "============================================================\n",
      "\n",
      "Loading model from: checkpoints/core_agent/final\n",
      "/content/llm-training-pipeline/scripts/pipeline_lib/unsloth_utils.py:38: UserWarning: WARNING: Unsloth should be imported before [transformers] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n",
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "2026-02-14 08:32:26.890015: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-14 08:32:26.903866: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1771057946.920867   14345 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1771057946.926132   14345 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1771057946.940053   14345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771057946.940077   14345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771057946.940080   14345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771057946.940082   14345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-02-14 08:32:26.943447: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "⚙️  Running in WANDB offline mode\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2026.2.1: Fast Gpt_Oss patching. Transformers: 4.57.6. vLLM: 0.15.1.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.179 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Loading checkpoint shards: 100% 4/4 [00:03<00:00,  1.18it/s]\n",
      "Unsloth: PEFT set target_parameters but found no matching parameters.\n",
      "This is expected for MoE models - Unsloth handles MoE expert LoRA targeting separately.\n",
      "\n",
      "Loading preference data: data/rust/strandset/ipo/train\n",
      "  Training pairs: 3\n",
      "num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
      "[datasets.arrow_dataset|WARNING]num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
      "Extracting prompt in train dataset (num_proc=3): 100% 3/3 [00:00<00:00, 12.81 examples/s]\n",
      "num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
      "[datasets.arrow_dataset|WARNING]num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
      "Applying chat template to train dataset (num_proc=3): 100% 3/3 [00:01<00:00,  1.80 examples/s]\n",
      "num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
      "[datasets.arrow_dataset|WARNING]num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
      "Tokenizing train dataset (num_proc=3): 100% 3/3 [00:01<00:00,  1.88 examples/s]\n",
      "\n",
      "Starting IPO training...\n",
      "  Output: checkpoints/core_agent_ipo\n",
      "  Loss type: ipo\n",
      "  Beta: 0.1\n",
      "  LR: 5e-07\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 3 | Num Epochs = 100 | Total steps = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 16\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 16 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 63,700,992 of 20,978,458,176 (0.30% trained)\n",
      "  0% 0/100 [00:00<?, ?it/s]Unsloth: Will smartly offload gradients to save VRAM!\n",
      "{'loss': 24.5944, 'grad_norm': 60.94816970825195, 'learning_rate': 2e-07, 'rewards/chosen': 0.10915359854698181, 'rewards/rejected': 0.1050591766834259, 'rewards/accuracies': 0.6666666865348816, 'rewards/margins': 0.004094405099749565, 'logps/chosen': -0.8164748549461365, 'logps/rejected': -0.864345133304596, 'logits/chosen': -3.713439702987671, 'logits/rejected': -3.6819660663604736, 'epoch': 5.0}\n",
      "{'loss': 24.4701, 'grad_norm': 43.2380485534668, 'learning_rate': 4.5e-07, 'rewards/chosen': 0.10849502682685852, 'rewards/rejected': 0.10313601046800613, 'rewards/accuracies': 0.6666666865348816, 'rewards/margins': 0.005359013564884663, 'logps/chosen': -0.8230603933334351, 'logps/rejected': -0.8835769295692444, 'logits/chosen': -3.7046000957489014, 'logits/rejected': -3.6745147705078125, 'epoch': 10.0}\n",
      "{'loss': 24.3253, 'grad_norm': 40.38898849487305, 'learning_rate': 4.975670171853925e-07, 'rewards/chosen': 0.10849761217832565, 'rewards/rejected': 0.10165854543447495, 'rewards/accuracies': 0.6666666865348816, 'rewards/margins': 0.006839071866124868, 'logps/chosen': -0.8230345249176025, 'logps/rejected': -0.8983514904975891, 'logits/chosen': -3.714641809463501, 'logits/rejected': -3.6841936111450195, 'epoch': 15.0}\n",
      "{'loss': 24.1489, 'grad_norm': 41.85124206542969, 'learning_rate': 4.877641290737883e-07, 'rewards/chosen': 0.10731614381074905, 'rewards/rejected': 0.09866371750831604, 'rewards/accuracies': 0.6666666865348816, 'rewards/margins': 0.008652420714497566, 'logps/chosen': -0.8348494172096252, 'logps/rejected': -0.9282997846603394, 'logits/chosen': -3.722355604171753, 'logits/rejected': -3.691905975341797, 'epoch': 20.0}\n",
      "{'loss': 23.9846, 'grad_norm': 44.817901611328125, 'learning_rate': 4.707368982147317e-07, 'rewards/chosen': 0.10688473284244537, 'rewards/rejected': 0.09653249382972717, 'rewards/accuracies': 0.9333333373069763, 'rewards/margins': 0.010352236218750477, 'logps/chosen': -0.8391634225845337, 'logps/rejected': -0.949612021446228, 'logits/chosen': -3.730393171310425, 'logits/rejected': -3.700221538543701, 'epoch': 25.0}\n",
      "{'loss': 23.8172, 'grad_norm': 48.34473419189453, 'learning_rate': 4.470026884016804e-07, 'rewards/chosen': 0.1053234189748764, 'rewards/rejected': 0.09322746843099594, 'rewards/accuracies': 1.0, 'rewards/margins': 0.012095947749912739, 'logps/chosen': -0.8547766208648682, 'logps/rejected': -0.9826623201370239, 'logits/chosen': -3.7299351692199707, 'logits/rejected': -3.6993472576141357, 'epoch': 30.0}\n",
      "{'loss': 23.6458, 'grad_norm': 50.150840759277344, 'learning_rate': 4.172826515897145e-07, 'rewards/chosen': 0.10386592894792557, 'rewards/rejected': 0.08996786922216415, 'rewards/accuracies': 1.0, 'rewards/margins': 0.013898058794438839, 'logps/chosen': -0.8693514466285706, 'logps/rejected': -1.0152583122253418, 'logits/chosen': -3.7353196144104004, 'logits/rejected': -3.705482244491577, 'epoch': 35.0}\n",
      "{'loss': 23.4785, 'grad_norm': 46.77414321899414, 'learning_rate': 3.824798160583012e-07, 'rewards/chosen': 0.10310360789299011, 'rewards/rejected': 0.08743443340063095, 'rewards/accuracies': 1.0, 'rewards/margins': 0.015669167041778564, 'logps/chosen': -0.8769746422767639, 'logps/rejected': -1.0405925512313843, 'logits/chosen': -3.7389185428619385, 'logits/rejected': -3.7101447582244873, 'epoch': 40.0}\n",
      "{'loss': 23.3522, 'grad_norm': 51.34490966796875, 'learning_rate': 3.43651648353978e-07, 'rewards/chosen': 0.10222498327493668, 'rewards/rejected': 0.0852162316441536, 'rewards/accuracies': 1.0, 'rewards/margins': 0.017008747905492783, 'logps/chosen': -0.8857609033584595, 'logps/rejected': -1.0627747774124146, 'logits/chosen': -3.7278428077697754, 'logits/rejected': -3.6996841430664062, 'epoch': 45.0}\n",
      "{'loss': 23.2251, 'grad_norm': 58.54438781738281, 'learning_rate': 3.0197792270443976e-07, 'rewards/chosen': 0.09963319450616837, 'rewards/rejected': 0.08126439154148102, 'rewards/accuracies': 1.0, 'rewards/margins': 0.018368814140558243, 'logps/chosen': -0.9116787314414978, 'logps/rejected': -1.1022930145263672, 'logits/chosen': -3.7284576892852783, 'logits/rejected': -3.6990063190460205, 'epoch': 50.0}\n",
      "{'loss': 23.0861, 'grad_norm': 66.74105834960938, 'learning_rate': 2.5872487417562527e-07, 'rewards/chosen': 0.09789394587278366, 'rewards/rejected': 0.07802499085664749, 'rewards/accuracies': 1.0, 'rewards/margins': 0.01986895315349102, 'logps/chosen': -0.9290712475776672, 'logps/rejected': -1.134687066078186, 'logits/chosen': -3.7172646522521973, 'logits/rejected': -3.689286708831787, 'epoch': 55.0}\n",
      "{'loss': 22.9156, 'grad_norm': 75.39918518066406, 'learning_rate': 2.152067247599837e-07, 'rewards/chosen': 0.09666978567838669, 'rewards/rejected': 0.07494919747114182, 'rewards/accuracies': 1.0, 'rewards/margins': 0.021720586344599724, 'logps/chosen': -0.941312849521637, 'logps/rejected': -1.1654448509216309, 'logits/chosen': -3.7098190784454346, 'logits/rejected': -3.682263135910034, 'epoch': 60.0}\n",
      "{'loss': 22.8053, 'grad_norm': 72.57791137695312, 'learning_rate': 1.7274575140626315e-07, 'rewards/chosen': 0.09527166932821274, 'rewards/rejected': 0.07234220206737518, 'rewards/accuracies': 1.0, 'rewards/margins': 0.022929465398192406, 'logps/chosen': -0.9552939534187317, 'logps/rejected': -1.1915148496627808, 'logits/chosen': -3.6999175548553467, 'logits/rejected': -3.672630548477173, 'epoch': 65.0}\n",
      "{'loss': 22.6455, 'grad_norm': 77.7601089477539, 'learning_rate': 1.3263210930352737e-07, 'rewards/chosen': 0.0932663232088089, 'rewards/rejected': 0.06858775019645691, 'rewards/accuracies': 1.0, 'rewards/margins': 0.024678582325577736, 'logps/chosen': -0.9753473997116089, 'logps/rejected': -1.2290594577789307, 'logits/chosen': -3.6939284801483154, 'logits/rejected': -3.6664083003997803, 'epoch': 70.0}\n",
      "{'loss': 22.5789, 'grad_norm': 78.66082763671875, 'learning_rate': 9.608463116858542e-08, 'rewards/chosen': 0.0937931165099144, 'rewards/rejected': 0.06838247925043106, 'rewards/accuracies': 1.0, 'rewards/margins': 0.025410642847418785, 'logps/chosen': -0.9700794816017151, 'logps/rejected': -1.231112003326416, 'logits/chosen': -3.6898322105407715, 'logits/rejected': -3.660905122756958, 'epoch': 75.0}\n",
      "{'loss': 22.5039, 'grad_norm': 81.67276763916016, 'learning_rate': 6.42137936306514e-08, 'rewards/chosen': 0.0919576957821846, 'rewards/rejected': 0.06571942567825317, 'rewards/accuracies': 1.0, 'rewards/margins': 0.026238270103931427, 'logps/chosen': -0.9884337186813354, 'logps/rejected': -1.2577426433563232, 'logits/chosen': -3.678903818130493, 'logits/rejected': -3.6515276432037354, 'epoch': 80.0}\n",
      "{'loss': 22.4636, 'grad_norm': 80.64114379882812, 'learning_rate': 3.798797596089351e-08, 'rewards/chosen': 0.09168289601802826, 'rewards/rejected': 0.06499084830284119, 'rewards/accuracies': 1.0, 'rewards/margins': 0.02669205330312252, 'logps/chosen': -0.9911817908287048, 'logps/rejected': -1.2650283575057983, 'logits/chosen': -3.680272102355957, 'logits/rejected': -3.6521952152252197, 'epoch': 85.0}\n",
      "{'loss': 22.4217, 'grad_norm': 80.07549285888672, 'learning_rate': 1.8204036358303172e-08, 'rewards/chosen': 0.09099916368722916, 'rewards/rejected': 0.06385216861963272, 'rewards/accuracies': 1.0, 'rewards/margins': 0.027146989479660988, 'logps/chosen': -0.9980189800262451, 'logps/rejected': -1.2764151096343994, 'logits/chosen': -3.6716244220733643, 'logits/rejected': -3.6436550617218018, 'epoch': 90.0}\n",
      "{'loss': 22.4142, 'grad_norm': 84.443359375, 'learning_rate': 5.463099816548577e-09, 'rewards/chosen': 0.09066103398799896, 'rewards/rejected': 0.06341894716024399, 'rewards/accuracies': 1.0, 'rewards/margins': 0.027242090553045273, 'logps/chosen': -1.001400351524353, 'logps/rejected': -1.2807475328445435, 'logits/chosen': -3.674271583557129, 'logits/rejected': -3.6467764377593994, 'epoch': 95.0}\n",
      "{'loss': 22.4337, 'grad_norm': 75.58770751953125, 'learning_rate': 1.5229324522605947e-10, 'rewards/chosen': 0.0902944952249527, 'rewards/rejected': 0.06327839940786362, 'rewards/accuracies': 1.0, 'rewards/margins': 0.02701609954237938, 'logps/chosen': -1.0050657987594604, 'logps/rejected': -1.2821528911590576, 'logits/chosen': -3.6792616844177246, 'logits/rejected': -3.651015043258667, 'epoch': 100.0}\n",
      "{'train_runtime': 967.0351, 'train_samples_per_second': 1.655, 'train_steps_per_second': 0.103, 'train_loss': 23.265525283813478, 'epoch': 100.0}\n",
      "100% 100/100 [16:07<00:00,  9.67s/it]\n",
      "Saved lora to checkpoints/core_agent_ipo/final\n",
      "\n",
      "IPO training complete!\n",
      "  Adapter saved to: checkpoints/core_agent_ipo/final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"storageQuotaExceeded\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint backed up to Drive.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6a7b9c0d1",
   "metadata": {},
   "source": [
    "### 4.2 Verify IPO"
   ]
  },
  {
   "cell_type": "code",
   "id": "d4e5f6a7b9c0d1e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T08:49:15.062985Z",
     "start_time": "2026-02-14T08:49:15.003031Z"
    }
   },
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "elif not CONFIG[\"include_ipo\"]:\n",
    "    print(\"Skipping \\u2014 IPO disabled\")\n",
    "else:\n",
    "    ckpt_path = \"checkpoints/core_agent_ipo/final\"\n",
    "\n",
    "    print(\"IPO Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(ckpt_path):\n",
    "        files = os.listdir(ckpt_path)\n",
    "        print(f\"  \\u2713 IPO checkpoint: {ckpt_path} ({len(files)} files)\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 IPO checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "    tb_dir = \"checkpoints/core_agent_ipo\"\n",
    "    tb_files = []\n",
    "    if os.path.exists(tb_dir):\n",
    "        for root, dirs, fnames in os.walk(tb_dir):\n",
    "            for fn in fnames:\n",
    "                if fn.startswith(\"events.out.tfevents\"):\n",
    "                    tb_files.append(os.path.join(root, fn))\n",
    "    if tb_files:\n",
    "        print(f\"  \\u2713 TensorBoard logs found ({len(tb_files)} event files)\")\n",
    "        print(f\"    Monitor KL divergence: warn >0.3, abort >0.5\")\n",
    "    else:\n",
    "        print(f\"  \\u2014 No TensorBoard logs found\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPO Verification:\n",
      "============================================================\n",
      "  ✓ IPO checkpoint: checkpoints/core_agent_ipo/final (7 files)\n",
      "  ✓ TensorBoard logs found (1 event files)\n",
      "    Monitor KL divergence: warn >0.3, abort >0.5\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b9c0d1e2f3",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Test Model\n",
    "\n",
    "Load the trained model and generate Rust code interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b9c0d1e2f3a4",
   "metadata": {},
   "source": [
    "### 5.1 Load Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "a7b9c0d1e2f3a4b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T08:58:12.061947Z",
     "start_time": "2026-02-14T08:57:02.211510Z"
    }
   },
   "source": "from unsloth import FastLanguageModel\nfrom peft import PeftModel\nimport torch\n\nCHECKPOINT_PRIORITY = [\n    \"checkpoints/core_agent_ipo/final\",\n    \"checkpoints/core_agent/final\",\n    \"checkpoints/lang_rust/final\",\n]\n\n# Merged model can be loaded directly (no adapter needed)\nMERGED_PATH = \"checkpoints/gpt-oss-20b-rust-merged\"\n\nMODEL_PATH = None\nis_adapter = False\nfor path in CHECKPOINT_PRIORITY:\n    if os.path.exists(path) and os.path.exists(os.path.join(path, \"adapter_config.json\")):\n        MODEL_PATH = path\n        is_adapter = True\n        break\n\nif MODEL_PATH is None and os.path.exists(MERGED_PATH):\n    MODEL_PATH = MERGED_PATH\n    is_adapter = False\n\nif MODEL_PATH is None:\n    print(\"\\u2717 No checkpoint found. Train the model first.\")\nelse:\n    print(f\"Loading model from: {MODEL_PATH}\")\n    print(f\"  Type: {'LoRA adapter' if is_adapter else 'merged model'}\")\n\n    # GPT-OSS fused MoE experts (GptOssExperts) don't support BNB 4-bit\n    # quantization at load time (no `down_projs` sub-module for traversal).\n    # See: https://github.com/unslothai/unsloth/issues/3775\n    #\n    # Strategy: use Unsloth's pre-quantized BNB model (already 4-bit, no\n    # quantizer runs at load). Falls back to bfloat16 if unavailable.\n    model = None\n    base_name = \"openai/gpt-oss-20b\"\n\n    # Try 1: Pre-quantized BNB 4-bit (recommended by Unsloth BNB inference notebook)\n    try:\n        print(\"  Loading pre-quantized BNB 4-bit model...\")\n        model, tokenizer = FastLanguageModel.from_pretrained(\n            \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\",\n            max_seq_length=4096,\n            dtype=None,\n            load_in_4bit=False,  # Already quantized — don't re-quantize\n        )\n        print(\"  Mode: BNB 4-bit (pre-quantized)\")\n    except Exception as e:\n        print(f\"  Pre-quantized BNB failed: {e}\")\n\n    # Try 2: bfloat16 without quantization (~40GB, fits H100 80GB)\n    if model is None:\n        print(\"  Loading in bfloat16 (no quantization)...\")\n        model, tokenizer = FastLanguageModel.from_pretrained(\n            base_name,\n            max_seq_length=4096,\n            dtype=torch.bfloat16,\n            load_in_4bit=False,\n        )\n        print(\"  Mode: bfloat16 (no quantization)\")\n\n    print(\"=\" * 60)\n\n    if is_adapter:\n        print(f\"  Applying LoRA adapter from {MODEL_PATH}...\")\n        model = PeftModel.from_pretrained(model, MODEL_PATH)\n\n    FastLanguageModel.for_inference(model)\n    print(\"\\u2713 Model loaded!\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "Loading model from: checkpoints/core_agent_ipo/final\n",
      "  Type: LoRA adapter\n",
      "  Loading pre-quantized BNB 4-bit model...\n",
      "==((====))==  Unsloth 2026.2.1: Fast Gpt_Oss patching. Transformers: 4.57.6. vLLM: 0.15.1.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.179 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7cde2a2360f048598b1ce11da63173a7"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "model-00000-of-00002.safetensors:   0%|          | 0.00/4.79G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab5dc3b848314be3b8ce359f7d5e902e"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.80G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "35c365eb780e462aa8b2ca05fb6572c9"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.17G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c203b2a669a347dabbbc87409dd7b1fe"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4e21c465ec914b8f917e0c1c40ce44a0"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/165 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c16c572d8498438c8cc88212f5f74f48"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa2e94badb2749c0ad9e94053dcaeda3"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/27.9M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "06ffdddf0d8448a8b1bc0427b08c578d"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/446 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24c53f666f6c472e91f06a11fdfe2a9c"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dbb435d0bdfc4bcebfadbc8a5b247cd5"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mode: BNB 4-bit (pre-quantized)\n",
      "============================================================\n",
      "  Applying LoRA adapter from checkpoints/core_agent_ipo/final...\n",
      "Unsloth: PEFT set target_parameters but found no matching parameters.\n",
      "This is expected for MoE models - Unsloth handles MoE expert LoRA targeting separately.\n",
      "✓ Model loaded!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "b9c0d1e2f3a4b5c6",
   "metadata": {},
   "source": [
    "### 5.2 Generate Rust Code"
   ]
  },
  {
   "cell_type": "code",
   "id": "c0d1e2f3a4b5c6d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T09:00:47.878818Z",
     "start_time": "2026-02-14T08:58:15.213884Z"
    }
   },
   "source": "import sys\nsys.path.insert(0, \"scripts\")\nfrom dataset_formatters.harmony import encode_harmony_messages\n\nTEST_PROMPTS = [\n    \"Write a Rust function `fn merge_sorted(a: &[i32], b: &[i32]) -> Vec<i32>` that merges two sorted slices into a single sorted vector.\",\n    \"This Rust code fails the borrow checker. Fix it:\\n```rust\\nfn main() {\\n    let mut v = vec![1, 2, 3];\\n    let first = &v[0];\\n    v.push(4);\\n    println!(\\\"{}\\\", first);\\n}\\n```\",\n    \"Write an async Rust function using tokio that fetches a URL with reqwest, retries up to 3 times on failure, and returns the response body as a String.\",\n]\n\ndef generate_rust(prompt, max_tokens=1024):\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    formatted = encode_harmony_messages(\n        messages,\n        developer_instructions=\"You are a Rust programming expert. Write correct, idiomatic code.\",\n        add_generation_prompt=True,\n    )\n    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_tokens,\n            temperature=0.3,\n            do_sample=True,\n            top_p=0.9,\n        )\n    return tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n\nfor i, prompt in enumerate(TEST_PROMPTS, 1):\n    print(f\"\\n{'=' * 60}\")\n    print(f\"Test {i}: {prompt[:80]}...\")\n    print(\"=\" * 60)\n    response = generate_rust(prompt)\n    print(response)\n    print()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d1e2f3a4b5c6d7e8",
   "metadata": {},
   "source": [
    "### 5.3 Custom Prompt"
   ]
  },
  {
   "cell_type": "code",
   "id": "e2f3a4b5c6d7e8f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T09:01:03.978646Z",
     "start_time": "2026-02-14T09:00:47.898977Z"
    }
   },
   "source": [
    "CUSTOM_PROMPT = \"Write a Rust function that reads a CSV file and returns the sum of a specified column.\"\n",
    "\n",
    "print(f\"Prompt: {CUSTOM_PROMPT}\")\n",
    "print(\"=\" * 60)\n",
    "print(generate_rust(CUSTOM_PROMPT))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Write a Rust function that reads a CSV file and returns the sum of a specified column.\n",
      "============================================================\n",
      "﻿using System;\n",
      "using System.Collections.Generic;\n",
      "using System.Linq;\n",
      "using System.Text;\n",
      "using System.Threading.Tasks;\n",
      "\n",
      "namespace _04._Theatre_Promotion\n",
      "{\n",
      "    internal class Program\n",
      "    {\n",
      "        static void Main(string[] args)\n",
      "        {\n",
      "            string day = Console.ReadLine();\n",
      "            int age = int.Parse(Console.ReadLine());\n",
      "\n",
      "            double price = 0;\n",
      "\n",
      "            if (age < 0 || age > 122)\n",
      "            {\n",
      "                Console.WriteLine(\"Error!\");\n",
      "                return;\n",
      "            }\n",
      "\n",
      "            switch (day)\n",
      "            {\n",
      "                case \"Weekday\":\n",
      "                    if (age <= 18 || age >= 64)\n",
      "                        price = 12;\n",
      "                    else\n",
      "                        price = 18;\n",
      "                    break;\n",
      "                case \"Weekend\":\n",
      "                    if (age <= 18 || age >= 64)\n",
      "                        price = 15;\n",
      "                    else\n",
      "                        price = 20;\n",
      "                    break;\n",
      "                case \"Holiday\":\n",
      "                    if (age <= 18)\n",
      "                        price = 5;\n",
      "                    else if (age <= 64)\n",
      "                        price = 12;\n",
      "                    else\n",
      "                        price = 10;\n",
      "                    break;\n",
      "                default:\n",
      "                    Console.WriteLine(\"Error!\");\n",
      "                    return;\n",
      "            }\n",
      "\n",
      "            Console.WriteLine($\"{price} $\");\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "f3a4b5c6d7e8f9a0",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Export\n",
    "\n",
    "Merge the final adapter and export to HuggingFace + GGUF formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5c6d7e8f9a0b1",
   "metadata": {},
   "source": [
    "### 6.1 Export to GGUF"
   ]
  },
  {
   "cell_type": "code",
   "id": "b5c6d7e8f9a0b1c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T09:24:42.498058Z",
     "start_time": "2026-02-14T09:11:09.230790Z"
    }
   },
   "source": [
    "ADAPTER_PRIORITY = [\n",
    "    \"checkpoints/core_agent_ipo/final\",\n",
    "    \"checkpoints/core_agent/final\",\n",
    "    \"checkpoints/lang_rust/final\",\n",
    "]\n",
    "\n",
    "adapter_path = None\n",
    "for path in ADAPTER_PRIORITY:\n",
    "    if os.path.exists(path):\n",
    "        adapter_path = path\n",
    "        break\n",
    "\n",
    "if adapter_path is None:\n",
    "    print(\"\\u2717 No adapter checkpoint found.\")\n",
    "else:\n",
    "    export_dir = \"checkpoints/gpt-oss-20b-rust-export-v3\"\n",
    "    print(f\"Exporting adapter: {adapter_path}\")\n",
    "    print(f\"Output: {export_dir}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/19_merge_adapter.py \\\n",
    "        --adapter_path {adapter_path} \\\n",
    "        --output_dir {export_dir} \\\n",
    "        --export_formats hf gguf_q4\n",
    "\n",
    "    drive_helper.backup(export_dir, \"checkpoints/gpt-oss-20b-rust-export-v3\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nExport backed up to Drive.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting adapter: checkpoints/core_agent_ipo/final\n",
      "Output: checkpoints/gpt-oss-20b-rust-export-v3\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Merging Adapter: merge-lang-rust\n",
      "============================================================\n",
      "  Base model: openai/gpt-oss-20b\n",
      "  Adapter: checkpoints/core_agent_ipo/final\n",
      "  Output: checkpoints/gpt-oss-20b-rust-export-v3\n",
      "  Formats: ['hf', 'gguf_q4']\n",
      "/content/llm-training-pipeline/scripts/pipeline_lib/unsloth_utils.py:138: UserWarning: WARNING: Unsloth should be imported before [transformers] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n",
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "2026-02-14 09:11:13.195345: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-14 09:11:13.208100: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1771060273.223848   30675 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1771060273.228896   30675 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1771060273.241764   30675 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771060273.241785   30675 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771060273.241789   30675 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771060273.241790   30675 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-02-14 09:11:13.245188: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2026.2.1: Fast Gpt_Oss patching. Transformers: 4.57.6. vLLM: 0.15.1.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.179 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Loading checkpoint shards: 100% 4/4 [00:03<00:00,  1.16it/s]\n",
      "Unsloth: PEFT set target_parameters but found no matching parameters.\n",
      "This is expected for MoE models - Unsloth handles MoE expert LoRA targeting separately.\n",
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Unsloth: Copying 3 files from cache to `checkpoints/gpt-oss-20b-rust-export-v3/hf`: 100% 3/3 [00:25<00:00,  8.36s/it]\n",
      "Successfully copied all 3 files from cache to `checkpoints/gpt-oss-20b-rust-export-v3/hf`\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Unsloth: Preparing safetensor model files: 100% 3/3 [00:00<00:00, 75800.67it/s]\n",
      "Unsloth: Merging weights into 16bit: 100% 3/3 [03:56<00:00, 78.71s/it]\n",
      "Unsloth: Regenerating safetensors index for dequantized MXFP4 model...\n",
      "Unsloth: Merge process complete. Saved to `/content/llm-training-pipeline/checkpoints/gpt-oss-20b-rust-export-v3/hf`\n",
      "Exported HuggingFace format to checkpoints/gpt-oss-20b-rust-export-v3/hf\n",
      "Unsloth: Merging model weights to mxfp4 format...\n",
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
      "Fetching 1 files: 100% 1/1 [00:00<00:00,  9.09it/s]\n",
      "Checking cache directory for required files...\n",
      "Unsloth: Copying 3 files from cache to `checkpoints/gpt-oss-20b-rust-export-v3/gguf_q4`: 100% 3/3 [00:33<00:00, 11.06s/it]\n",
      "Successfully copied all 3 files from cache to `checkpoints/gpt-oss-20b-rust-export-v3/gguf_q4`\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Unsloth: Preparing safetensor model files: 100% 3/3 [00:00<00:00, 96052.76it/s]\n",
      "Unsloth: Merging weights into mxfp4: 100% 3/3 [00:25<00:00,  8.57s/it]\n",
      "Unsloth: Merge process complete. Saved to `/content/llm-training-pipeline/checkpoints/gpt-oss-20b-rust-export-v3/gguf_q4`\n",
      "Unsloth: Converting to GGUF format...\n",
      "Unsloth: GPT-OSS model detected - using special conversion settings\n",
      "==((====))==  Unsloth: Conversion from HF to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF  might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF  to ['None'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: llama.cpp found in the system. Skipping installation.\n",
      "Unsloth: Preparing converter script...\n",
      "Unsloth: [1] Converting model into  GGUF format.\n",
      "This might take 3 minutes...\n",
      "Unsloth: Initial conversion completed! Files: ['checkpoints/gpt-oss-20b-rust-export-v3/gguf_q4_gguf/gpt-oss-20b.MXFP4.gguf']\n",
      "Unsloth: GPT-OSS model - skipping additional quantizations\n",
      "Unsloth: All GGUF conversions completed successfully!\n",
      "Generated files: ['checkpoints/gpt-oss-20b-rust-export-v3/gguf_q4_gguf/gpt-oss-20b.MXFP4.gguf']\n",
      "Unsloth: example usage for text only LLMs: llama.cpp/llama-cli --model checkpoints/gpt-oss-20b-rust-export-v3/gguf_q4_gguf/gpt-oss-20b.MXFP4.gguf -p \"why is the sky blue?\"\n",
      "Unsloth: Saved Ollama Modelfile to checkpoints/gpt-oss-20b-rust-export-v3/gguf_q4_gguf/Modelfile\n",
      "Unsloth: convert model to ollama format by running - ollama create model_name -f checkpoints/gpt-oss-20b-rust-export-v3/gguf_q4_gguf/Modelfile\n",
      "Exported GGUF (q4_k_m) to checkpoints/gpt-oss-20b-rust-export-v3/gguf_q4\n",
      "\n",
      "Running smoke test...\n",
      "==((====))==  Unsloth 2026.2.1: Fast Gpt_Oss patching. Transformers: 4.57.6. vLLM: 0.15.1.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.179 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Loading checkpoint shards: 100% 3/3 [00:06<00:00,  2.15s/it]\n",
      "  Smoke test FAILED: Unsloth: Critical error since some weights are not initialized.\n",
      "Please try updating Unsloth, transformers and timm via:\n",
      "`pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo transformers timm`\n",
      "<LogRecord: transformers.modeling_utils, 30, /usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py, 5525, \"Some weights of the model checkpoint at checkpoints/gpt-oss-20b-rust-export-v3/hf were not used when initializing GptOssForCausalLM: ['model.layers.0.mlp.router.bias', 'model.layers.0.mlp.router.weight', 'model.layers.1.mlp.router.bias', 'model.layers.1.mlp.router.weight', 'model.layers.10.mlp.router.bias', 'model.layers.10.mlp.router.weight', 'model.layers.11.mlp.router.bias', 'model.layers.11.mlp.router.weight', 'model.layers.12.mlp.router.bias', 'model.layers.12.mlp.router.weight', 'model.layers.13.mlp.router.bias', 'model.layers.13.mlp.router.weight', 'model.layers.14.mlp.router.bias', 'model.layers.14.mlp.router.weight', 'model.layers.15.mlp.router.bias', 'model.layers.15.mlp.router.weight', 'model.layers.16.mlp.router.bias', 'model.layers.16.mlp.router.weight', 'model.layers.17.mlp.router.bias', 'model.layers.17.mlp.router.weight', 'model.layers.18.mlp.router.bias', 'model.layers.18.mlp.router.weight', 'model.layers.19.mlp.router.bias', 'model.layers.19.mlp.router.weight', 'model.layers.2.mlp.router.bias', 'model.layers.2.mlp.router.weight', 'model.layers.20.mlp.router.bias', 'model.layers.20.mlp.router.weight', 'model.layers.21.mlp.router.bias', 'model.layers.21.mlp.router.weight', 'model.layers.22.mlp.router.bias', 'model.layers.22.mlp.router.weight', 'model.layers.23.mlp.router.bias', 'model.layers.23.mlp.router.weight', 'model.layers.3.mlp.router.bias', 'model.layers.3.mlp.router.weight', 'model.layers.4.mlp.router.bias', 'model.layers.4.mlp.router.weight', 'model.layers.5.mlp.router.bias', 'model.layers.5.mlp.router.weight', 'model.layers.6.mlp.router.bias', 'model.layers.6.mlp.router.weight', 'model.layers.7.mlp.router.bias', 'model.layers.7.mlp.router.weight', 'model.layers.8.mlp.router.bias', 'model.layers.8.mlp.router.weight', 'model.layers.9.mlp.router.bias', 'model.layers.9.mlp.router.weight']\n",
      "- This IS expected if you are initializing GptOssForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GptOssForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\">\n",
      "\n",
      "============================================================\n",
      "Merge complete!\n",
      "  Output: checkpoints/gpt-oss-20b-rust-export-v3\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'drive_helper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2414694361.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python scripts/19_merge_adapter.py          --adapter_path {adapter_path}          --output_dir {export_dir}          --export_formats hf gguf_q4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mdrive_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"checkpoints/gpt-oss-20b-rust-export-v3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mDRIVE_MODE\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"local\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nExport backed up to Drive.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'drive_helper' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "c6d7e8f9a0b1c2d3",
   "metadata": {},
   "source": [
    "### 6.2 Download GGUF"
   ]
  },
  {
   "cell_type": "code",
   "id": "d7e8f9a0b1c2d3e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T09:26:21.254136Z",
     "start_time": "2026-02-14T09:26:21.141955Z"
    }
   },
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    import glob\n",
    "\n",
    "    export_dir = \"checkpoints/gpt-oss-20b-rust-export-v3\"\n",
    "    gguf_files = glob.glob(os.path.join(export_dir, \"*.gguf\"))\n",
    "\n",
    "    if gguf_files:\n",
    "        gguf_path = gguf_files[0]\n",
    "        size_gb = os.path.getsize(gguf_path) / (1024**3)\n",
    "        print(f\"Downloading: {os.path.basename(gguf_path)} ({size_gb:.1f} GB)\")\n",
    "        files.download(gguf_path)\n",
    "    else:\n",
    "        print(\"\\u2717 No GGUF file found. Run export (6.1) first.\")\n",
    "else:\n",
    "    print(\"Download not available outside Colab.\")\n",
    "    print(\"GGUF file is at: checkpoints/gpt-oss-20b-rust-export-v3/\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ No GGUF file found. Run export (6.1) first.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "8v6ymy6v2kf",
   "source": "### 6.3 Upload to HuggingFace Hub",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "rsbhf5sc7r",
   "source": "# --- Configuration ---\nHF_REPO_ID = \"\"  # e.g. \"your-username/gpt-oss-20b-rust-agent-v3\"\nHF_PRIVATE = True\n\nassert HF_REPO_ID, \"Set HF_REPO_ID above before running this cell.\"\n\nimport glob\nfrom huggingface_hub import HfApi\n\n# Authenticate: try Colab Secrets first, then interactive login\ntry:\n    from google.colab import userdata\n    hf_token = userdata.get(\"HF_TOKEN\")\n    print(\"Using HF_TOKEN from Colab Secrets.\")\nexcept Exception:\n    from huggingface_hub import login\n    login()\n    hf_token = None  # login() stores token globally\n\napi = HfApi(token=hf_token)\n\n# Create repo (no-op if it already exists)\napi.create_repo(repo_id=HF_REPO_ID, private=HF_PRIVATE, exist_ok=True)\nprint(f\"Repo ready: https://huggingface.co/{HF_REPO_ID}\")\n\n# --- Model card ---\nexport_dir = \"checkpoints/gpt-oss-20b-rust-export-v3\"\nhf_dir = os.path.join(export_dir, \"hf\")\n\nmodel_card = \"\"\"\\\n---\nbase_model: openai/gpt-oss-20b\ntags:\n  - rust\n  - code-agent\n  - gpt-oss\n  - qlora\n  - unsloth\nlicense: apache-2.0\npipeline_tag: text-generation\n---\n\n# GPT-OSS 20B Rust Agent (v3 — Strandset)\n\nFine-tuned from [openai/gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b) using the\n[Strandset-Rust-v1](https://huggingface.co/datasets/Fortytwo-Network/Strandset-Rust-v1) dataset (191K examples).\n\n## Training Pipeline\n\n1. **Lang Adapter** — Rust domain specialisation (code generation/completion)\n2. **Core Agent SFT** — Debug and review training (bug detection/code review)\n3. **IPO** — Synthetic preference pairs (if enabled)\n\nTrained with [Unsloth](https://github.com/unslothai/unsloth) QLoRA on NVIDIA H100 80GB.\n\n## Usage\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"{repo_id}\")\ntokenizer = AutoTokenizer.from_pretrained(\"{repo_id}\")\n```\n\n## GGUF\n\nA quantised GGUF file is included for use with llama.cpp or Ollama.\n\"\"\".format(repo_id=HF_REPO_ID)\n\nreadme_path = os.path.join(hf_dir, \"README.md\")\nwith open(readme_path, \"w\") as f:\n    f.write(model_card)\nprint(f\"Wrote model card to {readme_path}\")\n\n# --- Upload HF safetensors model ---\nassert os.path.isdir(hf_dir), f\"HF export dir not found: {hf_dir}. Run export (6.1) first.\"\nprint(f\"Uploading HF model from {hf_dir} ...\")\napi.upload_folder(\n    folder_path=hf_dir,\n    repo_id=HF_REPO_ID,\n    commit_message=\"Upload merged HF model (v3 — Strandset pipeline)\",\n    token=hf_token,\n)\nprint(\"HF model uploaded.\")\n\n# --- Upload GGUF file ---\ngguf_files = glob.glob(os.path.join(export_dir, \"**/*.gguf\"), recursive=True)\nif gguf_files:\n    gguf_path = gguf_files[0]\n    gguf_name = os.path.basename(gguf_path)\n    size_gb = os.path.getsize(gguf_path) / (1024**3)\n    print(f\"Uploading GGUF: {gguf_name} ({size_gb:.1f} GB) ...\")\n    api.upload_file(\n        path_or_fileobj=gguf_path,\n        path_in_repo=gguf_name,\n        repo_id=HF_REPO_ID,\n        commit_message=f\"Upload GGUF quantisation ({gguf_name})\",\n        token=hf_token,\n    )\n    print(\"GGUF uploaded.\")\nelse:\n    print(\"No GGUF file found — skipping. Run export (6.1) to generate one.\")\n\nprint(f\"\\nDone! View your model at: https://huggingface.co/{HF_REPO_ID}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e8f9a0b1c2d3e4f5",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Complete!\n",
    "\n",
    "Your GPT-OSS 20B Rust coding agent (v3 \\u2014 Strandset) is trained and ready to use.\n",
    "\n",
    "**Data source:** [Strandset-Rust-v1](https://huggingface.co/datasets/Fortytwo-Network/Strandset-Rust-v1) (191K examples, Apache 2.0)\n",
    "\n",
    "**Pipeline:**\n",
    "1. Lang Adapter: Rust domain specialisation from code generation/completion examples\n",
    "2. Core Agent SFT: Debug and review training from bug_detection/code_review examples\n",
    "3. IPO: Synthetic preference pairs from bug_detection (if enabled)\n",
    "\n",
    "**Outputs:**\n",
    "- Checkpoints: `checkpoints/core_agent_{ipo}/final`\n",
    "- Exported model: `checkpoints/gpt-oss-20b-rust-export-v3/`\n",
    "- All backed up to Google Drive: `gpt-oss-20b-rust-agent-v3/`\n",
    "\n",
    "**Compared to v2:**\n",
    "- No Rust toolchain required \\u2014 runs on any Colab GPU instance\n",
    "- No cargo-mutants or trajectory generation \\u2014 faster setup\n",
    "- No GRPO RL \\u2014 no execution-based rewards\n",
    "- For better results, consider upgrading to v2 with mutation data + GRPO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}