{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4e5f6a7b8",
   "metadata": {},
   "source": [
    "# Train GPT-OSS 20B → Rust Coding Agent (v3 — Strandset)\n",
    "\n",
    "Simplified pipeline using [Strandset-Rust-v1](https://huggingface.co/datasets/Fortytwo-Network/Strandset-Rust-v1) (191K verified Rust examples, Apache 2.0) as the sole data source.\n",
    "\n",
    "**Key differences from v2:**\n",
    "- **No Rust toolchain** — no `rustup`, `cargo-mutants`, or compilation needed\n",
    "- **No mutation/trajectory generation** — data comes entirely from Strandset\n",
    "- **No GRPO** — no execution-based rewards without cargo\n",
    "- **IPO from synthetic preferences** — bug_detection pairs (fixed=chosen, buggy=rejected)\n",
    "\n",
    "**3-Phase Pipeline:**\n",
    "1. **Lang Adapter** — Rust domain specialisation via QLoRA (script 13 + 19)\n",
    "2. **Core Agent SFT** — Debug/review training from Strandset (script 14)\n",
    "3. **IPO Preference** — Synthetic preference pairs from bug_detection (script 17)\n",
    "\n",
    "**Requirements:**\n",
    "- **GPU**: A100 40GB+ (H100 80GB recommended for FP8)\n",
    "- **Storage**: Google Drive for persistent checkpoints\n",
    "- **No Rust toolchain required**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5f6a7b8c9",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6a7b8c9d0",
   "metadata": {},
   "source": [
    "### 0.1 Mount Google Drive & Clone Repository\n",
    "\n",
    "**PyCharm / headless users:** If `drive.mount()` doesn't work, set `use_service_account = True`\n",
    "and provide your service-account JSON key in Step 0.3."
   ]
  },
  {
   "cell_type": "code",
   "id": "d4e5f6a7b8c9d0e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T05:06:40.243995Z",
     "start_time": "2026-02-14T05:06:39.087042Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "use_service_account = True\n",
    "\n",
    "DRIVE_MOUNTED = False\n",
    "\n",
    "if IN_COLAB and not use_service_account:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        DRIVE_MOUNTED = True\n",
    "        print(\"Google Drive mounted\")\n",
    "    except Exception as e:\n",
    "        print(f\"drive.mount() failed: {e}\")\n",
    "        print(\"Falling back to local-only mode.\")\n",
    "        print(\"Tip: set use_service_account=True and provide a JSON key in Step 0.3.\")\n",
    "elif IN_COLAB and use_service_account:\n",
    "    print(\"Service-account mode selected \\u2014 skipping drive.mount()\")\n",
    "    print(\"Configure credentials in Step 0.3.\")\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "\n",
    "REPO_URL = \"https://github.com/rmarnold/llm-training-pipeline.git\"\n",
    "BRANCH = \"main\"\n",
    "\n",
    "REPO_DIR = \"/content/llm-training-pipeline\"\n",
    "\n",
    "if IN_COLAB:\n",
    "    if os.path.exists(REPO_DIR):\n",
    "        %cd {REPO_DIR}\n",
    "        !git pull origin {BRANCH}\n",
    "    else:\n",
    "        !git clone -b {BRANCH} {REPO_URL} {REPO_DIR}\n",
    "        %cd {REPO_DIR}\n",
    "\n",
    "    PROJECT_ROOT = REPO_DIR\n",
    "else:\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"\\nProject root: {PROJECT_ROOT}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service-account mode selected — skipping drive.mount()\n",
      "Configure credentials in Step 0.3.\n",
      "Cloning into '/content/llm-training-pipeline'...\n",
      "remote: Enumerating objects: 1089, done.\u001b[K\n",
      "remote: Counting objects: 100% (260/260), done.\u001b[K\n",
      "remote: Compressing objects: 100% (184/184), done.\u001b[K\n",
      "remote: Total 1089 (delta 148), reused 170 (delta 75), pack-reused 829 (from 1)\u001b[K\n",
      "Receiving objects: 100% (1089/1089), 1.80 MiB | 5.73 MiB/s, done.\n",
      "Resolving deltas: 100% (689/689), done.\n",
      "/content/llm-training-pipeline\n",
      "\n",
      "Project root: /content/llm-training-pipeline\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8c9d0e1f2",
   "metadata": {},
   "source": [
    "### 0.2 Install Dependencies\n",
    "\n",
    "Installs pipeline deps and latest Unsloth. **No Rust toolchain needed** — all training data\n",
    "comes from Strandset.\n",
    "\n",
    "**Note:** flash-attn is intentionally NOT installed. FA3 is incompatible with GPT-OSS\n",
    "backward passes. Unsloth's Flex Attention replaces it automatically."
   ]
  },
  {
   "cell_type": "code",
   "id": "f6a7b8c9d0e1f2a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T05:10:04.579482Z",
     "start_time": "2026-02-14T05:06:40.253027Z"
    }
   },
   "source": [
    "if IN_COLAB:\n",
    "    print(\"Installing Python dependencies...\")\n",
    "    print(\"=\" * 60)\n",
    "    !pip install -q -e \".[gpt_oss,colab]\"\n",
    "\n",
    "    # Fix pyarrow binary incompatibility with datasets 4.x on Colab\n",
    "    !pip install -q --force-reinstall pyarrow\n",
    "\n",
    "    # Force latest Unsloth with Split LoRA + FP8 RL\n",
    "    print(\"\\nInstalling latest Unsloth (Split LoRA + Flex Attention)...\")\n",
    "    !pip install -q --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo\n",
    "    !pip install -q \"unsloth[colab-new]\"\n",
    "\n",
    "    # vLLM for FP8 inference (H100 only, optional)\n",
    "    !pip install -q vllm>=0.12.0 2>/dev/null || true\n",
    "\n",
    "    # Verification\n",
    "    from importlib.metadata import version, PackageNotFoundError\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Dependency Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for pkg in [\"unsloth\", \"trl\", \"peft\", \"datasets\", \"tiktoken\", \"vllm\"]:\n",
    "        try:\n",
    "            ver = version(pkg)\n",
    "            print(f\"\\u2713 {pkg}: {ver}\")\n",
    "        except PackageNotFoundError:\n",
    "            if pkg == \"vllm\":\n",
    "                print(f\"\\u2014 {pkg}: not installed (optional, H100 FP8 only)\")\n",
    "            else:\n",
    "                print(f\"\\u2717 {pkg}: not installed\")\n",
    "\n",
    "    print(\"\\nNote: No Rust toolchain needed for v3 (Strandset-only pipeline)\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"Running locally \\u2014 ensure deps are installed:\")\n",
    "    print(\"  pip install -e '.[gpt_oss]'\")\n",
    "    print(\"  pip install --upgrade unsloth unsloth_zoo\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing Python dependencies...\n",
      "============================================================\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.3/432.3 kB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m115.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.5/376.5 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m129.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m915.7/915.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.9/837.9 kB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building editable for llm-training-pipeline (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for pyfastcopy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.9.0+cu128 requires torch==2.9.0, but you have torch 2.10.0 which is incompatible.\n",
      "fastai 2.8.6 requires torch<2.10,>=1.10, but you have torch 2.10.0 which is incompatible.\n",
      "cuda-python 12.9.5 requires cuda-bindings~=12.9.5, but you have cuda-bindings 12.9.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "Installing latest Unsloth (Split LoRA + Flex Attention)...\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.3/432.3 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.5/376.5 kB\u001b[0m \u001b[31m577.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: unsloth 2026.2.1 does not provide the extra 'triton'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "============================================================\n",
      "Dependency Verification:\n",
      "============================================================\n",
      "✓ unsloth: 2026.2.1\n",
      "✓ trl: 0.24.0\n",
      "✓ peft: 0.18.1\n",
      "✓ datasets: 4.3.0\n",
      "✓ tiktoken: 0.12.0\n",
      "✓ vllm: 0.15.1\n",
      "\n",
      "Note: No Rust toolchain needed for v3 (Strandset-only pipeline)\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0e1f2a3b4",
   "metadata": {},
   "source": [
    "### 0.3 Configure Pipeline\n",
    "\n",
    "**Training Scope** (`training_scope`):\n",
    "- `full` \\u2014 All 3 phases (Lang Adapter + Core Agent + IPO)\n",
    "- `quick_test` \\u2014 Short runs (100 steps each) to verify setup\n",
    "- `lang_adapter_only` \\u2014 Only train lang_rust adapter + merge\n",
    "\n",
    "**Service Account Setup** (for Drive backup):\n",
    "1. Set `use_service_account = True` in cell 0.1\n",
    "2. Run cell 0.3 \\u2014 it will try Colab Secrets, then file, then paste prompt\n",
    "3. Set `DRIVE_FOLDER_ID` in Colab Secrets, or set `drive_folder_id` below"
   ]
  },
  {
   "cell_type": "code",
   "id": "b8c9d0e1f2a3b4c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T05:11:38.745754Z",
     "start_time": "2026-02-14T05:10:04.642271Z"
    }
   },
   "source": "import json\ntraining_scope = \"quick_test\"  # \"full\", \"quick_test\", \"lang_adapter_only\"\n\ngpu_tier = \"h100_80gb\"  # \"a100_40gb\", \"a100_80gb\", \"h100_80gb\"\n\nmax_steps_override = 0  # Set >0 to cap all stages (0 = use defaults)\n\ninclude_ipo = True  # False to skip IPO preference training\n\nenable_qat_export = False  # True for MXFP4 QAT export\n\n# ============================================================\n# SERVICE ACCOUNT CREDENTIALS\n# ============================================================\n# Priority order:\n#   1. Existing file at /content/service_account.json (instant, no timeout)\n#   2. Colab Secrets (only if no file found — may timeout outside browser UI)\n#   3. Paste JSON key via input() prompt\n#   4. Fall back to local mode (no Drive backup)\n\ndrive_folder_id = \"\"  # Google Drive folder ID\n\n_SA_VM_PATH = \"/content/service_account.json\"\nservice_account_key = \"\"\n\nif use_service_account and IN_COLAB:\n    # 1. Check for existing file first (avoids Colab Secrets timeout on re-runs)\n    if os.path.exists(_SA_VM_PATH):\n        service_account_key = _SA_VM_PATH\n        print(f\"Using existing key file: {_SA_VM_PATH}\")\n    else:\n        # 2. Try Colab Secrets (may timeout if not running in browser UI)\n        try:\n            from google.colab import userdata\n            _key_json = userdata.get(\"SERVICE_ACCOUNT_KEY\")\n            if _key_json:\n                with open(_SA_VM_PATH, \"w\") as _f:\n                    _f.write(_key_json)\n                service_account_key = _SA_VM_PATH\n                print(\"Service account key loaded from Colab Secrets.\")\n        except Exception as _e:\n            print(f\"  Colab Secrets lookup failed: {type(_e).__name__}: {_e}\")\n\n        # 3. Fall back to paste prompt\n        if not service_account_key:\n            try:\n                print(\"No service account key found.\")\n                _key_text = input(\"Paste service account JSON (entire content in one go): \")\n                _key_text = _key_text.strip()\n                if _key_text:\n                    json.loads(_key_text)\n                    with open(_SA_VM_PATH, \"w\") as _f:\n                        _f.write(_key_text)\n                    service_account_key = _SA_VM_PATH\n                    print(f\"Saved to {_SA_VM_PATH}\")\n            except json.JSONDecodeError:\n                print(\"  Invalid JSON \\u2014 key not saved.\")\n            except EOFError:\n                pass\n\n    # Resolve drive_folder_id\n    if not drive_folder_id:\n        try:\n            from google.colab import userdata\n            drive_folder_id = userdata.get(\"DRIVE_FOLDER_ID\") or \"\"\n            if drive_folder_id:\n                print(f\"Drive folder ID loaded from Colab Secrets.\")\n        except Exception:\n            pass\n\n    if not drive_folder_id and service_account_key:\n        _fid = input(\"Enter Google Drive folder ID (from URL): \").strip()\n        if _fid:\n            drive_folder_id = _fid\n\n    if not service_account_key:\n        print(\"No service account key \\u2014 Drive backup disabled.\")\n\nelif use_service_account:\n    for _path in [_SA_VM_PATH, \"service_account.json\"]:\n        if os.path.exists(_path):\n            service_account_key = _path\n            print(f\"Using key file: {_path}\")\n            break\n    if not service_account_key:\n        print(\"Running locally \\u2014 set service_account_key to your JSON key path.\")\n\n# ============================================================\n# DRIVE MODE\n# ============================================================\nfrom scripts.pipeline_lib.drive_utils import DriveHelper\n\nDRIVE_BASE = \"/content/drive/MyDrive/gpt-oss-20b-rust-agent-v3\"\n\nif DRIVE_MOUNTED:\n    DRIVE_MODE = \"mounted\"\nelif use_service_account and service_account_key and drive_folder_id:\n    DRIVE_MODE = \"service_account\"\nelse:\n    DRIVE_MODE = \"local\"\n\ndrive_helper = DriveHelper(\n    mode=DRIVE_MODE,\n    drive_base=DRIVE_BASE,\n    credentials_path=service_account_key or None,\n    folder_id=drive_folder_id or None,\n)\n\n# ============================================================\n# GPU TIER CONFIGS\n# ============================================================\n\nGPU_CONFIGS = {\n    \"a100_40gb\": {\n        \"moe_backend\": \"unsloth_triton\",\n        \"load_mode\": \"4bit\",\n        \"fast_inference\": False,\n        \"lang_rust\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 8192, \"max_steps\": 3000},\n        \"core_agent\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 12288, \"max_steps\": 2000},\n        \"ipo\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 12288, \"max_steps\": 1000},\n    },\n    \"a100_80gb\": {\n        \"moe_backend\": \"unsloth_triton\",\n        \"load_mode\": \"4bit\",\n        \"fast_inference\": False,\n        \"lang_rust\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 8192, \"max_steps\": 5000},\n        \"core_agent\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 16384, \"max_steps\": 3000},\n        \"ipo\": {\"batch\": 1, \"grad_accum\": 16, \"seq_len\": 16384, \"max_steps\": 2000},\n    },\n    \"h100_80gb\": {\n        \"moe_backend\": \"grouped_mm\",\n        \"load_mode\": \"fp8\",\n        \"fast_inference\": True,\n        \"lang_rust\": {\"batch\": 2, \"grad_accum\": 4, \"seq_len\": 8192, \"max_steps\": 5000},\n        \"core_agent\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 16384, \"max_steps\": 3000},\n        \"ipo\": {\"batch\": 1, \"grad_accum\": 16, \"seq_len\": 16384, \"max_steps\": 2000},\n    },\n}\n\nif training_scope == \"quick_test\":\n    max_steps_override = 100\n\ngpu_cfg = GPU_CONFIGS[gpu_tier]\n\nCONFIG = {\n    \"training_scope\": training_scope,\n    \"gpu_tier\": gpu_tier,\n    \"include_ipo\": include_ipo,\n    \"enable_qat_export\": enable_qat_export,\n    \"moe_backend\": gpu_cfg[\"moe_backend\"],\n    \"load_mode\": gpu_cfg[\"load_mode\"],\n    \"fast_inference\": gpu_cfg[\"fast_inference\"],\n    # Lang adapter\n    \"lang_rust_batch\": gpu_cfg[\"lang_rust\"][\"batch\"],\n    \"lang_rust_grad_accum\": gpu_cfg[\"lang_rust\"][\"grad_accum\"],\n    \"lang_rust_seq_len\": gpu_cfg[\"lang_rust\"][\"seq_len\"],\n    \"lang_rust_max_steps\": max_steps_override or gpu_cfg[\"lang_rust\"][\"max_steps\"],\n    # Core agent\n    \"core_agent_batch\": gpu_cfg[\"core_agent\"][\"batch\"],\n    \"core_agent_grad_accum\": gpu_cfg[\"core_agent\"][\"grad_accum\"],\n    \"core_agent_seq_len\": gpu_cfg[\"core_agent\"][\"seq_len\"],\n    \"core_agent_max_steps\": max_steps_override or gpu_cfg[\"core_agent\"][\"max_steps\"],\n    # IPO\n    \"ipo_batch\": gpu_cfg[\"ipo\"][\"batch\"],\n    \"ipo_grad_accum\": gpu_cfg[\"ipo\"][\"grad_accum\"],\n    \"ipo_seq_len\": gpu_cfg[\"ipo\"][\"seq_len\"],\n    \"ipo_max_steps\": max_steps_override or gpu_cfg[\"ipo\"][\"max_steps\"],\n    # Eval\n    \"eval_num_samples\": 10 if training_scope == \"quick_test\" else 50,\n}\n\nprint(\"=\" * 60)\nprint(\"PIPELINE CONFIGURATION (v3 \\u2014 Strandset)\")\nprint(\"=\" * 60)\nprint(f\"\\nScope: {training_scope.upper()}\")\nprint(f\"GPU tier: {gpu_tier}\")\nprint(f\"MoE backend: {CONFIG['moe_backend']}\")\nprint(f\"Load mode: {CONFIG['load_mode']}\")\nprint(f\"Fast inference (vLLM): {CONFIG['fast_inference']}\")\nprint(f\"Include IPO: {include_ipo}\")\nprint(f\"QAT export: {enable_qat_export}\")\nprint(f\"Drive mode: {DRIVE_MODE}\")\nif max_steps_override:\n    print(f\"Max steps override: {max_steps_override}\")\nprint(f\"\\nLang Adapter:  batch={CONFIG['lang_rust_batch']} x grad_accum={CONFIG['lang_rust_grad_accum']}, seq={CONFIG['lang_rust_seq_len']}, steps={CONFIG['lang_rust_max_steps']}\")\nprint(f\"Core Agent:    batch={CONFIG['core_agent_batch']} x grad_accum={CONFIG['core_agent_grad_accum']}, seq={CONFIG['core_agent_seq_len']}, steps={CONFIG['core_agent_max_steps']}\")\nif include_ipo:\n    print(f\"IPO:           batch={CONFIG['ipo_batch']} x grad_accum={CONFIG['ipo_grad_accum']}, seq={CONFIG['ipo_seq_len']}, steps={CONFIG['ipo_max_steps']}\")\nprint(\"=\" * 60)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2a3b4c5d6",
   "metadata": {},
   "source": [
    "### 0.4 Set Up Persistent Storage"
   ]
  },
  {
   "cell_type": "code",
   "id": "d0e1f2a3b4c5d6e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T05:11:46.348402Z",
     "start_time": "2026-02-14T05:11:38.789478Z"
    }
   },
   "source": [
    "DRIVE_SUBDIRS = [\n",
    "    \"checkpoints/lang_rust\",\n",
    "    \"checkpoints/core_agent\",\n",
    "    \"checkpoints/core_agent_ipo\",\n",
    "    \"checkpoints/gpt-oss-20b-rust-merged\",\n",
    "    \"data/rust/strandset\",\n",
    "    \"logs\",\n",
    "]\n",
    "\n",
    "if DRIVE_MODE == \"mounted\":\n",
    "    print(f\"Setting up storage at: {DRIVE_BASE}\")\n",
    "    for subdir in DRIVE_SUBDIRS:\n",
    "        os.makedirs(os.path.join(DRIVE_BASE, subdir), exist_ok=True)\n",
    "\n",
    "    for dir_name in [\"checkpoints\", \"data\", \"logs\"]:\n",
    "        local_path = os.path.join(PROJECT_ROOT, dir_name)\n",
    "        drive_path = os.path.join(DRIVE_BASE, dir_name)\n",
    "\n",
    "        if os.path.exists(local_path) and not os.path.islink(local_path):\n",
    "            !cp -r {local_path}/* {drive_path}/ 2>/dev/null || true\n",
    "            !rm -rf {local_path}\n",
    "        elif os.path.islink(local_path):\n",
    "            os.unlink(local_path)\n",
    "\n",
    "        os.symlink(drive_path, local_path)\n",
    "        print(f\"  {dir_name} -> Drive (mounted)\")\n",
    "\n",
    "elif DRIVE_MODE == \"service_account\":\n",
    "    print(\"Setting up local storage + Drive API restore...\")\n",
    "    for subdir in DRIVE_SUBDIRS:\n",
    "        os.makedirs(os.path.join(PROJECT_ROOT, subdir), exist_ok=True)\n",
    "        drive_helper.ensure_dir(subdir)\n",
    "\n",
    "    for dir_name in [\"checkpoints\", \"data\", \"logs\"]:\n",
    "        local_path = os.path.join(PROJECT_ROOT, dir_name)\n",
    "        if os.path.islink(local_path):\n",
    "            os.unlink(local_path)\n",
    "            os.makedirs(local_path, exist_ok=True)\n",
    "        print(f\"  {dir_name} -> local (backed up via Drive API)\")\n",
    "\n",
    "    print(\"\\nRestoring existing data from Drive...\")\n",
    "    for subdir in DRIVE_SUBDIRS:\n",
    "        local_target = os.path.join(PROJECT_ROOT, subdir)\n",
    "        drive_helper.restore(subdir, local_target)\n",
    "    print(\"Restore complete.\")\n",
    "\n",
    "else:\n",
    "    for d in [\"checkpoints\", \"data/rust\", \"logs\"]:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "    print(\"Local directories created (no Drive backup).\")\n",
    "\n",
    "print(\"\\nStorage ready!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up local storage + Drive API restore...\n",
      "  checkpoints -> local (backed up via Drive API)\n",
      "  data -> local (backed up via Drive API)\n",
      "  logs -> local (backed up via Drive API)\n",
      "\n",
      "Restoring existing data from Drive...\n",
      "Restore complete.\n",
      "\n",
      "Storage ready!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4c5d6e7f8",
   "metadata": {},
   "source": [
    "### 0.5 Check GPU & Configure MoE Backend"
   ]
  },
  {
   "cell_type": "code",
   "id": "f2a3b4c5d6e7f8a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T05:11:46.736827Z",
     "start_time": "2026-02-14T05:11:46.418327Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    capability = torch.cuda.get_device_capability()\n",
    "    is_h100 = \"H100\" in gpu_name or \"H200\" in gpu_name or \"B200\" in gpu_name\n",
    "\n",
    "    CONFIG[\"use_fp8\"] = capability[0] >= 9 and is_h100\n",
    "\n",
    "    if is_h100:\n",
    "        detected_tier = \"h100_80gb\"\n",
    "    elif gpu_memory >= 70:\n",
    "        detected_tier = \"a100_80gb\"\n",
    "    else:\n",
    "        detected_tier = \"a100_40gb\"\n",
    "\n",
    "    if detected_tier != CONFIG[\"gpu_tier\"]:\n",
    "        print(f\"NOTE: Auto-detected {detected_tier}, overriding configured {CONFIG['gpu_tier']}\")\n",
    "        CONFIG[\"gpu_tier\"] = detected_tier\n",
    "        gpu_cfg = GPU_CONFIGS[detected_tier]\n",
    "        CONFIG[\"moe_backend\"] = gpu_cfg[\"moe_backend\"]\n",
    "        CONFIG[\"load_mode\"] = gpu_cfg[\"load_mode\"]\n",
    "        CONFIG[\"fast_inference\"] = gpu_cfg[\"fast_inference\"]\n",
    "\n",
    "    os.environ[\"UNSLOTH_MOE_BACKEND\"] = CONFIG[\"moe_backend\"]\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"GPU: {gpu_name} ({gpu_memory:.0f} GB)\")\n",
    "    print(f\"Compute capability: {capability[0]}.{capability[1]}\")\n",
    "    print(f\"Tier: {CONFIG['gpu_tier']}\")\n",
    "    print(f\"\\nSplit LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(f\"Load mode: {CONFIG['load_mode']}\")\n",
    "    print(f\"FP8 available: {CONFIG['use_fp8']}\")\n",
    "    print(f\"Fast inference (vLLM): {CONFIG['fast_inference']}\")\n",
    "\n",
    "    if gpu_memory < 40:\n",
    "        print(\"\\nWARNING: <40 GB VRAM. Long-context training (16K+) may OOM.\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"No GPU detected!\")\n",
    "    CONFIG[\"use_fp8\"] = False\n",
    "    os.environ[\"UNSLOTH_MOE_BACKEND\"] = \"native_torch\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GPU: NVIDIA H100 80GB HBM3 (79 GB)\n",
      "Compute capability: 9.0\n",
      "Tier: h100_80gb\n",
      "\n",
      "Split LoRA backend: grouped_mm\n",
      "Load mode: fp8\n",
      "FP8 available: True\n",
      "Fast inference (vLLM): True\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6e7f8a9b0",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Data Preparation\n",
    "\n",
    "Downloads Strandset-Rust-v1 from HuggingFace, parses the 15 task categories,\n",
    "and formats everything in Harmony for each training stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5d6e7f8a9b0c1",
   "metadata": {},
   "source": [
    "### 1.1 Download & Format Strandset"
   ]
  },
  {
   "cell_type": "code",
   "id": "c5d6e7f8a9b0c1d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T05:11:57.845119Z",
     "start_time": "2026-02-14T05:11:46.740519Z"
    }
   },
   "source": [
    "max_samples = 500 if CONFIG[\"training_scope\"] == \"quick_test\" else 0  # 0 = all\n",
    "\n",
    "print(\"Downloading & formatting Strandset-Rust-v1...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cmd = \"python scripts/20_prepare_strandset.py\"\n",
    "if max_samples:\n",
    "    cmd += f\" --max_samples {max_samples}\"\n",
    "if not CONFIG[\"include_ipo\"]:\n",
    "    cmd += \" --no-preferences\"\n",
    "\n",
    "!{cmd}\n",
    "\n",
    "drive_helper.backup(\"data/rust/strandset\", \"data/rust/strandset\")\n",
    "if DRIVE_MODE != \"local\":\n",
    "    print(\"\\nBacked up Strandset data to Drive.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading & formatting Strandset-Rust-v1...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Preparing Strandset-Rust-v1\n",
      "============================================================\n",
      "  Dataset: Fortytwo-Network/Strandset-Rust-v1\n",
      "  Output: data/rust/strandset\n",
      "  Max samples: 500\n",
      "\n",
      "Loading dataset...\n",
      "README.md: 6.93kB [00:00, 31.6MB/s]\n",
      "data/train-00000-of-00001.parquet: 100% 109M/109M [00:02<00:00, 50.0MB/s] \n",
      "data/test-00000-of-00001.parquet: 100% 359k/359k [00:00<00:00, 1.12MB/s]\n",
      "Generating train split: 100% 191008/191008 [00:00<00:00, 580229.59 examples/s]\n",
      "Generating test split: 100% 225/225 [00:00<00:00, 69091.32 examples/s]\n",
      "  Train split: 191,008 examples\n",
      "  Test split: 225 examples\n",
      "\n",
      "Processing 500 examples...\n",
      "\n",
      "Saving datasets...\n",
      "\n",
      "============================================================\n",
      "Strandset preparation complete!\n",
      "============================================================\n",
      "  Total processed: 500\n",
      "  Lang adapter:    0\n",
      "  Core agent:      0 (debug=0, review=0)\n",
      "  IPO pairs:       0\n",
      "  Skipped (empty): 500\n",
      "  Skipped (parse): 0\n",
      "\n",
      "  Category breakdown:\n",
      "    code_generation: 92\n",
      "    docstring_generation: 70\n",
      "    comment_generation: 65\n",
      "    code_summarization: 60\n",
      "    code_explanation: 48\n",
      "    variable_naming: 47\n",
      "    function_naming: 46\n",
      "    code_refactoring: 41\n",
      "    code_review: 17\n",
      "    code_optimization: 11\n",
      "    bug_detection: 3\n",
      "\n",
      "  Stats: data/rust/strandset/stats.json\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"storageQuotaExceeded\"\n"
     ]
    },
    {
     "ename": "HttpError",
     "evalue": "<HttpError 403 when requesting https://www.googleapis.com/upload/drive/v3/files?fields=id&supportsAllDrives=true&alt=json&uploadType=resumable returned \"Service Accounts do not have storage quota. Leverage shared drives (https://developers.google.com/workspace/drive/api/guides/about-shareddrives), or use OAuth delegation (http://support.google.com/a/answer/7281227) instead.\". Details: \"[{'message': 'Service Accounts do not have storage quota. Leverage shared drives (https://developers.google.com/workspace/drive/api/guides/about-shareddrives), or use OAuth delegation (http://support.google.com/a/answer/7281227) instead.', 'domain': 'usageLimits', 'reason': 'storageQuotaExceeded'}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3935071435.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{cmd}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdrive_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/rust/strandset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data/rust/strandset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mDRIVE_MODE\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"local\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nBacked up Strandset data to Drive.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/llm-training-pipeline/scripts/pipeline_lib/drive_utils.py\u001b[0m in \u001b[0;36mbackup\u001b[0;34m(self, local_path, drive_relative_path)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0m_copy_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api_upload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrive_relative_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrive_relative_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/llm-training-pipeline/scripts/pipeline_lib/drive_utils.py\u001b[0m in \u001b[0;36m_api_upload\u001b[0;34m(self, local_path, relative_path)\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0mrel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelative_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelative_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api_upload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_api_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/llm-training-pipeline/scripts/pipeline_lib/drive_utils.py\u001b[0m in \u001b[0;36m_api_upload\u001b[0;34m(self, local_path, relative_path)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 self._service.files().create(\n\u001b[1;32m    191\u001b[0m                     \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmedia_body\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmedia\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupportsAllDrives\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                 ).execute()\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;31m# Directory — walk and upload each file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mbody\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_retries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_retries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mnext_chunk\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m   1091\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_process_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36m_process_response\u001b[0;34m(self, resp, content)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_error_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m         return (\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 403 when requesting https://www.googleapis.com/upload/drive/v3/files?fields=id&supportsAllDrives=true&alt=json&uploadType=resumable returned \"Service Accounts do not have storage quota. Leverage shared drives (https://developers.google.com/workspace/drive/api/guides/about-shareddrives), or use OAuth delegation (http://support.google.com/a/answer/7281227) instead.\". Details: \"[{'message': 'Service Accounts do not have storage quota. Leverage shared drives (https://developers.google.com/workspace/drive/api/guides/about-shareddrives), or use OAuth delegation (http://support.google.com/a/answer/7281227) instead.', 'domain': 'usageLimits', 'reason': 'storageQuotaExceeded'}]\">"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f8a9b0c1d2e3",
   "metadata": {},
   "source": [
    "### 1.2 Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "e7f8a9b0c1d2e3f4",
   "metadata": {},
   "source": [
    "data_checks = [\n",
    "    (\"Strandset lang_rust\", \"data/rust/strandset/lang_rust/train\"),\n",
    "    (\"Strandset core_agent\", \"data/rust/strandset/core_agent/train\"),\n",
    "    (\"Strandset IPO\", \"data/rust/strandset/ipo/train\"),\n",
    "    (\"Strandset eval\", \"data/rust/strandset/eval/test\"),\n",
    "    (\"Stats\", \"data/rust/strandset/stats.json\"),\n",
    "]\n",
    "\n",
    "print(\"Data Verification:\")\n",
    "print(\"=\" * 60)\n",
    "for name, path in data_checks:\n",
    "    exists = os.path.exists(path)\n",
    "    if exists and os.path.isdir(path):\n",
    "        items = os.listdir(path)\n",
    "        print(f\"  \\u2713 {name}: {path} ({len(items)} items)\")\n",
    "    elif exists:\n",
    "        size_kb = os.path.getsize(path) / 1024\n",
    "        print(f\"  \\u2713 {name}: {path} ({size_kb:.1f} KB)\")\n",
    "    else:\n",
    "        needed = True\n",
    "        if not CONFIG[\"include_ipo\"] and \"IPO\" in name:\n",
    "            needed = False\n",
    "        if CONFIG[\"training_scope\"] == \"lang_adapter_only\" and name in (\"Strandset core_agent\", \"Strandset IPO\"):\n",
    "            needed = False\n",
    "        sym = \"\\u2717\" if needed else \"\\u2014\"\n",
    "        label = \"MISSING\" if needed else \"not needed\"\n",
    "        print(f\"  {sym} {name}: {label}\")\n",
    "\n",
    "# Show stats if available\n",
    "stats_path = \"data/rust/strandset/stats.json\"\n",
    "if os.path.exists(stats_path):\n",
    "    import json\n",
    "    with open(stats_path) as f:\n",
    "        stats = json.load(f)\n",
    "    print(f\"\\n  Total processed: {stats.get('total_processed', '?'):,}\")\n",
    "    print(f\"  Lang adapter: {stats.get('lang_rust', '?'):,}\")\n",
    "    print(f\"  Core agent: {stats.get('core_agent_debug', 0) + stats.get('core_agent_review', 0):,}\")\n",
    "    print(f\"  IPO pairs: {stats.get('ipo', '?'):,}\")\n",
    "print(\"=\" * 60)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f8a9b0c1d2e3f4a5",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Lang Adapter Training\n",
    "\n",
    "Train a QLoRA adapter (rank 64) to specialise GPT-OSS 20B on Rust syntax, stdlib, and idioms.\n",
    "Uses Strandset's code_generation, code_completion, docstring, comment, and naming examples.\n",
    "Then merge the adapter into the base weights for downstream training.\n",
    "\n",
    "**Split LoRA** backend auto-enabled for 7-12x faster MoE training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0c1d2e3f4a5b6",
   "metadata": {},
   "source": [
    "### 2.1 Train lang_rust Adapter"
   ]
  },
  {
   "cell_type": "code",
   "id": "b0c1d2e3f4a5b6c7",
   "metadata": {},
   "source": [
    "batch = CONFIG[\"lang_rust_batch\"]\n",
    "grad_accum = CONFIG[\"lang_rust_grad_accum\"]\n",
    "max_steps = CONFIG[\"lang_rust_max_steps\"]\n",
    "seq_len = CONFIG[\"lang_rust_seq_len\"]\n",
    "\n",
    "cmd = f\"python scripts/13_train_lang_adapter.py\"\n",
    "cmd += f\" --train_data_path data/rust/strandset/lang_rust/train\"\n",
    "cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "print(f\"Training lang_rust adapter...\")\n",
    "print(f\"  Data: data/rust/strandset/lang_rust/train\")\n",
    "print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "print(f\"  Max steps: {max_steps}\")\n",
    "print(f\"  Seq length: {seq_len} (from config)\")\n",
    "print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "!{cmd}\n",
    "\n",
    "drive_helper.backup(\"checkpoints/lang_rust\", \"checkpoints/lang_rust\")\n",
    "if DRIVE_MODE != \"local\":\n",
    "    print(\"\\nCheckpoint backed up to Drive.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4a5b6c7d8",
   "metadata": {},
   "source": [
    "### 2.2 Merge lang_rust into Base"
   ]
  },
  {
   "cell_type": "code",
   "id": "d2e3f4a5b6c7d8e9",
   "metadata": {},
   "source": [
    "print(\"Merging lang_rust adapter into base model...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "!python scripts/19_merge_adapter.py \\\n",
    "    --adapter_path checkpoints/lang_rust/final \\\n",
    "    --output_dir checkpoints/gpt-oss-20b-rust-merged \\\n",
    "    --export_formats hf\n",
    "\n",
    "drive_helper.backup(\"checkpoints/gpt-oss-20b-rust-merged\", \"checkpoints/gpt-oss-20b-rust-merged\")\n",
    "if DRIVE_MODE != \"local\":\n",
    "    print(\"\\nMerged model backed up to Drive.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e3f4a5b6c7d8e9f0",
   "metadata": {},
   "source": [
    "### 2.3 Verify Merge"
   ]
  },
  {
   "cell_type": "code",
   "id": "f4a5b6c7d8e9f0a1",
   "metadata": {},
   "source": [
    "merged_path = \"checkpoints/gpt-oss-20b-rust-merged\"\n",
    "adapter_path = \"checkpoints/lang_rust/final\"\n",
    "\n",
    "print(\"Merge Verification:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if os.path.exists(merged_path):\n",
    "    files = os.listdir(merged_path)\n",
    "    safetensors = [f for f in files if f.endswith(\".safetensors\")]\n",
    "    print(f\"  \\u2713 Merged model: {merged_path}\")\n",
    "    print(f\"    {len(safetensors)} safetensors shard(s), {len(files)} total files\")\n",
    "else:\n",
    "    print(f\"  \\u2717 Merged model not found at {merged_path}\")\n",
    "\n",
    "if os.path.exists(adapter_path):\n",
    "    adapter_files = os.listdir(adapter_path)\n",
    "    print(f\"  \\u2713 Adapter: {adapter_path} ({len(adapter_files)} files)\")\n",
    "else:\n",
    "    print(f\"  \\u2717 Adapter not found at {adapter_path}\")\n",
    "\n",
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"\\n\\u2713 lang_adapter_only scope complete. Stopping here.\")\n",
    "\n",
    "print(\"=\" * 60)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a5b6c7d8e9f0a1b2",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Core Agent SFT\n",
    "\n",
    "Train a higher-rank LoRA adapter (rank 128) on Strandset's debug/review examples.\n",
    "Uses the merged lang_rust model as the base.\n",
    "\n",
    "**Split LoRA** + **Auto packing** (3x faster, zero-config)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c7d8e9f0a1b2c3",
   "metadata": {},
   "source": [
    "### 3.1 Train core_agent Adapter"
   ]
  },
  {
   "cell_type": "code",
   "id": "c7d8e9f0a1b2c3d4",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    batch = CONFIG[\"core_agent_batch\"]\n",
    "    grad_accum = CONFIG[\"core_agent_grad_accum\"]\n",
    "    max_steps = CONFIG[\"core_agent_max_steps\"]\n",
    "    seq_len = CONFIG[\"core_agent_seq_len\"]\n",
    "\n",
    "    cmd = f\"python scripts/14_train_core_agent.py\"\n",
    "    cmd += f\" --train_data_path data/rust/strandset/core_agent/train\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    print(f\"Training core_agent adapter...\")\n",
    "    print(f\"  Data: data/rust/strandset/core_agent/train\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Seq length: {seq_len} (from config)\")\n",
    "    print(f\"  LoRA rank: 128\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(f\"  Auto packing: enabled (uncontaminated)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/core_agent\", \"checkpoints/core_agent\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d8e9f0a1b2c3d4e5",
   "metadata": {},
   "source": [
    "### 3.2 Verify core_agent"
   ]
  },
  {
   "cell_type": "code",
   "id": "e9f0a1b2c3d4e5f6",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    ckpt_path = \"checkpoints/core_agent/final\"\n",
    "\n",
    "    print(\"Core Agent Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(ckpt_path):\n",
    "        files = os.listdir(ckpt_path)\n",
    "        print(f\"  \\u2713 Checkpoint: {ckpt_path} ({len(files)} files)\")\n",
    "\n",
    "        adapter_config = os.path.join(ckpt_path, \"adapter_config.json\")\n",
    "        if os.path.exists(adapter_config):\n",
    "            import json\n",
    "            with open(adapter_config) as f:\n",
    "                cfg = json.load(f)\n",
    "            print(f\"    LoRA rank: {cfg.get('r', '?')}\")\n",
    "            print(f\"    Alpha: {cfg.get('lora_alpha', '?')}\")\n",
    "            print(f\"    Target modules: {cfg.get('target_modules', '?')}\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 Checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f0a1b2c3d4e5f6a7",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: IPO Preference Training (Optional)\n",
    "\n",
    "Train with Identity Preference Optimisation on synthetic preference pairs\n",
    "from Strandset's bug_detection category (fixed=chosen, buggy=rejected).\n",
    "\n",
    "Very low learning rate (5e-7), 1 epoch only to avoid collapse.\n",
    "\n",
    "Set `include_ipo=False` in Step 0.3 to skip."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4e5f6a7b9",
   "metadata": {},
   "source": [
    "### 4.1 Train with IPO"
   ]
  },
  {
   "cell_type": "code",
   "id": "b2c3d4e5f6a7b9c0",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "elif not CONFIG[\"include_ipo\"]:\n",
    "    print(\"Skipping \\u2014 IPO disabled (include_ipo=False)\")\n",
    "else:\n",
    "    batch = CONFIG[\"ipo_batch\"]\n",
    "    grad_accum = CONFIG[\"ipo_grad_accum\"]\n",
    "    max_steps = CONFIG[\"ipo_max_steps\"]\n",
    "\n",
    "    ipo_checkpoint = \"checkpoints/core_agent/final\"\n",
    "\n",
    "    cmd = f\"python scripts/17_ipo_preference.py\"\n",
    "    cmd += f\" --checkpoint {ipo_checkpoint}\"\n",
    "    cmd += f\" --train_data_path data/rust/strandset/ipo/train\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    print(f\"Training with IPO (synthetic preferences)...\")\n",
    "    print(f\"  Checkpoint: {ipo_checkpoint}\")\n",
    "    print(f\"  Data: data/rust/strandset/ipo/train\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Loss: IPO (beta=0.1)\")\n",
    "    print(f\"  Load mode: {CONFIG['load_mode']}\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/core_agent_ipo\", \"checkpoints/core_agent_ipo\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6a7b9c0d1",
   "metadata": {},
   "source": [
    "### 4.2 Verify IPO"
   ]
  },
  {
   "cell_type": "code",
   "id": "d4e5f6a7b9c0d1e2",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "elif not CONFIG[\"include_ipo\"]:\n",
    "    print(\"Skipping \\u2014 IPO disabled\")\n",
    "else:\n",
    "    ckpt_path = \"checkpoints/core_agent_ipo/final\"\n",
    "\n",
    "    print(\"IPO Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(ckpt_path):\n",
    "        files = os.listdir(ckpt_path)\n",
    "        print(f\"  \\u2713 IPO checkpoint: {ckpt_path} ({len(files)} files)\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 IPO checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "    tb_dir = \"checkpoints/core_agent_ipo\"\n",
    "    tb_files = []\n",
    "    if os.path.exists(tb_dir):\n",
    "        for root, dirs, fnames in os.walk(tb_dir):\n",
    "            for fn in fnames:\n",
    "                if fn.startswith(\"events.out.tfevents\"):\n",
    "                    tb_files.append(os.path.join(root, fn))\n",
    "    if tb_files:\n",
    "        print(f\"  \\u2713 TensorBoard logs found ({len(tb_files)} event files)\")\n",
    "        print(f\"    Monitor KL divergence: warn >0.3, abort >0.5\")\n",
    "    else:\n",
    "        print(f\"  \\u2014 No TensorBoard logs found\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b9c0d1e2f3",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Test Model\n",
    "\n",
    "Load the trained model and generate Rust code interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b9c0d1e2f3a4",
   "metadata": {},
   "source": [
    "### 5.1 Load Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "a7b9c0d1e2f3a4b5",
   "metadata": {},
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "CHECKPOINT_PRIORITY = [\n",
    "    \"checkpoints/core_agent_ipo/final\",\n",
    "    \"checkpoints/core_agent/final\",\n",
    "    \"checkpoints/gpt-oss-20b-rust-merged\",\n",
    "]\n",
    "\n",
    "MODEL_PATH = None\n",
    "for path in CHECKPOINT_PRIORITY:\n",
    "    if os.path.exists(path):\n",
    "        MODEL_PATH = path\n",
    "        break\n",
    "\n",
    "if MODEL_PATH is None:\n",
    "    print(\"\\u2717 No checkpoint found. Train the model first.\")\n",
    "else:\n",
    "    print(f\"Loading model from: {MODEL_PATH}\")\n",
    "\n",
    "    load_kwargs = {\n",
    "        \"max_seq_length\": 4096,\n",
    "        \"dtype\": torch.bfloat16,\n",
    "    }\n",
    "    if CONFIG.get(\"load_mode\") == \"fp8\" and CONFIG.get(\"use_fp8\"):\n",
    "        load_kwargs[\"load_in_fp8\"] = True\n",
    "        print(\"  Mode: FP8 (H100)\")\n",
    "    else:\n",
    "        load_kwargs[\"load_in_4bit\"] = True\n",
    "        print(\"  Mode: 4-bit QLoRA\")\n",
    "\n",
    "    if CONFIG.get(\"fast_inference\"):\n",
    "        load_kwargs[\"fast_inference\"] = True\n",
    "        print(\"  Inference: vLLM backend\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(MODEL_PATH, **load_kwargs)\n",
    "    FastLanguageModel.for_inference(model)\n",
    "\n",
    "    print(\"\\u2713 Model loaded!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b9c0d1e2f3a4b5c6",
   "metadata": {},
   "source": [
    "### 5.2 Generate Rust Code"
   ]
  },
  {
   "cell_type": "code",
   "id": "c0d1e2f3a4b5c6d7",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"scripts\")\n",
    "from dataset_formatters.harmony import encode_harmony_messages\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    \"Write a Rust function `fn merge_sorted(a: &[i32], b: &[i32]) -> Vec<i32>` that merges two sorted slices into a single sorted vector.\",\n",
    "    \"This Rust code fails the borrow checker. Fix it:\\n```rust\\nfn main() {\\n    let mut v = vec![1, 2, 3];\\n    let first = &v[0];\\n    v.push(4);\\n    println!(\\\"{}\\\", first);\\n}\\n```\",\n",
    "    \"Write an async Rust function using tokio that fetches a URL with reqwest, retries up to 3 times on failure, and returns the response body as a String.\",\n",
    "]\n",
    "\n",
    "def generate_rust(prompt, max_tokens=1024):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted = encode_harmony_messages(\n",
    "        messages,\n",
    "        developer_instructions=\"You are a Rust programming expert. Write correct, idiomatic code.\",\n",
    "    )\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.3,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "for i, prompt in enumerate(TEST_PROMPTS, 1):\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Test {i}: {prompt[:80]}...\")\n",
    "    print(\"=\" * 60)\n",
    "    response = generate_rust(prompt)\n",
    "    print(response)\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d1e2f3a4b5c6d7e8",
   "metadata": {},
   "source": [
    "### 5.3 Custom Prompt"
   ]
  },
  {
   "cell_type": "code",
   "id": "e2f3a4b5c6d7e8f9",
   "metadata": {},
   "source": [
    "CUSTOM_PROMPT = \"Write a Rust function that reads a CSV file and returns the sum of a specified column.\"\n",
    "\n",
    "print(f\"Prompt: {CUSTOM_PROMPT}\")\n",
    "print(\"=\" * 60)\n",
    "print(generate_rust(CUSTOM_PROMPT))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f3a4b5c6d7e8f9a0",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Export\n",
    "\n",
    "Merge the final adapter and export to HuggingFace + GGUF formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5c6d7e8f9a0b1",
   "metadata": {},
   "source": [
    "### 6.1 Export to GGUF"
   ]
  },
  {
   "cell_type": "code",
   "id": "b5c6d7e8f9a0b1c2",
   "metadata": {},
   "source": [
    "ADAPTER_PRIORITY = [\n",
    "    \"checkpoints/core_agent_ipo/final\",\n",
    "    \"checkpoints/core_agent/final\",\n",
    "    \"checkpoints/lang_rust/final\",\n",
    "]\n",
    "\n",
    "adapter_path = None\n",
    "for path in ADAPTER_PRIORITY:\n",
    "    if os.path.exists(path):\n",
    "        adapter_path = path\n",
    "        break\n",
    "\n",
    "if adapter_path is None:\n",
    "    print(\"\\u2717 No adapter checkpoint found.\")\n",
    "else:\n",
    "    export_dir = \"checkpoints/gpt-oss-20b-rust-export-v3\"\n",
    "    print(f\"Exporting adapter: {adapter_path}\")\n",
    "    print(f\"Output: {export_dir}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/19_merge_adapter.py \\\n",
    "        --adapter_path {adapter_path} \\\n",
    "        --output_dir {export_dir} \\\n",
    "        --export_formats hf gguf_q4\n",
    "\n",
    "    drive_helper.backup(export_dir, \"checkpoints/gpt-oss-20b-rust-export-v3\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nExport backed up to Drive.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c6d7e8f9a0b1c2d3",
   "metadata": {},
   "source": [
    "### 6.2 Download GGUF"
   ]
  },
  {
   "cell_type": "code",
   "id": "d7e8f9a0b1c2d3e4",
   "metadata": {},
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    import glob\n",
    "\n",
    "    export_dir = \"checkpoints/gpt-oss-20b-rust-export-v3\"\n",
    "    gguf_files = glob.glob(os.path.join(export_dir, \"*.gguf\"))\n",
    "\n",
    "    if gguf_files:\n",
    "        gguf_path = gguf_files[0]\n",
    "        size_gb = os.path.getsize(gguf_path) / (1024**3)\n",
    "        print(f\"Downloading: {os.path.basename(gguf_path)} ({size_gb:.1f} GB)\")\n",
    "        files.download(gguf_path)\n",
    "    else:\n",
    "        print(\"\\u2717 No GGUF file found. Run export (6.1) first.\")\n",
    "else:\n",
    "    print(\"Download not available outside Colab.\")\n",
    "    print(\"GGUF file is at: checkpoints/gpt-oss-20b-rust-export-v3/\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e8f9a0b1c2d3e4f5",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Complete!\n",
    "\n",
    "Your GPT-OSS 20B Rust coding agent (v3 \\u2014 Strandset) is trained and ready to use.\n",
    "\n",
    "**Data source:** [Strandset-Rust-v1](https://huggingface.co/datasets/Fortytwo-Network/Strandset-Rust-v1) (191K examples, Apache 2.0)\n",
    "\n",
    "**Pipeline:**\n",
    "1. Lang Adapter: Rust domain specialisation from code generation/completion examples\n",
    "2. Core Agent SFT: Debug and review training from bug_detection/code_review examples\n",
    "3. IPO: Synthetic preference pairs from bug_detection (if enabled)\n",
    "\n",
    "**Outputs:**\n",
    "- Checkpoints: `checkpoints/core_agent_{ipo}/final`\n",
    "- Exported model: `checkpoints/gpt-oss-20b-rust-export-v3/`\n",
    "- All backed up to Google Drive: `gpt-oss-20b-rust-agent-v3/`\n",
    "\n",
    "**Compared to v2:**\n",
    "- No Rust toolchain required \\u2014 runs on any Colab GPU instance\n",
    "- No cargo-mutants or trajectory generation \\u2014 faster setup\n",
    "- No GRPO RL \\u2014 no execution-based rewards\n",
    "- For better results, consider upgrading to v2 with mutation data + GRPO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}