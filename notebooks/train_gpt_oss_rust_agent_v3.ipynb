{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4e5f6a7b8",
   "metadata": {},
   "source": [
    "# Train GPT-OSS 20B \u2192 Rust Coding Agent (v3 \u2014 Strandset)\n",
    "\n",
    "Simplified pipeline using [Strandset-Rust-v1](https://huggingface.co/datasets/Fortytwo-Network/Strandset-Rust-v1) (191K verified Rust examples, Apache 2.0) as the sole data source.\n",
    "\n",
    "**Key differences from v2:**\n",
    "- **No Rust toolchain** \u2014 no `rustup`, `cargo-mutants`, or compilation needed\n",
    "- **No mutation/trajectory generation** \u2014 data comes entirely from Strandset\n",
    "- **No GRPO** \u2014 no execution-based rewards without cargo\n",
    "- **IPO from synthetic preferences** \u2014 bug_detection pairs (fixed=chosen, buggy=rejected)\n",
    "\n",
    "**3-Phase Pipeline:**\n",
    "1. **Lang Adapter** \u2014 Rust domain specialisation via QLoRA (script 13 + 19)\n",
    "2. **Core Agent SFT** \u2014 Debug/review training from Strandset (script 14)\n",
    "3. **IPO Preference** \u2014 Synthetic preference pairs from bug_detection (script 17)\n",
    "\n",
    "**Requirements:**\n",
    "- **GPU**: A100 40GB+ (H100 80GB recommended for FP8)\n",
    "- **Storage**: Google Drive for persistent checkpoints\n",
    "- **No Rust toolchain required**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5f6a7b8c9",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6a7b8c9d0",
   "metadata": {},
   "source": [
    "### 0.1 Mount Google Drive & Clone Repository\n",
    "\n",
    "**PyCharm / headless users:** If `drive.mount()` doesn't work, set `use_service_account = True`\n",
    "and provide your service-account JSON key in Step 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7b8c9d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "use_service_account = True\n",
    "\n",
    "DRIVE_MOUNTED = False\n",
    "\n",
    "if IN_COLAB and not use_service_account:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        DRIVE_MOUNTED = True\n",
    "        print(\"Google Drive mounted\")\n",
    "    except Exception as e:\n",
    "        print(f\"drive.mount() failed: {e}\")\n",
    "        print(\"Falling back to local-only mode.\")\n",
    "        print(\"Tip: set use_service_account=True and provide a JSON key in Step 0.3.\")\n",
    "elif IN_COLAB and use_service_account:\n",
    "    print(\"Service-account mode selected \\u2014 skipping drive.mount()\")\n",
    "    print(\"Configure credentials in Step 0.3.\")\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "\n",
    "REPO_URL = \"https://github.com/rmarnold/llm-training-pipeline.git\"\n",
    "BRANCH = \"main\"\n",
    "\n",
    "REPO_DIR = \"/content/llm-training-pipeline\"\n",
    "\n",
    "if IN_COLAB:\n",
    "    if os.path.exists(REPO_DIR):\n",
    "        %cd {REPO_DIR}\n",
    "        !git pull origin {BRANCH}\n",
    "    else:\n",
    "        !git clone -b {BRANCH} {REPO_URL} {REPO_DIR}\n",
    "        %cd {REPO_DIR}\n",
    "\n",
    "    PROJECT_ROOT = REPO_DIR\n",
    "else:\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"\\nProject root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8c9d0e1f2",
   "metadata": {},
   "source": [
    "### 0.2 Install Dependencies\n",
    "\n",
    "Installs pipeline deps and latest Unsloth. **No Rust toolchain needed** \u2014 all training data\n",
    "comes from Strandset.\n",
    "\n",
    "**Note:** flash-attn is intentionally NOT installed. FA3 is incompatible with GPT-OSS\n",
    "backward passes. Unsloth's Flex Attention replaces it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8c9d0e1f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    print(\"Installing Python dependencies...\")\n",
    "    print(\"=\" * 60)\n",
    "    !pip install -q -e \".[gpt_oss,colab]\"\n",
    "\n",
    "    # Fix pyarrow binary incompatibility with datasets 4.x on Colab\n",
    "    !pip install -q --force-reinstall pyarrow\n",
    "\n",
    "    # Force latest Unsloth with Split LoRA + FP8 RL\n",
    "    print(\"\\nInstalling latest Unsloth (Split LoRA + Flex Attention)...\")\n",
    "    !pip install -q --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo\n",
    "    !pip install -q \"unsloth[colab-new]\"\n",
    "\n",
    "    # vLLM for FP8 inference (H100 only, optional)\n",
    "    !pip install -q vllm>=0.12.0 2>/dev/null || true\n",
    "\n",
    "    # Verification\n",
    "    from importlib.metadata import version, PackageNotFoundError\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Dependency Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for pkg in [\"unsloth\", \"trl\", \"peft\", \"datasets\", \"tiktoken\", \"vllm\"]:\n",
    "        try:\n",
    "            ver = version(pkg)\n",
    "            print(f\"\\u2713 {pkg}: {ver}\")\n",
    "        except PackageNotFoundError:\n",
    "            if pkg == \"vllm\":\n",
    "                print(f\"\\u2014 {pkg}: not installed (optional, H100 FP8 only)\")\n",
    "            else:\n",
    "                print(f\"\\u2717 {pkg}: not installed\")\n",
    "\n",
    "    print(\"\\nNote: No Rust toolchain needed for v3 (Strandset-only pipeline)\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"Running locally \\u2014 ensure deps are installed:\")\n",
    "    print(\"  pip install -e '.[gpt_oss]'\")\n",
    "    print(\"  pip install --upgrade unsloth unsloth_zoo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0e1f2a3b4",
   "metadata": {},
   "source": [
    "### 0.3 Configure Pipeline\n",
    "\n",
    "**Training Scope** (`training_scope`):\n",
    "- `full` \\u2014 All 3 phases (Lang Adapter + Core Agent + IPO)\n",
    "- `quick_test` \\u2014 Short runs (100 steps each) to verify setup\n",
    "- `lang_adapter_only` \\u2014 Only train lang_rust adapter + merge\n",
    "\n",
    "**Service Account Setup** (for Drive backup):\n",
    "1. Set `use_service_account = True` in cell 0.1\n",
    "2. Run cell 0.3 \\u2014 it will try Colab Secrets, then file, then paste prompt\n",
    "3. Set `DRIVE_FOLDER_ID` in Colab Secrets, or set `drive_folder_id` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e1f2a3b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "training_scope = \"quick_test\"  # \"full\", \"quick_test\", \"lang_adapter_only\"\n",
    "\n",
    "gpu_tier = \"h100_80gb\"  # \"a100_40gb\", \"a100_80gb\", \"h100_80gb\"\n",
    "\n",
    "max_steps_override = 0  # Set >0 to cap all stages (0 = use defaults)\n",
    "\n",
    "include_ipo = True  # False to skip IPO preference training\n",
    "\n",
    "enable_qat_export = False  # True for MXFP4 QAT export\n",
    "\n",
    "# ============================================================\n",
    "# SERVICE ACCOUNT CREDENTIALS\n",
    "# ============================================================\n",
    "drive_folder_id = \"\"  # Google Drive folder ID\n",
    "\n",
    "_SA_VM_PATH = \"/content/service_account.json\"\n",
    "service_account_key = \"\"\n",
    "\n",
    "if use_service_account and IN_COLAB:\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        _key_json = userdata.get(\"SERVICE_ACCOUNT_KEY\")\n",
    "        if _key_json:\n",
    "            with open(_SA_VM_PATH, \"w\") as _f:\n",
    "                _f.write(_key_json)\n",
    "            service_account_key = _SA_VM_PATH\n",
    "            print(\"Service account key loaded from Colab Secrets.\")\n",
    "        if not drive_folder_id:\n",
    "            drive_folder_id = userdata.get(\"DRIVE_FOLDER_ID\") or \"\"\n",
    "            if drive_folder_id:\n",
    "                print(f\"Drive folder ID loaded from Colab Secrets.\")\n",
    "    except Exception as _e:\n",
    "        print(f\"  Colab Secrets lookup failed: {type(_e).__name__}: {_e}\")\n",
    "\n",
    "    if not service_account_key and os.path.exists(_SA_VM_PATH):\n",
    "        service_account_key = _SA_VM_PATH\n",
    "        print(f\"Using key file on VM: {_SA_VM_PATH}\")\n",
    "\n",
    "    if not service_account_key:\n",
    "        try:\n",
    "            print(\"No service account key found.\")\n",
    "            _key_text = input(\"Paste service account JSON (entire content in one go): \")\n",
    "            _key_text = _key_text.strip()\n",
    "            if _key_text:\n",
    "                json.loads(_key_text)\n",
    "                with open(_SA_VM_PATH, \"w\") as _f:\n",
    "                    _f.write(_key_text)\n",
    "                service_account_key = _SA_VM_PATH\n",
    "                print(f\"Saved to {_SA_VM_PATH}\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"  Invalid JSON \\u2014 key not saved.\")\n",
    "        except EOFError:\n",
    "            pass\n",
    "\n",
    "    if not drive_folder_id and service_account_key:\n",
    "        _fid = input(\"Enter Google Drive folder ID (from URL): \").strip()\n",
    "        if _fid:\n",
    "            drive_folder_id = _fid\n",
    "\n",
    "    if not service_account_key:\n",
    "        print(\"No service account key \\u2014 Drive backup disabled.\")\n",
    "\n",
    "elif use_service_account:\n",
    "    for _path in [_SA_VM_PATH, \"service_account.json\"]:\n",
    "        if os.path.exists(_path):\n",
    "            service_account_key = _path\n",
    "            print(f\"Using key file: {_path}\")\n",
    "            break\n",
    "    if not service_account_key:\n",
    "        print(\"Running locally \\u2014 set service_account_key to your JSON key path.\")\n",
    "\n",
    "# ============================================================\n",
    "# DRIVE MODE\n",
    "# ============================================================\n",
    "from scripts.pipeline_lib.drive_utils import DriveHelper\n",
    "\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/gpt-oss-20b-rust-agent-v3\"\n",
    "\n",
    "if DRIVE_MOUNTED:\n",
    "    DRIVE_MODE = \"mounted\"\n",
    "elif use_service_account and service_account_key and drive_folder_id:\n",
    "    DRIVE_MODE = \"service_account\"\n",
    "else:\n",
    "    DRIVE_MODE = \"local\"\n",
    "\n",
    "drive_helper = DriveHelper(\n",
    "    mode=DRIVE_MODE,\n",
    "    drive_base=DRIVE_BASE,\n",
    "    credentials_path=service_account_key or None,\n",
    "    folder_id=drive_folder_id or None,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# GPU TIER CONFIGS\n",
    "# ============================================================\n",
    "\n",
    "GPU_CONFIGS = {\n",
    "    \"a100_40gb\": {\n",
    "        \"moe_backend\": \"unsloth_triton\",\n",
    "        \"load_mode\": \"4bit\",\n",
    "        \"fast_inference\": False,\n",
    "        \"lang_rust\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 8192, \"max_steps\": 3000},\n",
    "        \"core_agent\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 12288, \"max_steps\": 2000},\n",
    "        \"ipo\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 12288, \"max_steps\": 1000},\n",
    "    },\n",
    "    \"a100_80gb\": {\n",
    "        \"moe_backend\": \"unsloth_triton\",\n",
    "        \"load_mode\": \"4bit\",\n",
    "        \"fast_inference\": False,\n",
    "        \"lang_rust\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 8192, \"max_steps\": 5000},\n",
    "        \"core_agent\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 16384, \"max_steps\": 3000},\n",
    "        \"ipo\": {\"batch\": 1, \"grad_accum\": 16, \"seq_len\": 16384, \"max_steps\": 2000},\n",
    "    },\n",
    "    \"h100_80gb\": {\n",
    "        \"moe_backend\": \"grouped_mm\",\n",
    "        \"load_mode\": \"fp8\",\n",
    "        \"fast_inference\": True,\n",
    "        \"lang_rust\": {\"batch\": 2, \"grad_accum\": 4, \"seq_len\": 8192, \"max_steps\": 5000},\n",
    "        \"core_agent\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 16384, \"max_steps\": 3000},\n",
    "        \"ipo\": {\"batch\": 1, \"grad_accum\": 16, \"seq_len\": 16384, \"max_steps\": 2000},\n",
    "    },\n",
    "}\n",
    "\n",
    "if training_scope == \"quick_test\":\n",
    "    max_steps_override = 100\n",
    "\n",
    "gpu_cfg = GPU_CONFIGS[gpu_tier]\n",
    "\n",
    "CONFIG = {\n",
    "    \"training_scope\": training_scope,\n",
    "    \"gpu_tier\": gpu_tier,\n",
    "    \"include_ipo\": include_ipo,\n",
    "    \"enable_qat_export\": enable_qat_export,\n",
    "    \"moe_backend\": gpu_cfg[\"moe_backend\"],\n",
    "    \"load_mode\": gpu_cfg[\"load_mode\"],\n",
    "    \"fast_inference\": gpu_cfg[\"fast_inference\"],\n",
    "    # Lang adapter\n",
    "    \"lang_rust_batch\": gpu_cfg[\"lang_rust\"][\"batch\"],\n",
    "    \"lang_rust_grad_accum\": gpu_cfg[\"lang_rust\"][\"grad_accum\"],\n",
    "    \"lang_rust_seq_len\": gpu_cfg[\"lang_rust\"][\"seq_len\"],\n",
    "    \"lang_rust_max_steps\": max_steps_override or gpu_cfg[\"lang_rust\"][\"max_steps\"],\n",
    "    # Core agent\n",
    "    \"core_agent_batch\": gpu_cfg[\"core_agent\"][\"batch\"],\n",
    "    \"core_agent_grad_accum\": gpu_cfg[\"core_agent\"][\"grad_accum\"],\n",
    "    \"core_agent_seq_len\": gpu_cfg[\"core_agent\"][\"seq_len\"],\n",
    "    \"core_agent_max_steps\": max_steps_override or gpu_cfg[\"core_agent\"][\"max_steps\"],\n",
    "    # IPO\n",
    "    \"ipo_batch\": gpu_cfg[\"ipo\"][\"batch\"],\n",
    "    \"ipo_grad_accum\": gpu_cfg[\"ipo\"][\"grad_accum\"],\n",
    "    \"ipo_seq_len\": gpu_cfg[\"ipo\"][\"seq_len\"],\n",
    "    \"ipo_max_steps\": max_steps_override or gpu_cfg[\"ipo\"][\"max_steps\"],\n",
    "    # Eval\n",
    "    \"eval_num_samples\": 10 if training_scope == \"quick_test\" else 50,\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PIPELINE CONFIGURATION (v3 \\u2014 Strandset)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nScope: {training_scope.upper()}\")\n",
    "print(f\"GPU tier: {gpu_tier}\")\n",
    "print(f\"MoE backend: {CONFIG['moe_backend']}\")\n",
    "print(f\"Load mode: {CONFIG['load_mode']}\")\n",
    "print(f\"Fast inference (vLLM): {CONFIG['fast_inference']}\")\n",
    "print(f\"Include IPO: {include_ipo}\")\n",
    "print(f\"QAT export: {enable_qat_export}\")\n",
    "print(f\"Drive mode: {DRIVE_MODE}\")\n",
    "if max_steps_override:\n",
    "    print(f\"Max steps override: {max_steps_override}\")\n",
    "print(f\"\\nLang Adapter:  batch={CONFIG['lang_rust_batch']} x grad_accum={CONFIG['lang_rust_grad_accum']}, seq={CONFIG['lang_rust_seq_len']}, steps={CONFIG['lang_rust_max_steps']}\")\n",
    "print(f\"Core Agent:    batch={CONFIG['core_agent_batch']} x grad_accum={CONFIG['core_agent_grad_accum']}, seq={CONFIG['core_agent_seq_len']}, steps={CONFIG['core_agent_max_steps']}\")\n",
    "if include_ipo:\n",
    "    print(f\"IPO:           batch={CONFIG['ipo_batch']} x grad_accum={CONFIG['ipo_grad_accum']}, seq={CONFIG['ipo_seq_len']}, steps={CONFIG['ipo_max_steps']}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2a3b4c5d6",
   "metadata": {},
   "source": [
    "### 0.4 Set Up Persistent Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2a3b4c5d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVE_SUBDIRS = [\n",
    "    \"checkpoints/lang_rust\",\n",
    "    \"checkpoints/core_agent\",\n",
    "    \"checkpoints/core_agent_ipo\",\n",
    "    \"checkpoints/gpt-oss-20b-rust-merged\",\n",
    "    \"data/rust/strandset\",\n",
    "    \"logs\",\n",
    "]\n",
    "\n",
    "if DRIVE_MODE == \"mounted\":\n",
    "    print(f\"Setting up storage at: {DRIVE_BASE}\")\n",
    "    for subdir in DRIVE_SUBDIRS:\n",
    "        os.makedirs(os.path.join(DRIVE_BASE, subdir), exist_ok=True)\n",
    "\n",
    "    for dir_name in [\"checkpoints\", \"data\", \"logs\"]:\n",
    "        local_path = os.path.join(PROJECT_ROOT, dir_name)\n",
    "        drive_path = os.path.join(DRIVE_BASE, dir_name)\n",
    "\n",
    "        if os.path.exists(local_path) and not os.path.islink(local_path):\n",
    "            !cp -r {local_path}/* {drive_path}/ 2>/dev/null || true\n",
    "            !rm -rf {local_path}\n",
    "        elif os.path.islink(local_path):\n",
    "            os.unlink(local_path)\n",
    "\n",
    "        os.symlink(drive_path, local_path)\n",
    "        print(f\"  {dir_name} -> Drive (mounted)\")\n",
    "\n",
    "elif DRIVE_MODE == \"service_account\":\n",
    "    print(\"Setting up local storage + Drive API restore...\")\n",
    "    for subdir in DRIVE_SUBDIRS:\n",
    "        os.makedirs(os.path.join(PROJECT_ROOT, subdir), exist_ok=True)\n",
    "        drive_helper.ensure_dir(subdir)\n",
    "\n",
    "    for dir_name in [\"checkpoints\", \"data\", \"logs\"]:\n",
    "        local_path = os.path.join(PROJECT_ROOT, dir_name)\n",
    "        if os.path.islink(local_path):\n",
    "            os.unlink(local_path)\n",
    "            os.makedirs(local_path, exist_ok=True)\n",
    "        print(f\"  {dir_name} -> local (backed up via Drive API)\")\n",
    "\n",
    "    print(\"\\nRestoring existing data from Drive...\")\n",
    "    for subdir in DRIVE_SUBDIRS:\n",
    "        local_target = os.path.join(PROJECT_ROOT, subdir)\n",
    "        drive_helper.restore(subdir, local_target)\n",
    "    print(\"Restore complete.\")\n",
    "\n",
    "else:\n",
    "    for d in [\"checkpoints\", \"data/rust\", \"logs\"]:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "    print(\"Local directories created (no Drive backup).\")\n",
    "\n",
    "print(\"\\nStorage ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4c5d6e7f8",
   "metadata": {},
   "source": [
    "### 0.5 Check GPU & Configure MoE Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b4c5d6e7f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    capability = torch.cuda.get_device_capability()\n",
    "    is_h100 = \"H100\" in gpu_name or \"H200\" in gpu_name or \"B200\" in gpu_name\n",
    "\n",
    "    CONFIG[\"use_fp8\"] = capability[0] >= 9 and is_h100\n",
    "\n",
    "    if is_h100:\n",
    "        detected_tier = \"h100_80gb\"\n",
    "    elif gpu_memory >= 70:\n",
    "        detected_tier = \"a100_80gb\"\n",
    "    else:\n",
    "        detected_tier = \"a100_40gb\"\n",
    "\n",
    "    if detected_tier != CONFIG[\"gpu_tier\"]:\n",
    "        print(f\"NOTE: Auto-detected {detected_tier}, overriding configured {CONFIG['gpu_tier']}\")\n",
    "        CONFIG[\"gpu_tier\"] = detected_tier\n",
    "        gpu_cfg = GPU_CONFIGS[detected_tier]\n",
    "        CONFIG[\"moe_backend\"] = gpu_cfg[\"moe_backend\"]\n",
    "        CONFIG[\"load_mode\"] = gpu_cfg[\"load_mode\"]\n",
    "        CONFIG[\"fast_inference\"] = gpu_cfg[\"fast_inference\"]\n",
    "\n",
    "    os.environ[\"UNSLOTH_MOE_BACKEND\"] = CONFIG[\"moe_backend\"]\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"GPU: {gpu_name} ({gpu_memory:.0f} GB)\")\n",
    "    print(f\"Compute capability: {capability[0]}.{capability[1]}\")\n",
    "    print(f\"Tier: {CONFIG['gpu_tier']}\")\n",
    "    print(f\"\\nSplit LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(f\"Load mode: {CONFIG['load_mode']}\")\n",
    "    print(f\"FP8 available: {CONFIG['use_fp8']}\")\n",
    "    print(f\"Fast inference (vLLM): {CONFIG['fast_inference']}\")\n",
    "\n",
    "    if gpu_memory < 40:\n",
    "        print(\"\\nWARNING: <40 GB VRAM. Long-context training (16K+) may OOM.\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"No GPU detected!\")\n",
    "    CONFIG[\"use_fp8\"] = False\n",
    "    os.environ[\"UNSLOTH_MOE_BACKEND\"] = \"native_torch\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6e7f8a9b0",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Data Preparation\n",
    "\n",
    "Downloads Strandset-Rust-v1 from HuggingFace, parses the 15 task categories,\n",
    "and formats everything in Harmony for each training stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5d6e7f8a9b0c1",
   "metadata": {},
   "source": [
    "### 1.1 Download & Format Strandset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6e7f8a9b0c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = 500 if CONFIG[\"training_scope\"] == \"quick_test\" else 0  # 0 = all\n",
    "\n",
    "print(\"Downloading & formatting Strandset-Rust-v1...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cmd = \"python scripts/20_prepare_strandset.py\"\n",
    "if max_samples:\n",
    "    cmd += f\" --max_samples {max_samples}\"\n",
    "if not CONFIG[\"include_ipo\"]:\n",
    "    cmd += \" --no-preferences\"\n",
    "\n",
    "!{cmd}\n",
    "\n",
    "drive_helper.backup(\"data/rust/strandset\", \"data/rust/strandset\")\n",
    "if DRIVE_MODE != \"local\":\n",
    "    print(\"\\nBacked up Strandset data to Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f8a9b0c1d2e3",
   "metadata": {},
   "source": [
    "### 1.2 Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8a9b0c1d2e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_checks = [\n",
    "    (\"Strandset lang_rust\", \"data/rust/strandset/lang_rust/train\"),\n",
    "    (\"Strandset core_agent\", \"data/rust/strandset/core_agent/train\"),\n",
    "    (\"Strandset IPO\", \"data/rust/strandset/ipo/train\"),\n",
    "    (\"Strandset eval\", \"data/rust/strandset/eval/test\"),\n",
    "    (\"Stats\", \"data/rust/strandset/stats.json\"),\n",
    "]\n",
    "\n",
    "print(\"Data Verification:\")\n",
    "print(\"=\" * 60)\n",
    "for name, path in data_checks:\n",
    "    exists = os.path.exists(path)\n",
    "    if exists and os.path.isdir(path):\n",
    "        items = os.listdir(path)\n",
    "        print(f\"  \\u2713 {name}: {path} ({len(items)} items)\")\n",
    "    elif exists:\n",
    "        size_kb = os.path.getsize(path) / 1024\n",
    "        print(f\"  \\u2713 {name}: {path} ({size_kb:.1f} KB)\")\n",
    "    else:\n",
    "        needed = True\n",
    "        if not CONFIG[\"include_ipo\"] and \"IPO\" in name:\n",
    "            needed = False\n",
    "        if CONFIG[\"training_scope\"] == \"lang_adapter_only\" and name in (\"Strandset core_agent\", \"Strandset IPO\"):\n",
    "            needed = False\n",
    "        sym = \"\\u2717\" if needed else \"\\u2014\"\n",
    "        label = \"MISSING\" if needed else \"not needed\"\n",
    "        print(f\"  {sym} {name}: {label}\")\n",
    "\n",
    "# Show stats if available\n",
    "stats_path = \"data/rust/strandset/stats.json\"\n",
    "if os.path.exists(stats_path):\n",
    "    import json\n",
    "    with open(stats_path) as f:\n",
    "        stats = json.load(f)\n",
    "    print(f\"\\n  Total processed: {stats.get('total_processed', '?'):,}\")\n",
    "    print(f\"  Lang adapter: {stats.get('lang_rust', '?'):,}\")\n",
    "    print(f\"  Core agent: {stats.get('core_agent_debug', 0) + stats.get('core_agent_review', 0):,}\")\n",
    "    print(f\"  IPO pairs: {stats.get('ipo', '?'):,}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9b0c1d2e3f4a5",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Lang Adapter Training\n",
    "\n",
    "Train a QLoRA adapter (rank 64) to specialise GPT-OSS 20B on Rust syntax, stdlib, and idioms.\n",
    "Uses Strandset's code_generation, code_completion, docstring, comment, and naming examples.\n",
    "Then merge the adapter into the base weights for downstream training.\n",
    "\n",
    "**Split LoRA** backend auto-enabled for 7-12x faster MoE training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0c1d2e3f4a5b6",
   "metadata": {},
   "source": [
    "### 2.1 Train lang_rust Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1d2e3f4a5b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = CONFIG[\"lang_rust_batch\"]\n",
    "grad_accum = CONFIG[\"lang_rust_grad_accum\"]\n",
    "max_steps = CONFIG[\"lang_rust_max_steps\"]\n",
    "seq_len = CONFIG[\"lang_rust_seq_len\"]\n",
    "\n",
    "cmd = f\"python scripts/13_train_lang_adapter.py\"\n",
    "cmd += f\" --train_data_path data/rust/strandset/lang_rust/train\"\n",
    "cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "print(f\"Training lang_rust adapter...\")\n",
    "print(f\"  Data: data/rust/strandset/lang_rust/train\")\n",
    "print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "print(f\"  Max steps: {max_steps}\")\n",
    "print(f\"  Seq length: {seq_len} (from config)\")\n",
    "print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "!{cmd}\n",
    "\n",
    "drive_helper.backup(\"checkpoints/lang_rust\", \"checkpoints/lang_rust\")\n",
    "if DRIVE_MODE != \"local\":\n",
    "    print(\"\\nCheckpoint backed up to Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4a5b6c7d8",
   "metadata": {},
   "source": [
    "### 2.2 Merge lang_rust into Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3f4a5b6c7d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Merging lang_rust adapter into base model...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "!python scripts/19_merge_adapter.py \\\n",
    "    --adapter_path checkpoints/lang_rust/final \\\n",
    "    --output_dir checkpoints/gpt-oss-20b-rust-merged \\\n",
    "    --export_formats hf\n",
    "\n",
    "drive_helper.backup(\"checkpoints/gpt-oss-20b-rust-merged\", \"checkpoints/gpt-oss-20b-rust-merged\")\n",
    "if DRIVE_MODE != \"local\":\n",
    "    print(\"\\nMerged model backed up to Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f4a5b6c7d8e9f0",
   "metadata": {},
   "source": [
    "### 2.3 Verify Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5b6c7d8e9f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_path = \"checkpoints/gpt-oss-20b-rust-merged\"\n",
    "adapter_path = \"checkpoints/lang_rust/final\"\n",
    "\n",
    "print(\"Merge Verification:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if os.path.exists(merged_path):\n",
    "    files = os.listdir(merged_path)\n",
    "    safetensors = [f for f in files if f.endswith(\".safetensors\")]\n",
    "    print(f\"  \\u2713 Merged model: {merged_path}\")\n",
    "    print(f\"    {len(safetensors)} safetensors shard(s), {len(files)} total files\")\n",
    "else:\n",
    "    print(f\"  \\u2717 Merged model not found at {merged_path}\")\n",
    "\n",
    "if os.path.exists(adapter_path):\n",
    "    adapter_files = os.listdir(adapter_path)\n",
    "    print(f\"  \\u2713 Adapter: {adapter_path} ({len(adapter_files)} files)\")\n",
    "else:\n",
    "    print(f\"  \\u2717 Adapter not found at {adapter_path}\")\n",
    "\n",
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"\\n\\u2713 lang_adapter_only scope complete. Stopping here.\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6c7d8e9f0a1b2",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Core Agent SFT\n",
    "\n",
    "Train a higher-rank LoRA adapter (rank 128) on Strandset's debug/review examples.\n",
    "Uses the merged lang_rust model as the base.\n",
    "\n",
    "**Split LoRA** + **Auto packing** (3x faster, zero-config)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c7d8e9f0a1b2c3",
   "metadata": {},
   "source": [
    "### 3.1 Train core_agent Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8e9f0a1b2c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    batch = CONFIG[\"core_agent_batch\"]\n",
    "    grad_accum = CONFIG[\"core_agent_grad_accum\"]\n",
    "    max_steps = CONFIG[\"core_agent_max_steps\"]\n",
    "    seq_len = CONFIG[\"core_agent_seq_len\"]\n",
    "\n",
    "    cmd = f\"python scripts/14_train_core_agent.py\"\n",
    "    cmd += f\" --train_data_path data/rust/strandset/core_agent/train\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    print(f\"Training core_agent adapter...\")\n",
    "    print(f\"  Data: data/rust/strandset/core_agent/train\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Seq length: {seq_len} (from config)\")\n",
    "    print(f\"  LoRA rank: 128\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(f\"  Auto packing: enabled (uncontaminated)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/core_agent\", \"checkpoints/core_agent\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e9f0a1b2c3d4e5",
   "metadata": {},
   "source": [
    "### 3.2 Verify core_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f0a1b2c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    ckpt_path = \"checkpoints/core_agent/final\"\n",
    "\n",
    "    print(\"Core Agent Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(ckpt_path):\n",
    "        files = os.listdir(ckpt_path)\n",
    "        print(f\"  \\u2713 Checkpoint: {ckpt_path} ({len(files)} files)\")\n",
    "\n",
    "        adapter_config = os.path.join(ckpt_path, \"adapter_config.json\")\n",
    "        if os.path.exists(adapter_config):\n",
    "            import json\n",
    "            with open(adapter_config) as f:\n",
    "                cfg = json.load(f)\n",
    "            print(f\"    LoRA rank: {cfg.get('r', '?')}\")\n",
    "            print(f\"    Alpha: {cfg.get('lora_alpha', '?')}\")\n",
    "            print(f\"    Target modules: {cfg.get('target_modules', '?')}\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 Checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a1b2c3d4e5f6a7",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: IPO Preference Training (Optional)\n",
    "\n",
    "Train with Identity Preference Optimisation on synthetic preference pairs\n",
    "from Strandset's bug_detection category (fixed=chosen, buggy=rejected).\n",
    "\n",
    "Very low learning rate (5e-7), 1 epoch only to avoid collapse.\n",
    "\n",
    "Set `include_ipo=False` in Step 0.3 to skip."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4e5f6a7b9",
   "metadata": {},
   "source": [
    "### 4.1 Train with IPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5f6a7b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "elif not CONFIG[\"include_ipo\"]:\n",
    "    print(\"Skipping \\u2014 IPO disabled (include_ipo=False)\")\n",
    "else:\n",
    "    batch = CONFIG[\"ipo_batch\"]\n",
    "    grad_accum = CONFIG[\"ipo_grad_accum\"]\n",
    "    max_steps = CONFIG[\"ipo_max_steps\"]\n",
    "\n",
    "    ipo_checkpoint = \"checkpoints/core_agent/final\"\n",
    "\n",
    "    cmd = f\"python scripts/17_ipo_preference.py\"\n",
    "    cmd += f\" --checkpoint {ipo_checkpoint}\"\n",
    "    cmd += f\" --train_data_path data/rust/strandset/ipo/train\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    print(f\"Training with IPO (synthetic preferences)...\")\n",
    "    print(f\"  Checkpoint: {ipo_checkpoint}\")\n",
    "    print(f\"  Data: data/rust/strandset/ipo/train\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Loss: IPO (beta=0.1)\")\n",
    "    print(f\"  Load mode: {CONFIG['load_mode']}\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/core_agent_ipo\", \"checkpoints/core_agent_ipo\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6a7b9c0d1",
   "metadata": {},
   "source": [
    "### 4.2 Verify IPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7b9c0d1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "elif not CONFIG[\"include_ipo\"]:\n",
    "    print(\"Skipping \\u2014 IPO disabled\")\n",
    "else:\n",
    "    ckpt_path = \"checkpoints/core_agent_ipo/final\"\n",
    "\n",
    "    print(\"IPO Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(ckpt_path):\n",
    "        files = os.listdir(ckpt_path)\n",
    "        print(f\"  \\u2713 IPO checkpoint: {ckpt_path} ({len(files)} files)\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 IPO checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "    tb_dir = \"checkpoints/core_agent_ipo\"\n",
    "    tb_files = []\n",
    "    if os.path.exists(tb_dir):\n",
    "        for root, dirs, fnames in os.walk(tb_dir):\n",
    "            for fn in fnames:\n",
    "                if fn.startswith(\"events.out.tfevents\"):\n",
    "                    tb_files.append(os.path.join(root, fn))\n",
    "    if tb_files:\n",
    "        print(f\"  \\u2713 TensorBoard logs found ({len(tb_files)} event files)\")\n",
    "        print(f\"    Monitor KL divergence: warn >0.3, abort >0.5\")\n",
    "    else:\n",
    "        print(f\"  \\u2014 No TensorBoard logs found\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b9c0d1e2f3",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Test Model\n",
    "\n",
    "Load the trained model and generate Rust code interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b9c0d1e2f3a4",
   "metadata": {},
   "source": [
    "### 5.1 Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b9c0d1e2f3a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "CHECKPOINT_PRIORITY = [\n",
    "    \"checkpoints/core_agent_ipo/final\",\n",
    "    \"checkpoints/core_agent/final\",\n",
    "    \"checkpoints/gpt-oss-20b-rust-merged\",\n",
    "]\n",
    "\n",
    "MODEL_PATH = None\n",
    "for path in CHECKPOINT_PRIORITY:\n",
    "    if os.path.exists(path):\n",
    "        MODEL_PATH = path\n",
    "        break\n",
    "\n",
    "if MODEL_PATH is None:\n",
    "    print(\"\\u2717 No checkpoint found. Train the model first.\")\n",
    "else:\n",
    "    print(f\"Loading model from: {MODEL_PATH}\")\n",
    "\n",
    "    load_kwargs = {\n",
    "        \"max_seq_length\": 4096,\n",
    "        \"dtype\": torch.bfloat16,\n",
    "    }\n",
    "    if CONFIG.get(\"load_mode\") == \"fp8\" and CONFIG.get(\"use_fp8\"):\n",
    "        load_kwargs[\"load_in_fp8\"] = True\n",
    "        print(\"  Mode: FP8 (H100)\")\n",
    "    else:\n",
    "        load_kwargs[\"load_in_4bit\"] = True\n",
    "        print(\"  Mode: 4-bit QLoRA\")\n",
    "\n",
    "    if CONFIG.get(\"fast_inference\"):\n",
    "        load_kwargs[\"fast_inference\"] = True\n",
    "        print(\"  Inference: vLLM backend\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(MODEL_PATH, **load_kwargs)\n",
    "    FastLanguageModel.for_inference(model)\n",
    "\n",
    "    print(\"\\u2713 Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c0d1e2f3a4b5c6",
   "metadata": {},
   "source": [
    "### 5.2 Generate Rust Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d1e2f3a4b5c6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"scripts\")\n",
    "from dataset_formatters.harmony import encode_harmony_messages\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    \"Write a Rust function `fn merge_sorted(a: &[i32], b: &[i32]) -> Vec<i32>` that merges two sorted slices into a single sorted vector.\",\n",
    "    \"This Rust code fails the borrow checker. Fix it:\\n```rust\\nfn main() {\\n    let mut v = vec![1, 2, 3];\\n    let first = &v[0];\\n    v.push(4);\\n    println!(\\\"{}\\\", first);\\n}\\n```\",\n",
    "    \"Write an async Rust function using tokio that fetches a URL with reqwest, retries up to 3 times on failure, and returns the response body as a String.\",\n",
    "]\n",
    "\n",
    "def generate_rust(prompt, max_tokens=1024):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted = encode_harmony_messages(\n",
    "        messages,\n",
    "        developer_instructions=\"You are a Rust programming expert. Write correct, idiomatic code.\",\n",
    "    )\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.3,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "for i, prompt in enumerate(TEST_PROMPTS, 1):\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Test {i}: {prompt[:80]}...\")\n",
    "    print(\"=\" * 60)\n",
    "    response = generate_rust(prompt)\n",
    "    print(response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e2f3a4b5c6d7e8",
   "metadata": {},
   "source": [
    "### 5.3 Custom Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f3a4b5c6d7e8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_PROMPT = \"Write a Rust function that reads a CSV file and returns the sum of a specified column.\"\n",
    "\n",
    "print(f\"Prompt: {CUSTOM_PROMPT}\")\n",
    "print(\"=\" * 60)\n",
    "print(generate_rust(CUSTOM_PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a4b5c6d7e8f9a0",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Export\n",
    "\n",
    "Merge the final adapter and export to HuggingFace + GGUF formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5c6d7e8f9a0b1",
   "metadata": {},
   "source": [
    "### 6.1 Export to GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c6d7e8f9a0b1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADAPTER_PRIORITY = [\n",
    "    \"checkpoints/core_agent_ipo/final\",\n",
    "    \"checkpoints/core_agent/final\",\n",
    "    \"checkpoints/lang_rust/final\",\n",
    "]\n",
    "\n",
    "adapter_path = None\n",
    "for path in ADAPTER_PRIORITY:\n",
    "    if os.path.exists(path):\n",
    "        adapter_path = path\n",
    "        break\n",
    "\n",
    "if adapter_path is None:\n",
    "    print(\"\\u2717 No adapter checkpoint found.\")\n",
    "else:\n",
    "    export_dir = \"checkpoints/gpt-oss-20b-rust-export-v3\"\n",
    "    print(f\"Exporting adapter: {adapter_path}\")\n",
    "    print(f\"Output: {export_dir}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/19_merge_adapter.py \\\n",
    "        --adapter_path {adapter_path} \\\n",
    "        --output_dir {export_dir} \\\n",
    "        --export_formats hf gguf_q4\n",
    "\n",
    "    drive_helper.backup(export_dir, \"checkpoints/gpt-oss-20b-rust-export-v3\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nExport backed up to Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d7e8f9a0b1c2d3",
   "metadata": {},
   "source": [
    "### 6.2 Download GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e8f9a0b1c2d3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    import glob\n",
    "\n",
    "    export_dir = \"checkpoints/gpt-oss-20b-rust-export-v3\"\n",
    "    gguf_files = glob.glob(os.path.join(export_dir, \"*.gguf\"))\n",
    "\n",
    "    if gguf_files:\n",
    "        gguf_path = gguf_files[0]\n",
    "        size_gb = os.path.getsize(gguf_path) / (1024**3)\n",
    "        print(f\"Downloading: {os.path.basename(gguf_path)} ({size_gb:.1f} GB)\")\n",
    "        files.download(gguf_path)\n",
    "    else:\n",
    "        print(\"\\u2717 No GGUF file found. Run export (6.1) first.\")\n",
    "else:\n",
    "    print(\"Download not available outside Colab.\")\n",
    "    print(\"GGUF file is at: checkpoints/gpt-oss-20b-rust-export-v3/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f9a0b1c2d3e4f5",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Complete!\n",
    "\n",
    "Your GPT-OSS 20B Rust coding agent (v3 \\u2014 Strandset) is trained and ready to use.\n",
    "\n",
    "**Data source:** [Strandset-Rust-v1](https://huggingface.co/datasets/Fortytwo-Network/Strandset-Rust-v1) (191K examples, Apache 2.0)\n",
    "\n",
    "**Pipeline:**\n",
    "1. Lang Adapter: Rust domain specialisation from code generation/completion examples\n",
    "2. Core Agent SFT: Debug and review training from bug_detection/code_review examples\n",
    "3. IPO: Synthetic preference pairs from bug_detection (if enabled)\n",
    "\n",
    "**Outputs:**\n",
    "- Checkpoints: `checkpoints/core_agent_{ipo}/final`\n",
    "- Exported model: `checkpoints/gpt-oss-20b-rust-export-v3/`\n",
    "- All backed up to Google Drive: `gpt-oss-20b-rust-agent-v3/`\n",
    "\n",
    "**Compared to v2:**\n",
    "- No Rust toolchain required \\u2014 runs on any Colab GPU instance\n",
    "- No cargo-mutants or trajectory generation \\u2014 faster setup\n",
    "- No GRPO RL \\u2014 no execution-based rewards\n",
    "- For better results, consider upgrading to v2 with mutation data + GRPO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
