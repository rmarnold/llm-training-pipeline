{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4e5f6a7b8",
   "metadata": {},
   "source": [
    "# Train GPT-OSS 20B ‚Üí Rust Coding Agent (v3 ‚Äî Strandset)\n",
    "\n",
    "Simplified pipeline using [Strandset-Rust-v1](https://huggingface.co/datasets/Fortytwo-Network/Strandset-Rust-v1) (191K verified Rust examples, Apache 2.0) as the sole data source.\n",
    "\n",
    "**Key differences from v2:**\n",
    "- **No Rust toolchain** ‚Äî no `rustup`, `cargo-mutants`, or compilation needed\n",
    "- **No mutation/trajectory generation** ‚Äî data comes entirely from Strandset\n",
    "- **No GRPO** ‚Äî no execution-based rewards without cargo\n",
    "- **IPO from synthetic preferences** ‚Äî bug_detection pairs (fixed=chosen, buggy=rejected)\n",
    "\n",
    "**3-Phase Pipeline:**\n",
    "1. **Lang Adapter** ‚Äî Rust domain specialisation via QLoRA (script 13 + 19)\n",
    "2. **Core Agent SFT** ‚Äî Debug/review training from Strandset (script 14)\n",
    "3. **IPO Preference** ‚Äî Synthetic preference pairs from bug_detection (script 17)\n",
    "\n",
    "**Requirements:**\n",
    "- **GPU**: A100 40GB+ (H100 80GB recommended for FP8)\n",
    "- **Storage**: Google Drive for persistent checkpoints\n",
    "- **No Rust toolchain required**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5f6a7b8c9",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6a7b8c9d0",
   "metadata": {},
   "source": [
    "### 0.1 Mount Google Drive & Clone Repository\n",
    "\n",
    "**PyCharm / headless users:** If `drive.mount()` doesn't work, set `use_service_account = True`\n",
    "and provide your service-account JSON key in Step 0.3."
   ]
  },
  {
   "cell_type": "code",
   "id": "d4e5f6a7b8c9d0e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T06:31:58.686553Z",
     "start_time": "2026-02-14T06:31:57.588886Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "use_service_account = True\n",
    "\n",
    "DRIVE_MOUNTED = False\n",
    "\n",
    "if IN_COLAB and not use_service_account:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        DRIVE_MOUNTED = True\n",
    "        print(\"Google Drive mounted\")\n",
    "    except Exception as e:\n",
    "        print(f\"drive.mount() failed: {e}\")\n",
    "        print(\"Falling back to local-only mode.\")\n",
    "        print(\"Tip: set use_service_account=True and provide a JSON key in Step 0.3.\")\n",
    "elif IN_COLAB and use_service_account:\n",
    "    print(\"Service-account mode selected \\u2014 skipping drive.mount()\")\n",
    "    print(\"Configure credentials in Step 0.3.\")\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "\n",
    "REPO_URL = \"https://github.com/rmarnold/llm-training-pipeline.git\"\n",
    "BRANCH = \"main\"\n",
    "\n",
    "REPO_DIR = \"/content/llm-training-pipeline\"\n",
    "\n",
    "if IN_COLAB:\n",
    "    if os.path.exists(REPO_DIR):\n",
    "        %cd {REPO_DIR}\n",
    "        !git pull origin {BRANCH}\n",
    "    else:\n",
    "        !git clone -b {BRANCH} {REPO_URL} {REPO_DIR}\n",
    "        %cd {REPO_DIR}\n",
    "\n",
    "    PROJECT_ROOT = REPO_DIR\n",
    "else:\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"\\nProject root: {PROJECT_ROOT}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service-account mode selected ‚Äî skipping drive.mount()\n",
      "Configure credentials in Step 0.3.\n",
      "/content/llm-training-pipeline\n",
      "remote: Enumerating objects: 7, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects: 100% (4/4), 693 bytes | 693.00 KiB/s, done.\n",
      "From https://github.com/rmarnold/llm-training-pipeline\n",
      " * branch            main       -> FETCH_HEAD\n",
      "   07bd722..a398511  main       -> origin/main\n",
      "Updating 07bd722..a398511\n",
      "Fast-forward\n",
      " scripts/20_prepare_strandset.py | 14 \u001b[32m++++++++++++\u001b[m\u001b[31m--\u001b[m\n",
      " 1 file changed, 12 insertions(+), 2 deletions(-)\n",
      "\n",
      "Project root: /content/llm-training-pipeline\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8c9d0e1f2",
   "metadata": {},
   "source": [
    "### 0.2 Install Dependencies\n",
    "\n",
    "Installs pipeline deps and latest Unsloth. **No Rust toolchain needed** ‚Äî all training data\n",
    "comes from Strandset.\n",
    "\n",
    "**Note:** flash-attn is intentionally NOT installed. FA3 is incompatible with GPT-OSS\n",
    "backward passes. Unsloth's Flex Attention replaces it automatically."
   ]
  },
  {
   "cell_type": "code",
   "id": "f6a7b8c9d0e1f2a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T06:33:29.341837Z",
     "start_time": "2026-02-14T06:31:58.697845Z"
    }
   },
   "source": [
    "if IN_COLAB:\n",
    "    print(\"Installing Python dependencies...\")\n",
    "    print(\"=\" * 60)\n",
    "    !pip install -q -e \".[gpt_oss,colab]\"\n",
    "\n",
    "    # Fix pyarrow binary incompatibility with datasets 4.x on Colab\n",
    "    !pip install -q --force-reinstall pyarrow\n",
    "\n",
    "    # Force latest Unsloth with Split LoRA + FP8 RL\n",
    "    print(\"\\nInstalling latest Unsloth (Split LoRA + Flex Attention)...\")\n",
    "    !pip install -q --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo\n",
    "    !pip install -q \"unsloth[colab-new]\"\n",
    "\n",
    "    # vLLM for FP8 inference (H100 only, optional)\n",
    "    !pip install -q vllm>=0.12.0 2>/dev/null || true\n",
    "\n",
    "    # Verification\n",
    "    from importlib.metadata import version, PackageNotFoundError\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Dependency Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for pkg in [\"unsloth\", \"trl\", \"peft\", \"datasets\", \"tiktoken\", \"vllm\"]:\n",
    "        try:\n",
    "            ver = version(pkg)\n",
    "            print(f\"\\u2713 {pkg}: {ver}\")\n",
    "        except PackageNotFoundError:\n",
    "            if pkg == \"vllm\":\n",
    "                print(f\"\\u2014 {pkg}: not installed (optional, H100 FP8 only)\")\n",
    "            else:\n",
    "                print(f\"\\u2717 {pkg}: not installed\")\n",
    "\n",
    "    print(\"\\nNote: No Rust toolchain needed for v3 (Strandset-only pipeline)\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"Running locally \\u2014 ensure deps are installed:\")\n",
    "    print(\"  pip install -e '.[gpt_oss]'\")\n",
    "    print(\"  pip install --upgrade unsloth unsloth_zoo\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing Python dependencies...\n",
      "============================================================\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building editable for llm-training-pipeline (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "vllm 0.15.1 requires torch==2.9.1, but you have torch 2.10.0 which is incompatible.\n",
      "vllm 0.15.1 requires torchvision==0.24.1, but you have torchvision 0.25.0 which is incompatible.\n",
      "torchaudio 2.9.1 requires torch==2.9.1, but you have torch 2.10.0 which is incompatible.\n",
      "fastai 2.8.6 requires torch<2.10,>=1.10, but you have torch 2.10.0 which is incompatible.\n",
      "cuda-python 12.9.5 requires cuda-bindings~=12.9.5, but you have cuda-bindings 12.9.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "Installing latest Unsloth (Split LoRA + Flex Attention)...\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m432.3/432.3 kB\u001b[0m \u001b[31m502.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m376.5/376.5 kB\u001b[0m \u001b[31m586.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: unsloth 2026.2.1 does not provide the extra 'triton'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "============================================================\n",
      "Dependency Verification:\n",
      "============================================================\n",
      "‚úì unsloth: 2026.2.1\n",
      "‚úì trl: 0.24.0\n",
      "‚úì peft: 0.18.1\n",
      "‚úì datasets: 4.3.0\n",
      "‚úì tiktoken: 0.12.0\n",
      "‚úì vllm: 0.15.1\n",
      "\n",
      "Note: No Rust toolchain needed for v3 (Strandset-only pipeline)\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0e1f2a3b4",
   "metadata": {},
   "source": [
    "### 0.3 Configure Pipeline\n",
    "\n",
    "**Training Scope** (`training_scope`):\n",
    "- `full` \\u2014 All 3 phases (Lang Adapter + Core Agent + IPO)\n",
    "- `quick_test` \\u2014 Short runs (100 steps each) to verify setup\n",
    "- `lang_adapter_only` \\u2014 Only train lang_rust adapter + merge\n",
    "\n",
    "**Service Account Setup** (for Drive backup):\n",
    "1. Set `use_service_account = True` in cell 0.1\n",
    "2. Run cell 0.3 \\u2014 it will try Colab Secrets, then file, then paste prompt\n",
    "3. Set `DRIVE_FOLDER_ID` in Colab Secrets, or set `drive_folder_id` below"
   ]
  },
  {
   "cell_type": "code",
   "id": "b8c9d0e1f2a3b4c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T06:33:30.383568Z",
     "start_time": "2026-02-14T06:33:29.694633Z"
    }
   },
   "source": "import json\ntraining_scope = \"quick_test\"  # \"full\", \"quick_test\", \"lang_adapter_only\"\n\ngpu_tier = \"h100_80gb\"  # \"a100_40gb\", \"a100_80gb\", \"h100_80gb\"\n\nmax_steps_override = 0  # Set >0 to cap all stages (0 = use defaults)\n\ninclude_ipo = True  # False to skip IPO preference training\n\nenable_qat_export = False  # True for MXFP4 QAT export\n\n# ============================================================\n# SERVICE ACCOUNT CREDENTIALS\n# ============================================================\n# Priority order:\n#   1. Existing VALID file at /content/service_account.json (instant, no timeout)\n#   2. Colab Secrets (only if no valid file ‚Äî may timeout outside browser UI)\n#   3. Paste JSON key via input() prompt\n#   4. Fall back to local mode (no Drive backup)\n\ndrive_folder_id = \"18UpFpUhiNrs2Etha0uFjSGWmj1Ee1SnX\"  # Google Drive folder ID\n\n_SA_VM_PATH = \"/content/service_account.json\"\n_FOLDER_ID_PATH = \"/content/drive_folder_id.txt\"\nservice_account_key = \"\"\n\ndef _is_json(s):\n    \"\"\"Check if string looks like JSON (not a folder ID).\"\"\"\n    return s.strip().startswith(\"{\")\n\ndef _validate_sa_file(path):\n    \"\"\"Check that a service account JSON file exists and contains valid JSON.\"\"\"\n    try:\n        with open(path) as f:\n            data = json.load(f)\n        return isinstance(data, dict) and \"type\" in data\n    except (json.JSONDecodeError, OSError, ValueError):\n        return False\n\nif use_service_account and IN_COLAB:\n    # 1. Check for existing VALID file first (avoids Colab Secrets timeout on re-runs)\n    if os.path.exists(_SA_VM_PATH) and _validate_sa_file(_SA_VM_PATH):\n        service_account_key = _SA_VM_PATH\n        print(f\"Using existing key file: {_SA_VM_PATH}\")\n    else:\n        if os.path.exists(_SA_VM_PATH):\n            os.remove(_SA_VM_PATH)\n            print(f\"Removed invalid/empty key file: {_SA_VM_PATH}\")\n\n        # 2. Try Colab Secrets (may timeout if not running in browser UI)\n        try:\n            from google.colab import userdata\n            _key_json = userdata.get(\"SERVICE_ACCOUNT_KEY\")\n            if _key_json:\n                # Validate before saving\n                json.loads(_key_json)\n                with open(_SA_VM_PATH, \"w\") as _f:\n                    _f.write(_key_json)\n                service_account_key = _SA_VM_PATH\n                print(\"Service account key loaded from Colab Secrets.\")\n        except json.JSONDecodeError:\n            print(\"  Colab Secret SERVICE_ACCOUNT_KEY contains invalid JSON.\")\n        except Exception as _e:\n            print(f\"  Colab Secrets lookup failed: {type(_e).__name__}: {_e}\")\n\n        # 3. Fall back to paste prompt\n        if not service_account_key:\n            try:\n                print(\"No service account key found.\")\n                _key_text = input(\"Paste service account JSON (entire content in one go): \")\n                _key_text = _key_text.strip()\n                if _key_text:\n                    json.loads(_key_text)\n                    with open(_SA_VM_PATH, \"w\") as _f:\n                        _f.write(_key_text)\n                    service_account_key = _SA_VM_PATH\n                    print(f\"Saved to {_SA_VM_PATH}\")\n            except json.JSONDecodeError:\n                print(\"  Invalid JSON ‚Äî key not saved.\")\n            except EOFError:\n                pass\n\n    # Resolve drive_folder_id: saved file > Colab Secrets > input prompt\n    if not drive_folder_id and os.path.exists(_FOLDER_ID_PATH):\n        with open(_FOLDER_ID_PATH) as _f:\n            drive_folder_id = _f.read().strip()\n        if drive_folder_id:\n            print(f\"Using saved folder ID from {_FOLDER_ID_PATH}\")\n\n    if not drive_folder_id:\n        try:\n            from google.colab import userdata\n            _fid = userdata.get(\"DRIVE_FOLDER_ID\") or \"\"\n            if _fid and not _is_json(_fid):\n                drive_folder_id = _fid\n                print(f\"Drive folder ID loaded from Colab Secrets.\")\n            elif _fid:\n                print(\"WARNING: DRIVE_FOLDER_ID Colab Secret contains JSON, not a folder ID. Ignoring.\")\n        except Exception:\n            pass\n\n    if not drive_folder_id and service_account_key:\n        _fid = input(\"Enter Google Drive folder ID (from URL): \").strip()\n        if _fid and not _is_json(_fid):\n            drive_folder_id = _fid\n            # Persist so we don't have to re-enter on re-runs\n            with open(_FOLDER_ID_PATH, \"w\") as _f:\n                _f.write(drive_folder_id)\n            print(f\"Saved folder ID to {_FOLDER_ID_PATH}\")\n        elif _fid:\n            print(\"ERROR: That looks like JSON, not a folder ID.\")\n            print(\"The folder ID is the part after /folders/ in the Google Drive URL.\")\n\n    if not service_account_key:\n        print(\"No service account key ‚Äî Drive backup disabled.\")\n\nelif use_service_account:\n    for _path in [_SA_VM_PATH, \"service_account.json\"]:\n        if os.path.exists(_path):\n            service_account_key = _path\n            print(f\"Using key file: {_path}\")\n            break\n    if not service_account_key:\n        print(\"Running locally ‚Äî set service_account_key to your JSON key path.\")\n\n# ============================================================\n# DRIVE MODE\n# ============================================================\nfrom scripts.pipeline_lib.drive_utils import DriveHelper\n\nDRIVE_BASE = \"/content/drive/MyDrive/gpt-oss-20b-rust-agent-v3\"\n\nif DRIVE_MOUNTED:\n    DRIVE_MODE = \"mounted\"\nelif use_service_account and service_account_key and drive_folder_id:\n    DRIVE_MODE = \"service_account\"\nelse:\n    DRIVE_MODE = \"local\"\n\ndrive_helper = DriveHelper(\n    mode=DRIVE_MODE,\n    drive_base=DRIVE_BASE,\n    credentials_path=service_account_key or None,\n    folder_id=drive_folder_id or None,\n)\n\n# ============================================================\n# GPU TIER CONFIGS\n# ============================================================\n\nGPU_CONFIGS = {\n    \"a100_40gb\": {\n        \"moe_backend\": \"unsloth_triton\",\n        \"load_mode\": \"4bit\",\n        \"fast_inference\": False,\n        \"lang_rust\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 8192, \"max_steps\": 3000},\n        \"core_agent\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 12288, \"max_steps\": 2000},\n        \"ipo\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 12288, \"max_steps\": 1000},\n    },\n    \"a100_80gb\": {\n        \"moe_backend\": \"unsloth_triton\",\n        \"load_mode\": \"4bit\",\n        \"fast_inference\": False,\n        \"lang_rust\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 8192, \"max_steps\": 5000},\n        \"core_agent\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 16384, \"max_steps\": 3000},\n        \"ipo\": {\"batch\": 1, \"grad_accum\": 16, \"seq_len\": 16384, \"max_steps\": 2000},\n    },\n    \"h100_80gb\": {\n        \"moe_backend\": \"grouped_mm\",\n        \"load_mode\": \"fp8\",\n        \"fast_inference\": True,\n        \"lang_rust\": {\"batch\": 2, \"grad_accum\": 4, \"seq_len\": 8192, \"max_steps\": 5000},\n        \"core_agent\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 16384, \"max_steps\": 3000},\n        \"ipo\": {\"batch\": 1, \"grad_accum\": 16, \"seq_len\": 16384, \"max_steps\": 2000},\n    },\n}\n\nif training_scope == \"quick_test\":\n    max_steps_override = 100\n\ngpu_cfg = GPU_CONFIGS[gpu_tier]\n\nCONFIG = {\n    \"training_scope\": training_scope,\n    \"gpu_tier\": gpu_tier,\n    \"include_ipo\": include_ipo,\n    \"enable_qat_export\": enable_qat_export,\n    \"moe_backend\": gpu_cfg[\"moe_backend\"],\n    \"load_mode\": gpu_cfg[\"load_mode\"],\n    \"fast_inference\": gpu_cfg[\"fast_inference\"],\n    # Lang adapter\n    \"lang_rust_batch\": gpu_cfg[\"lang_rust\"][\"batch\"],\n    \"lang_rust_grad_accum\": gpu_cfg[\"lang_rust\"][\"grad_accum\"],\n    \"lang_rust_seq_len\": gpu_cfg[\"lang_rust\"][\"seq_len\"],\n    \"lang_rust_max_steps\": max_steps_override or gpu_cfg[\"lang_rust\"][\"max_steps\"],\n    # Core agent\n    \"core_agent_batch\": gpu_cfg[\"core_agent\"][\"batch\"],\n    \"core_agent_grad_accum\": gpu_cfg[\"core_agent\"][\"grad_accum\"],\n    \"core_agent_seq_len\": gpu_cfg[\"core_agent\"][\"seq_len\"],\n    \"core_agent_max_steps\": max_steps_override or gpu_cfg[\"core_agent\"][\"max_steps\"],\n    # IPO\n    \"ipo_batch\": gpu_cfg[\"ipo\"][\"batch\"],\n    \"ipo_grad_accum\": gpu_cfg[\"ipo\"][\"grad_accum\"],\n    \"ipo_seq_len\": gpu_cfg[\"ipo\"][\"seq_len\"],\n    \"ipo_max_steps\": max_steps_override or gpu_cfg[\"ipo\"][\"max_steps\"],\n    # Eval\n    \"eval_num_samples\": 10 if training_scope == \"quick_test\" else 50,\n}\n\nprint(\"=\" * 60)\nprint(\"PIPELINE CONFIGURATION (v3 \\u2014 Strandset)\")\nprint(\"=\" * 60)\nprint(f\"\\nScope: {training_scope.upper()}\")\nprint(f\"GPU tier: {gpu_tier}\")\nprint(f\"MoE backend: {CONFIG['moe_backend']}\")\nprint(f\"Load mode: {CONFIG['load_mode']}\")\nprint(f\"Fast inference (vLLM): {CONFIG['fast_inference']}\")\nprint(f\"Include IPO: {include_ipo}\")\nprint(f\"QAT export: {enable_qat_export}\")\nprint(f\"Drive mode: {DRIVE_MODE}\")\nif max_steps_override:\n    print(f\"Max steps override: {max_steps_override}\")\nprint(f\"\\nLang Adapter:  batch={CONFIG['lang_rust_batch']} x grad_accum={CONFIG['lang_rust_grad_accum']}, seq={CONFIG['lang_rust_seq_len']}, steps={CONFIG['lang_rust_max_steps']}\")\nprint(f\"Core Agent:    batch={CONFIG['core_agent_batch']} x grad_accum={CONFIG['core_agent_grad_accum']}, seq={CONFIG['core_agent_seq_len']}, steps={CONFIG['core_agent_max_steps']}\")\nif include_ipo:\n    print(f\"IPO:           batch={CONFIG['ipo_batch']} x grad_accum={CONFIG['ipo_grad_accum']}, seq={CONFIG['ipo_seq_len']}, steps={CONFIG['ipo_max_steps']}\")\nprint(\"=\" * 60)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing key file: /content/service_account.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:scripts.pipeline_lib.drive_utils:DriveHelper: folder is on personal My Drive, not a Shared Drive. Service accounts cannot CREATE new files here (no storage quota). Uploads of new files will be skipped. To fix: create a Shared Drive (requires Google Workspace) and move your folder there.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PIPELINE CONFIGURATION (v3 ‚Äî Strandset)\n",
      "============================================================\n",
      "\n",
      "Scope: QUICK_TEST\n",
      "GPU tier: h100_80gb\n",
      "MoE backend: grouped_mm\n",
      "Load mode: fp8\n",
      "Fast inference (vLLM): True\n",
      "Include IPO: True\n",
      "QAT export: False\n",
      "Drive mode: service_account\n",
      "Max steps override: 100\n",
      "\n",
      "Lang Adapter:  batch=2 x grad_accum=4, seq=8192, steps=100\n",
      "Core Agent:    batch=1 x grad_accum=4, seq=16384, steps=100\n",
      "IPO:           batch=1 x grad_accum=16, seq=16384, steps=100\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2a3b4c5d6",
   "metadata": {},
   "source": [
    "### 0.4 Set Up Persistent Storage"
   ]
  },
  {
   "cell_type": "code",
   "id": "d0e1f2a3b4c5d6e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T06:33:42.177078Z",
     "start_time": "2026-02-14T06:33:30.587961Z"
    }
   },
   "source": [
    "DRIVE_SUBDIRS = [\n",
    "    \"checkpoints/lang_rust\",\n",
    "    \"checkpoints/core_agent\",\n",
    "    \"checkpoints/core_agent_ipo\",\n",
    "    \"checkpoints/gpt-oss-20b-rust-merged\",\n",
    "    \"data/rust/strandset\",\n",
    "    \"logs\",\n",
    "]\n",
    "\n",
    "if DRIVE_MODE == \"mounted\":\n",
    "    print(f\"Setting up storage at: {DRIVE_BASE}\")\n",
    "    for subdir in DRIVE_SUBDIRS:\n",
    "        os.makedirs(os.path.join(DRIVE_BASE, subdir), exist_ok=True)\n",
    "\n",
    "    for dir_name in [\"checkpoints\", \"data\", \"logs\"]:\n",
    "        local_path = os.path.join(PROJECT_ROOT, dir_name)\n",
    "        drive_path = os.path.join(DRIVE_BASE, dir_name)\n",
    "\n",
    "        if os.path.exists(local_path) and not os.path.islink(local_path):\n",
    "            !cp -r {local_path}/* {drive_path}/ 2>/dev/null || true\n",
    "            !rm -rf {local_path}\n",
    "        elif os.path.islink(local_path):\n",
    "            os.unlink(local_path)\n",
    "\n",
    "        os.symlink(drive_path, local_path)\n",
    "        print(f\"  {dir_name} -> Drive (mounted)\")\n",
    "\n",
    "elif DRIVE_MODE == \"service_account\":\n",
    "    print(\"Setting up local storage + Drive API restore...\")\n",
    "    for subdir in DRIVE_SUBDIRS:\n",
    "        os.makedirs(os.path.join(PROJECT_ROOT, subdir), exist_ok=True)\n",
    "        drive_helper.ensure_dir(subdir)\n",
    "\n",
    "    for dir_name in [\"checkpoints\", \"data\", \"logs\"]:\n",
    "        local_path = os.path.join(PROJECT_ROOT, dir_name)\n",
    "        if os.path.islink(local_path):\n",
    "            os.unlink(local_path)\n",
    "            os.makedirs(local_path, exist_ok=True)\n",
    "        print(f\"  {dir_name} -> local (backed up via Drive API)\")\n",
    "\n",
    "    print(\"\\nRestoring existing data from Drive...\")\n",
    "    for subdir in DRIVE_SUBDIRS:\n",
    "        local_target = os.path.join(PROJECT_ROOT, subdir)\n",
    "        drive_helper.restore(subdir, local_target)\n",
    "    print(\"Restore complete.\")\n",
    "\n",
    "else:\n",
    "    for d in [\"checkpoints\", \"data/rust\", \"logs\"]:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "    print(\"Local directories created (no Drive backup).\")\n",
    "\n",
    "print(\"\\nStorage ready!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up local storage + Drive API restore...\n",
      "  checkpoints -> local (backed up via Drive API)\n",
      "  data -> local (backed up via Drive API)\n",
      "  logs -> local (backed up via Drive API)\n",
      "\n",
      "Restoring existing data from Drive...\n",
      "Restore complete.\n",
      "\n",
      "Storage ready!\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4c5d6e7f8",
   "metadata": {},
   "source": [
    "### 0.5 Check GPU & Configure MoE Backend"
   ]
  },
  {
   "cell_type": "code",
   "id": "f2a3b4c5d6e7f8a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T06:33:42.732570Z",
     "start_time": "2026-02-14T06:33:42.530757Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    capability = torch.cuda.get_device_capability()\n",
    "    is_h100 = \"H100\" in gpu_name or \"H200\" in gpu_name or \"B200\" in gpu_name\n",
    "\n",
    "    CONFIG[\"use_fp8\"] = capability[0] >= 9 and is_h100\n",
    "\n",
    "    if is_h100:\n",
    "        detected_tier = \"h100_80gb\"\n",
    "    elif gpu_memory >= 70:\n",
    "        detected_tier = \"a100_80gb\"\n",
    "    else:\n",
    "        detected_tier = \"a100_40gb\"\n",
    "\n",
    "    if detected_tier != CONFIG[\"gpu_tier\"]:\n",
    "        print(f\"NOTE: Auto-detected {detected_tier}, overriding configured {CONFIG['gpu_tier']}\")\n",
    "        CONFIG[\"gpu_tier\"] = detected_tier\n",
    "        gpu_cfg = GPU_CONFIGS[detected_tier]\n",
    "        CONFIG[\"moe_backend\"] = gpu_cfg[\"moe_backend\"]\n",
    "        CONFIG[\"load_mode\"] = gpu_cfg[\"load_mode\"]\n",
    "        CONFIG[\"fast_inference\"] = gpu_cfg[\"fast_inference\"]\n",
    "\n",
    "    os.environ[\"UNSLOTH_MOE_BACKEND\"] = CONFIG[\"moe_backend\"]\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"GPU: {gpu_name} ({gpu_memory:.0f} GB)\")\n",
    "    print(f\"Compute capability: {capability[0]}.{capability[1]}\")\n",
    "    print(f\"Tier: {CONFIG['gpu_tier']}\")\n",
    "    print(f\"\\nSplit LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(f\"Load mode: {CONFIG['load_mode']}\")\n",
    "    print(f\"FP8 available: {CONFIG['use_fp8']}\")\n",
    "    print(f\"Fast inference (vLLM): {CONFIG['fast_inference']}\")\n",
    "\n",
    "    if gpu_memory < 40:\n",
    "        print(\"\\nWARNING: <40 GB VRAM. Long-context training (16K+) may OOM.\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"No GPU detected!\")\n",
    "    CONFIG[\"use_fp8\"] = False\n",
    "    os.environ[\"UNSLOTH_MOE_BACKEND\"] = \"native_torch\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GPU: NVIDIA H100 80GB HBM3 (79 GB)\n",
      "Compute capability: 9.0\n",
      "Tier: h100_80gb\n",
      "\n",
      "Split LoRA backend: grouped_mm\n",
      "Load mode: fp8\n",
      "FP8 available: True\n",
      "Fast inference (vLLM): True\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6e7f8a9b0",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Data Preparation\n",
    "\n",
    "Downloads Strandset-Rust-v1 from HuggingFace, parses the 15 task categories,\n",
    "and formats everything in Harmony for each training stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5d6e7f8a9b0c1",
   "metadata": {},
   "source": [
    "### 1.1 Download & Format Strandset"
   ]
  },
  {
   "cell_type": "code",
   "id": "c5d6e7f8a9b0c1d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T06:33:55.579068Z",
     "start_time": "2026-02-14T06:33:42.741995Z"
    }
   },
   "source": [
    "max_samples = 500 if CONFIG[\"training_scope\"] == \"quick_test\" else 0  # 0 = all\n",
    "\n",
    "print(\"Downloading & formatting Strandset-Rust-v1...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cmd = \"python scripts/20_prepare_strandset.py\"\n",
    "if max_samples:\n",
    "    cmd += f\" --max_samples {max_samples}\"\n",
    "if not CONFIG[\"include_ipo\"]:\n",
    "    cmd += \" --no-preferences\"\n",
    "\n",
    "!{cmd}\n",
    "\n",
    "drive_helper.backup(\"data/rust/strandset\", \"data/rust/strandset\")\n",
    "if DRIVE_MODE != \"local\":\n",
    "    print(\"\\nBacked up Strandset data to Drive.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading & formatting Strandset-Rust-v1...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Preparing Strandset-Rust-v1\n",
      "============================================================\n",
      "  Dataset: Fortytwo-Network/Strandset-Rust-v1\n",
      "  Output: data/rust/strandset\n",
      "  Max samples: 500\n",
      "\n",
      "Loading dataset...\n",
      "  Train split: 191,008 examples\n",
      "  Test split: 225 examples\n",
      "\n",
      "Processing 500 examples...\n",
      "\n",
      "Saving datasets...\n",
      "Saving the dataset (1/1 shards): 100% 419/419 [00:00<00:00, 227379.14 examples/s]\n",
      "  lang_rust/train: 419 examples -> data/rust/strandset/lang_rust/train\n",
      "Saving the dataset (1/1 shards): 100% 48/48 [00:00<00:00, 31729.96 examples/s]\n",
      "  core_agent/train: 48 examples -> data/rust/strandset/core_agent/train\n",
      "Saving the dataset (1/1 shards): 100% 3/3 [00:00<00:00, 1774.24 examples/s]\n",
      "  ipo/train: 3 preference pairs -> data/rust/strandset/ipo/train\n",
      "Saving the dataset (1/1 shards): 100% 180/180 [00:00<00:00, 126758.68 examples/s]\n",
      "  eval/test: 180 examples -> data/rust/strandset/eval/test\n",
      "\n",
      "============================================================\n",
      "Strandset preparation complete!\n",
      "============================================================\n",
      "  Total processed: 500\n",
      "  Lang adapter:    419\n",
      "  Core agent:      48 (debug=3, review=45)\n",
      "  IPO pairs:       3\n",
      "  Skipped (empty): 33\n",
      "  Skipped (parse): 0\n",
      "\n",
      "  Category breakdown:\n",
      "    code_generation: 92\n",
      "    docstring_generation: 70\n",
      "    comment_generation: 65\n",
      "    code_summarization: 60\n",
      "    code_explanation: 48\n",
      "    variable_naming: 47\n",
      "    function_naming: 46\n",
      "    code_refactoring: 41\n",
      "    code_review: 17\n",
      "    code_optimization: 11\n",
      "    bug_detection: 3\n",
      "\n",
      "  Stats: data/rust/strandset/stats.json\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"storageQuotaExceeded\"\n",
      "WARNING:scripts.pipeline_lib.drive_utils:Drive backup skipped ‚Äî service account has no storage quota on personal My Drive. To enable backups, move your folder to a Shared Drive (requires Google Workspace). Training data is safe on the local VM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backed up Strandset data to Drive.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f8a9b0c1d2e3",
   "metadata": {},
   "source": [
    "### 1.2 Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "e7f8a9b0c1d2e3f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T06:33:56.129219Z",
     "start_time": "2026-02-14T06:33:55.930443Z"
    }
   },
   "source": [
    "data_checks = [\n",
    "    (\"Strandset lang_rust\", \"data/rust/strandset/lang_rust/train\"),\n",
    "    (\"Strandset core_agent\", \"data/rust/strandset/core_agent/train\"),\n",
    "    (\"Strandset IPO\", \"data/rust/strandset/ipo/train\"),\n",
    "    (\"Strandset eval\", \"data/rust/strandset/eval/test\"),\n",
    "    (\"Stats\", \"data/rust/strandset/stats.json\"),\n",
    "]\n",
    "\n",
    "print(\"Data Verification:\")\n",
    "print(\"=\" * 60)\n",
    "for name, path in data_checks:\n",
    "    exists = os.path.exists(path)\n",
    "    if exists and os.path.isdir(path):\n",
    "        items = os.listdir(path)\n",
    "        print(f\"  \\u2713 {name}: {path} ({len(items)} items)\")\n",
    "    elif exists:\n",
    "        size_kb = os.path.getsize(path) / 1024\n",
    "        print(f\"  \\u2713 {name}: {path} ({size_kb:.1f} KB)\")\n",
    "    else:\n",
    "        needed = True\n",
    "        if not CONFIG[\"include_ipo\"] and \"IPO\" in name:\n",
    "            needed = False\n",
    "        if CONFIG[\"training_scope\"] == \"lang_adapter_only\" and name in (\"Strandset core_agent\", \"Strandset IPO\"):\n",
    "            needed = False\n",
    "        sym = \"\\u2717\" if needed else \"\\u2014\"\n",
    "        label = \"MISSING\" if needed else \"not needed\"\n",
    "        print(f\"  {sym} {name}: {label}\")\n",
    "\n",
    "# Show stats if available\n",
    "stats_path = \"data/rust/strandset/stats.json\"\n",
    "if os.path.exists(stats_path):\n",
    "    import json\n",
    "    with open(stats_path) as f:\n",
    "        stats = json.load(f)\n",
    "    print(f\"\\n  Total processed: {stats.get('total_processed', '?'):,}\")\n",
    "    print(f\"  Lang adapter: {stats.get('lang_rust', '?'):,}\")\n",
    "    print(f\"  Core agent: {stats.get('core_agent_debug', 0) + stats.get('core_agent_review', 0):,}\")\n",
    "    print(f\"  IPO pairs: {stats.get('ipo', '?'):,}\")\n",
    "print(\"=\" * 60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Verification:\n",
      "============================================================\n",
      "  ‚úì Strandset lang_rust: data/rust/strandset/lang_rust/train (3 items)\n",
      "  ‚úì Strandset core_agent: data/rust/strandset/core_agent/train (3 items)\n",
      "  ‚úì Strandset IPO: data/rust/strandset/ipo/train (3 items)\n",
      "  ‚úì Strandset eval: data/rust/strandset/eval/test (3 items)\n",
      "  ‚úì Stats: data/rust/strandset/stats.json (0.5 KB)\n",
      "\n",
      "  Total processed: 500\n",
      "  Lang adapter: 419\n",
      "  Core agent: 48\n",
      "  IPO pairs: 3\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "f8a9b0c1d2e3f4a5",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Lang Adapter Training\n",
    "\n",
    "Train a QLoRA adapter (rank 64) to specialise GPT-OSS 20B on Rust syntax, stdlib, and idioms.\n",
    "Uses Strandset's code_generation, code_completion, docstring, comment, and naming examples.\n",
    "Then merge the adapter into the base weights for downstream training.\n",
    "\n",
    "**Split LoRA** backend auto-enabled for 7-12x faster MoE training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0c1d2e3f4a5b6",
   "metadata": {},
   "source": [
    "### 2.1 Train lang_rust Adapter"
   ]
  },
  {
   "cell_type": "code",
   "id": "b0c1d2e3f4a5b6c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T06:51:49.334538Z",
     "start_time": "2026-02-14T06:33:56.139588Z"
    }
   },
   "source": [
    "batch = CONFIG[\"lang_rust_batch\"]\n",
    "grad_accum = CONFIG[\"lang_rust_grad_accum\"]\n",
    "max_steps = CONFIG[\"lang_rust_max_steps\"]\n",
    "seq_len = CONFIG[\"lang_rust_seq_len\"]\n",
    "\n",
    "cmd = f\"python scripts/13_train_lang_adapter.py\"\n",
    "cmd += f\" --train_data_path data/rust/strandset/lang_rust/train\"\n",
    "cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "print(f\"Training lang_rust adapter...\")\n",
    "print(f\"  Data: data/rust/strandset/lang_rust/train\")\n",
    "print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "print(f\"  Max steps: {max_steps}\")\n",
    "print(f\"  Seq length: {seq_len} (from config)\")\n",
    "print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "!{cmd}\n",
    "\n",
    "drive_helper.backup(\"checkpoints/lang_rust\", \"checkpoints/lang_rust\")\n",
    "if DRIVE_MODE != \"local\":\n",
    "    print(\"\\nCheckpoint backed up to Drive.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lang_rust adapter...\n",
      "  Data: data/rust/strandset/lang_rust/train\n",
      "  Batch: 2 x 4 = 8\n",
      "  Max steps: 100\n",
      "  Seq length: 8192 (from config)\n",
      "  Split LoRA backend: grouped_mm\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Training Language Adapter: gpt-oss-20b-lang-rust-v1\n",
      "============================================================\n",
      "\n",
      "Loading model: openai/gpt-oss-20b\n",
      "/content/llm-training-pipeline/scripts/pipeline_lib/unsloth_utils.py:38: UserWarning: WARNING: Unsloth should be imported before [transformers] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n",
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "2026-02-14 06:34:00.256639: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1771050840.272669    9535 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1771050840.277973    9535 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1771050840.291557    9535 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771050840.291580    9535 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771050840.291584    9535 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771050840.291586    9535 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "‚öôÔ∏è  Running in WANDB offline mode\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2026.2.1: Fast Gpt_Oss patching. Transformers: 4.57.6. vLLM: 0.15.1.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.179 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Loading checkpoint shards: 100% 4/4 [00:03<00:00,  1.13it/s]\n",
      "\n",
      "Applying LoRA configuration...\n",
      "Unsloth: Detected MoE model with num_experts = 32 and target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']. Enabling LoRA on MoE parameters: ['mlp.experts.gate_up_proj', 'mlp.experts.down_proj']\n",
      "Unsloth: PEFT set target_parameters but found no matching parameters.\n",
      "This is expected for MoE models - Unsloth handles MoE expert LoRA targeting separately.\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "Trainable parameters: 31,850,496 / 11,072,953,920 (0.29%)\n",
      "\n",
      "Loading training data: data/rust/strandset/lang_rust/train\n",
      "  Training examples: 419\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=30): 100% 419/419 [00:08<00:00, 51.66 examples/s]\n",
      "\n",
      "Starting training...\n",
      "  Output: checkpoints/lang_rust\n",
      "  Epochs: 1\n",
      "  Max steps: 100\n",
      "  LR: 2e-05\n",
      "  Effective batch: 8\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 419 | Num Epochs = 2 | Total steps = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 31,850,496 of 20,946,607,680 (0.15% trained)\n",
      "  0% 0/100 [00:00<?, ?it/s]Unsloth: Will smartly offload gradients to save VRAM!\n",
      "{'loss': 2.4715, 'grad_norm': 5.41543436050415, 'learning_rate': 1.9912640693269754e-05, 'epoch': 0.19}\n",
      "{'loss': 1.4894, 'grad_norm': 2.70292592048645, 'learning_rate': 1.8947293298207637e-05, 'epoch': 0.38}\n",
      "{'loss': 1.2033, 'grad_norm': 3.3677244186401367, 'learning_rate': 1.7012367842724887e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0324, 'grad_norm': 3.5627381801605225, 'learning_rate': 1.4317543523384928e-05, 'epoch': 0.76}\n",
      "{'loss': 0.9155, 'grad_norm': 2.118595600128174, 'learning_rate': 1.1154846369695864e-05, 'epoch': 0.95}\n",
      "{'loss': 0.8628, 'grad_norm': 1.8189305067062378, 'learning_rate': 7.867003692562533e-06, 'epoch': 1.13}\n",
      "{'loss': 0.8288, 'grad_norm': 3.247483730316162, 'learning_rate': 4.8103042621878515e-06, 'epoch': 1.32}\n",
      "{'loss': 0.7834, 'grad_norm': 2.5645670890808105, 'learning_rate': 2.315988891431412e-06, 'epoch': 1.51}\n",
      "{'loss': 0.7747, 'grad_norm': 2.9281651973724365, 'learning_rate': 6.543553540053926e-07, 'epoch': 1.7}\n",
      "{'loss': 0.77, 'grad_norm': 2.1071248054504395, 'learning_rate': 5.467426590739511e-09, 'epoch': 1.9}\n",
      "{'train_runtime': 1005.2877, 'train_samples_per_second': 0.796, 'train_steps_per_second': 0.099, 'train_loss': 1.113185977935791, 'epoch': 1.9}\n",
      "100% 100/100 [16:45<00:00, 10.05s/it]\n",
      "Saved lora to checkpoints/lang_rust/final\n",
      "\n",
      "Training complete!\n",
      "  Adapter saved to: checkpoints/lang_rust/final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"storageQuotaExceeded\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint backed up to Drive.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4a5b6c7d8",
   "metadata": {},
   "source": [
    "### 2.2 Merge lang_rust into Base"
   ]
  },
  {
   "cell_type": "code",
   "id": "d2e3f4a5b6c7d8e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T06:52:37.195273Z",
     "start_time": "2026-02-14T06:51:49.540254Z"
    }
   },
   "source": [
    "print(\"Merging lang_rust adapter into base model...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "!python scripts/19_merge_adapter.py \\\n",
    "    --adapter_path checkpoints/lang_rust/final \\\n",
    "    --output_dir checkpoints/gpt-oss-20b-rust-merged \\\n",
    "    --export_formats hf\n",
    "\n",
    "drive_helper.backup(\"checkpoints/gpt-oss-20b-rust-merged\", \"checkpoints/gpt-oss-20b-rust-merged\")\n",
    "if DRIVE_MODE != \"local\":\n",
    "    print(\"\\nMerged model backed up to Drive.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging lang_rust adapter into base model...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Merging Adapter: merge-lang-rust\n",
      "============================================================\n",
      "  Base model: openai/gpt-oss-20b\n",
      "  Adapter: checkpoints/lang_rust/final\n",
      "  Output: checkpoints/gpt-oss-20b-rust-merged\n",
      "  Formats: ['hf']\n",
      "/content/llm-training-pipeline/scripts/pipeline_lib/unsloth_utils.py:138: UserWarning: WARNING: Unsloth should be imported before [transformers] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n",
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "2026-02-14 06:51:53.585545: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1771051913.602638   16115 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1771051913.608101   16115 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1771051913.621863   16115 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771051913.621888   16115 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771051913.621892   16115 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771051913.621895   16115 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2026.2.1: Fast Gpt_Oss patching. Transformers: 4.57.6. vLLM: 0.15.1.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.179 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Loading checkpoint shards: 100% 4/4 [00:03<00:00,  1.15it/s]\n",
      "Unsloth: PEFT set target_parameters but found no matching parameters.\n",
      "This is expected for MoE models - Unsloth handles MoE expert LoRA targeting separately.\n",
      "/usr/local/lib/python3.12/dist-packages/unsloth_zoo/saving_utils.py:1678: UserWarning: Model is not a PeftModel (no Lora adapters detected). Skipping Merge. Please use save_pretrained() or push_to_hub() instead!\n",
      "  warnings.warn(\"Model is not a PeftModel (no Lora adapters detected). Skipping Merge. Please use save_pretrained() or push_to_hub() instead!\")\n",
      "Exported HuggingFace format to checkpoints/gpt-oss-20b-rust-merged/hf\n",
      "\n",
      "Running smoke test...\n",
      "  Smoke test FAILED: Unsloth: No config file found - are you sure the `model_name` is correct?\n",
      "If you're using a model on your local device, confirm if the folder location exists.\n",
      "If you're using a HuggingFace online model, check if it exists.\n",
      "\n",
      "============================================================\n",
      "Merge complete!\n",
      "  Output: checkpoints/gpt-oss-20b-rust-merged\n",
      "============================================================\n",
      "\n",
      "Merged model backed up to Drive.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "e3f4a5b6c7d8e9f0",
   "metadata": {},
   "source": [
    "### 2.3 Verify Merge"
   ]
  },
  {
   "cell_type": "code",
   "id": "f4a5b6c7d8e9f0a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T06:52:37.611176Z",
     "start_time": "2026-02-14T06:52:37.399970Z"
    }
   },
   "source": [
    "merged_path = \"checkpoints/gpt-oss-20b-rust-merged\"\n",
    "adapter_path = \"checkpoints/lang_rust/final\"\n",
    "\n",
    "print(\"Merge Verification:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if os.path.exists(merged_path):\n",
    "    files = os.listdir(merged_path)\n",
    "    safetensors = [f for f in files if f.endswith(\".safetensors\")]\n",
    "    print(f\"  \\u2713 Merged model: {merged_path}\")\n",
    "    print(f\"    {len(safetensors)} safetensors shard(s), {len(files)} total files\")\n",
    "else:\n",
    "    print(f\"  \\u2717 Merged model not found at {merged_path}\")\n",
    "\n",
    "if os.path.exists(adapter_path):\n",
    "    adapter_files = os.listdir(adapter_path)\n",
    "    print(f\"  \\u2713 Adapter: {adapter_path} ({len(adapter_files)} files)\")\n",
    "else:\n",
    "    print(f\"  \\u2717 Adapter not found at {adapter_path}\")\n",
    "\n",
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"\\n\\u2713 lang_adapter_only scope complete. Stopping here.\")\n",
    "\n",
    "print(\"=\" * 60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge Verification:\n",
      "============================================================\n",
      "  ‚úì Merged model: checkpoints/gpt-oss-20b-rust-merged\n",
      "    0 safetensors shard(s), 0 total files\n",
      "  ‚úì Adapter: checkpoints/lang_rust/final (7 files)\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "a5b6c7d8e9f0a1b2",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Core Agent SFT\n",
    "\n",
    "Train a higher-rank LoRA adapter (rank 128) on Strandset's debug/review examples.\n",
    "Uses the merged lang_rust model as the base.\n",
    "\n",
    "**Split LoRA** + **Auto packing** (3x faster, zero-config)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c7d8e9f0a1b2c3",
   "metadata": {},
   "source": [
    "### 3.1 Train core_agent Adapter"
   ]
  },
  {
   "cell_type": "code",
   "id": "c7d8e9f0a1b2c3d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T07:09:15.109169Z",
     "start_time": "2026-02-14T06:52:37.613642Z"
    }
   },
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    batch = CONFIG[\"core_agent_batch\"]\n",
    "    grad_accum = CONFIG[\"core_agent_grad_accum\"]\n",
    "    max_steps = CONFIG[\"core_agent_max_steps\"]\n",
    "    seq_len = CONFIG[\"core_agent_seq_len\"]\n",
    "\n",
    "    cmd = f\"python scripts/14_train_core_agent.py\"\n",
    "    cmd += f\" --train_data_path data/rust/strandset/core_agent/train\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    print(f\"Training core_agent adapter...\")\n",
    "    print(f\"  Data: data/rust/strandset/core_agent/train\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Seq length: {seq_len} (from config)\")\n",
    "    print(f\"  LoRA rank: 128\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(f\"  Auto packing: enabled (uncontaminated)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/core_agent\", \"checkpoints/core_agent\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training core_agent adapter...\n",
      "  Data: data/rust/strandset/core_agent/train\n",
      "  Batch: 1 x 4 = 4\n",
      "  Max steps: 100\n",
      "  Seq length: 16384 (from config)\n",
      "  LoRA rank: 128\n",
      "  Split LoRA backend: grouped_mm\n",
      "  Auto packing: enabled (uncontaminated)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Training Core Agent: gpt-oss-20b-core-agent-v1\n",
      "============================================================\n",
      "\n",
      "Merging lang_rust adapter from checkpoints/lang_rust...\n",
      "  (Run scripts/19_merge_adapter.py first, or use --no-merge-lang-adapter)\n",
      "  Falling back to base model: openai/gpt-oss-20b\n",
      "\n",
      "Loading model: openai/gpt-oss-20b\n",
      "/content/llm-training-pipeline/scripts/pipeline_lib/unsloth_utils.py:38: UserWarning: WARNING: Unsloth should be imported before [transformers] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n",
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "2026-02-14 06:52:41.715951: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1771051961.731595   16584 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1771051961.736967   16584 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1771051961.749721   16584 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771051961.749745   16584 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771051961.749749   16584 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771051961.749750   16584 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "‚öôÔ∏è  Running in WANDB offline mode\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2026.2.1: Fast Gpt_Oss patching. Transformers: 4.57.6. vLLM: 0.15.1.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.179 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Loading checkpoint shards: 100% 4/4 [00:03<00:00,  1.15it/s]\n",
      "\n",
      "Applying LoRA (rank=128)...\n",
      "Unsloth: Detected MoE model with num_experts = 32 and target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']. Enabling LoRA on MoE parameters: ['mlp.experts.gate_up_proj', 'mlp.experts.down_proj']\n",
      "Unsloth: PEFT set target_parameters but found no matching parameters.\n",
      "This is expected for MoE models - Unsloth handles MoE expert LoRA targeting separately.\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "Trainable parameters: 63,700,992 / 11,104,804,416 (0.57%)\n",
      "\n",
      "Loading training data: data/rust/strandset/core_agent/train\n",
      "  Training examples: 48\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=30): 100% 48/48 [00:08<00:00,  5.77 examples/s]\n",
      "\n",
      "Starting training...\n",
      "  Output: checkpoints/core_agent\n",
      "  Epochs: 2\n",
      "  LR: 3e-05\n",
      "  Max seq length: 16384\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 48 | Num Epochs = 9 | Total steps = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 63,700,992 of 20,978,458,176 (0.30% trained)\n",
      "  1% 1/100 [00:53<1:27:27, 53.00s/it]Unsloth: Will smartly offload gradients to save VRAM!\n",
      "{'loss': 2.569, 'grad_norm': 14.853002548217773, 'learning_rate': 2.4e-05, 'epoch': 0.42}\n",
      "{'loss': 1.6648, 'grad_norm': 6.485804080963135, 'learning_rate': 2.9868961039904628e-05, 'epoch': 0.83}\n",
      "{'loss': 1.4102, 'grad_norm': 4.81356954574585, 'learning_rate': 2.9340536723015367e-05, 'epoch': 1.25}\n",
      "{'loss': 1.0183, 'grad_norm': 5.037090301513672, 'learning_rate': 2.8420939947311454e-05, 'epoch': 1.67}\n",
      "{'loss': 1.0504, 'grad_norm': 4.7158660888671875, 'learning_rate': 2.7135254915624213e-05, 'epoch': 2.08}\n",
      "{'loss': 0.8122, 'grad_norm': 3.043581008911133, 'learning_rate': 2.5518551764087326e-05, 'epoch': 2.5}\n",
      "{'loss': 0.7813, 'grad_norm': 3.1981608867645264, 'learning_rate': 2.3614929940244155e-05, 'epoch': 2.92}\n",
      "{'loss': 0.6561, 'grad_norm': 2.7986111640930176, 'learning_rate': 2.1476315285077393e-05, 'epoch': 3.33}\n",
      "{'loss': 0.6923, 'grad_norm': 4.827965259552002, 'learning_rate': 1.916104363142767e-05, 'epoch': 3.75}\n",
      "{'loss': 0.6305, 'grad_norm': 3.6427109241485596, 'learning_rate': 1.6732269554543794e-05, 'epoch': 4.17}\n",
      "{'loss': 0.601, 'grad_norm': 3.9166266918182373, 'learning_rate': 1.4256243679901665e-05, 'epoch': 4.58}\n",
      "{'loss': 0.5706, 'grad_norm': 5.767577171325684, 'learning_rate': 1.18005055388438e-05, 'epoch': 5.0}\n",
      "{'loss': 0.4661, 'grad_norm': 3.3268165588378906, 'learning_rate': 9.432041266226686e-06, 'epoch': 5.42}\n",
      "{'loss': 0.5233, 'grad_norm': 5.387261390686035, 'learning_rate': 7.215456393281777e-06, 'epoch': 5.83}\n",
      "{'loss': 0.4461, 'grad_norm': 4.427010536193848, 'learning_rate': 5.21121357713747e-06, 'epoch': 6.25}\n",
      "{'loss': 0.4068, 'grad_norm': 6.017603397369385, 'learning_rate': 3.473983337147118e-06, 'epoch': 6.67}\n",
      "{'loss': 0.4574, 'grad_norm': 5.143415927886963, 'learning_rate': 2.0511527856363916e-06, 'epoch': 7.08}\n",
      "{'loss': 0.4434, 'grad_norm': 3.647566318511963, 'learning_rate': 9.815330310080889e-07, 'epoch': 7.5}\n",
      "{'loss': 0.3793, 'grad_norm': 3.858246326446533, 'learning_rate': 2.943005118778597e-07, 'epoch': 7.92}\n",
      "{'loss': 0.3797, 'grad_norm': 3.7062745094299316, 'learning_rate': 8.201139886109265e-09, 'epoch': 8.33}\n",
      "{'train_runtime': 930.0078, 'train_samples_per_second': 0.43, 'train_steps_per_second': 0.108, 'train_loss': 0.7979407167434692, 'epoch': 8.33}\n",
      "100% 100/100 [15:29<00:00,  9.30s/it]\n",
      "Saved lora to checkpoints/core_agent/final\n",
      "\n",
      "Training complete!\n",
      "  Core agent adapter saved to: checkpoints/core_agent/final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"storageQuotaExceeded\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint backed up to Drive.\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "d8e9f0a1b2c3d4e5",
   "metadata": {},
   "source": [
    "### 3.2 Verify core_agent"
   ]
  },
  {
   "cell_type": "code",
   "id": "e9f0a1b2c3d4e5f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T07:09:15.559526Z",
     "start_time": "2026-02-14T07:09:15.319647Z"
    }
   },
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    ckpt_path = \"checkpoints/core_agent/final\"\n",
    "\n",
    "    print(\"Core Agent Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(ckpt_path):\n",
    "        files = os.listdir(ckpt_path)\n",
    "        print(f\"  \\u2713 Checkpoint: {ckpt_path} ({len(files)} files)\")\n",
    "\n",
    "        adapter_config = os.path.join(ckpt_path, \"adapter_config.json\")\n",
    "        if os.path.exists(adapter_config):\n",
    "            import json\n",
    "            with open(adapter_config) as f:\n",
    "                cfg = json.load(f)\n",
    "            print(f\"    LoRA rank: {cfg.get('r', '?')}\")\n",
    "            print(f\"    Alpha: {cfg.get('lora_alpha', '?')}\")\n",
    "            print(f\"    Target modules: {cfg.get('target_modules', '?')}\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 Checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core Agent Verification:\n",
      "============================================================\n",
      "  ‚úì Checkpoint: checkpoints/core_agent/final (7 files)\n",
      "    LoRA rank: 128\n",
      "    Alpha: 256\n",
      "    Target modules: ['gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'k_proj', 'down_proj']\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "f0a1b2c3d4e5f6a7",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: IPO Preference Training (Optional)\n",
    "\n",
    "Train with Identity Preference Optimisation on synthetic preference pairs\n",
    "from Strandset's bug_detection category (fixed=chosen, buggy=rejected).\n",
    "\n",
    "Very low learning rate (5e-7), 1 epoch only to avoid collapse.\n",
    "\n",
    "Set `include_ipo=False` in Step 0.3 to skip."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4e5f6a7b9",
   "metadata": {},
   "source": [
    "### 4.1 Train with IPO"
   ]
  },
  {
   "cell_type": "code",
   "id": "b2c3d4e5f6a7b9c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T07:26:16.643720Z",
     "start_time": "2026-02-14T07:09:15.567871Z"
    }
   },
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "elif not CONFIG[\"include_ipo\"]:\n",
    "    print(\"Skipping \\u2014 IPO disabled (include_ipo=False)\")\n",
    "else:\n",
    "    batch = CONFIG[\"ipo_batch\"]\n",
    "    grad_accum = CONFIG[\"ipo_grad_accum\"]\n",
    "    max_steps = CONFIG[\"ipo_max_steps\"]\n",
    "\n",
    "    ipo_checkpoint = \"checkpoints/core_agent/final\"\n",
    "\n",
    "    cmd = f\"python scripts/17_ipo_preference.py\"\n",
    "    cmd += f\" --checkpoint {ipo_checkpoint}\"\n",
    "    cmd += f\" --train_data_path data/rust/strandset/ipo/train\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    print(f\"Training with IPO (synthetic preferences)...\")\n",
    "    print(f\"  Checkpoint: {ipo_checkpoint}\")\n",
    "    print(f\"  Data: data/rust/strandset/ipo/train\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Loss: IPO (beta=0.1)\")\n",
    "    print(f\"  Load mode: {CONFIG['load_mode']}\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/core_agent_ipo\", \"checkpoints/core_agent_ipo\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with IPO (synthetic preferences)...\n",
      "  Checkpoint: checkpoints/core_agent/final\n",
      "  Data: data/rust/strandset/ipo/train\n",
      "  Batch: 1 x 16 = 16\n",
      "  Max steps: 100\n",
      "  Loss: IPO (beta=0.1)\n",
      "  Load mode: fp8\n",
      "  Split LoRA backend: grouped_mm\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "IPO Preference Training: gpt-oss-20b-ipo-v1\n",
      "============================================================\n",
      "\n",
      "Loading model from: checkpoints/core_agent/final\n",
      "/content/llm-training-pipeline/scripts/pipeline_lib/unsloth_utils.py:38: UserWarning: WARNING: Unsloth should be imported before [transformers] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n",
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "2026-02-14 07:09:19.826891: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1771052959.843711   21738 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1771052959.848935   21738 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1771052959.861891   21738 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771052959.861914   21738 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771052959.861918   21738 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771052959.861920   21738 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "‚öôÔ∏è  Running in WANDB offline mode\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2026.2.1: Fast Gpt_Oss patching. Transformers: 4.57.6. vLLM: 0.15.1.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.179 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Loading checkpoint shards: 100% 4/4 [00:03<00:00,  1.14it/s]\n",
      "Unsloth: PEFT set target_parameters but found no matching parameters.\n",
      "This is expected for MoE models - Unsloth handles MoE expert LoRA targeting separately.\n",
      "\n",
      "Loading preference data: data/rust/strandset/ipo/train\n",
      "  Training pairs: 3\n",
      "num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
      "[datasets.arrow_dataset|WARNING]num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
      "Extracting prompt in train dataset (num_proc=3): 100% 3/3 [00:00<00:00, 11.52 examples/s]\n",
      "num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
      "[datasets.arrow_dataset|WARNING]num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
      "Applying chat template to train dataset (num_proc=3): 100% 3/3 [00:01<00:00,  1.75 examples/s]\n",
      "num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
      "[datasets.arrow_dataset|WARNING]num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
      "Tokenizing train dataset (num_proc=3): 100% 3/3 [00:01<00:00,  1.73 examples/s]\n",
      "\n",
      "Starting IPO training...\n",
      "  Output: checkpoints/core_agent_ipo\n",
      "  Loss type: ipo\n",
      "  Beta: 0.1\n",
      "  LR: 5e-07\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 3 | Num Epochs = 100 | Total steps = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 16\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 16 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 63,700,992 of 20,978,458,176 (0.30% trained)\n",
      "  0% 0/100 [00:00<?, ?it/s]Unsloth: Will smartly offload gradients to save VRAM!\n",
      "{'loss': 24.778, 'grad_norm': 48.12209701538086, 'learning_rate': 2e-07, 'rewards/chosen': 0.11443879455327988, 'rewards/rejected': 0.11220554262399673, 'rewards/accuracies': 0.6666666865348816, 'rewards/margins': 0.0022332514636218548, 'logps/chosen': -0.7636227011680603, 'logps/rejected': -0.792881429195404, 'logits/chosen': -3.653944253921509, 'logits/rejected': -3.6249804496765137, 'epoch': 5.0}\n",
      "{'loss': 24.6997, 'grad_norm': 45.49223327636719, 'learning_rate': 4.5e-07, 'rewards/chosen': 0.11526422947645187, 'rewards/rejected': 0.11223903298377991, 'rewards/accuracies': 0.6666666865348816, 'rewards/margins': 0.003025212325155735, 'logps/chosen': -0.7553682327270508, 'logps/rejected': -0.7925466299057007, 'logits/chosen': -3.6580495834350586, 'logits/rejected': -3.6286516189575195, 'epoch': 10.0}\n",
      "{'loss': 24.5109, 'grad_norm': 38.51942443847656, 'learning_rate': 4.975670171853925e-07, 'rewards/chosen': 0.11556880921125412, 'rewards/rejected': 0.1106315478682518, 'rewards/accuracies': 0.6666666865348816, 'rewards/margins': 0.004937270190566778, 'logps/chosen': -0.7523225545883179, 'logps/rejected': -0.8086214661598206, 'logits/chosen': -3.6657192707061768, 'logits/rejected': -3.6343495845794678, 'epoch': 15.0}\n",
      "{'loss': 24.3501, 'grad_norm': 37.731414794921875, 'learning_rate': 4.877641290737883e-07, 'rewards/chosen': 0.11491182446479797, 'rewards/rejected': 0.10833845287561417, 'rewards/accuracies': 0.6666666865348816, 'rewards/margins': 0.006573379505425692, 'logps/chosen': -0.7588924169540405, 'logps/rejected': -0.8315524458885193, 'logits/chosen': -3.671444892883301, 'logits/rejected': -3.6395020484924316, 'epoch': 20.0}\n",
      "{'loss': 24.203, 'grad_norm': 39.64164733886719, 'learning_rate': 4.707368982147317e-07, 'rewards/chosen': 0.11382313072681427, 'rewards/rejected': 0.10574142634868622, 'rewards/accuracies': 0.7333333492279053, 'rewards/margins': 0.00808169599622488, 'logps/chosen': -0.7697793841362, 'logps/rejected': -0.8575225472450256, 'logits/chosen': -3.6787633895874023, 'logits/rejected': -3.6463825702667236, 'epoch': 25.0}\n",
      "{'loss': 24.0583, 'grad_norm': 42.77739715576172, 'learning_rate': 4.470026884016804e-07, 'rewards/chosen': 0.11285267025232315, 'rewards/rejected': 0.10328076779842377, 'rewards/accuracies': 0.9333333373069763, 'rewards/margins': 0.009571903385221958, 'logps/chosen': -0.77948397397995, 'logps/rejected': -0.8821292519569397, 'logits/chosen': -3.692563056945801, 'logits/rejected': -3.6592283248901367, 'epoch': 30.0}\n",
      "{'loss': 23.8828, 'grad_norm': 49.879817962646484, 'learning_rate': 4.172826515897145e-07, 'rewards/chosen': 0.11340737342834473, 'rewards/rejected': 0.10202423483133316, 'rewards/accuracies': 1.0, 'rewards/margins': 0.011383134871721268, 'logps/chosen': -0.7739370465278625, 'logps/rejected': -0.894694447517395, 'logits/chosen': -3.703061819076538, 'logits/rejected': -3.6694133281707764, 'epoch': 35.0}\n",
      "{'loss': 23.7207, 'grad_norm': 49.16166687011719, 'learning_rate': 3.824798160583012e-07, 'rewards/chosen': 0.1132907047867775, 'rewards/rejected': 0.1002245619893074, 'rewards/accuracies': 1.0, 'rewards/margins': 0.013066140934824944, 'logps/chosen': -0.7751035094261169, 'logps/rejected': -0.9126913547515869, 'logits/chosen': -3.707141160964966, 'logits/rejected': -3.6737101078033447, 'epoch': 40.0}\n",
      "{'loss': 23.5271, 'grad_norm': 51.857330322265625, 'learning_rate': 3.43651648353978e-07, 'rewards/chosen': 0.11236143112182617, 'rewards/rejected': 0.09726607799530029, 'rewards/accuracies': 1.0, 'rewards/margins': 0.015095354989171028, 'logps/chosen': -0.7843963503837585, 'logps/rejected': -0.9422762989997864, 'logits/chosen': -3.717926025390625, 'logits/rejected': -3.6843926906585693, 'epoch': 45.0}\n",
      "{'loss': 23.3934, 'grad_norm': 53.33951187133789, 'learning_rate': 3.0197792270443976e-07, 'rewards/chosen': 0.11281042546033859, 'rewards/rejected': 0.09631168097257614, 'rewards/accuracies': 1.0, 'rewards/margins': 0.016498753800988197, 'logps/chosen': -0.7799063920974731, 'logps/rejected': -0.9518201351165771, 'logits/chosen': -3.715813159942627, 'logits/rejected': -3.683114528656006, 'epoch': 50.0}\n",
      "{'loss': 23.2656, 'grad_norm': 55.65867233276367, 'learning_rate': 2.5872487417562527e-07, 'rewards/chosen': 0.11252855509519577, 'rewards/rejected': 0.09467799961566925, 'rewards/accuracies': 1.0, 'rewards/margins': 0.01785055547952652, 'logps/chosen': -0.7827250957489014, 'logps/rejected': -0.9681569337844849, 'logits/chosen': -3.7188994884490967, 'logits/rejected': -3.685267448425293, 'epoch': 55.0}\n",
      "{'loss': 23.1668, 'grad_norm': 55.332401275634766, 'learning_rate': 2.152067247599837e-07, 'rewards/chosen': 0.11155980080366135, 'rewards/rejected': 0.09266476333141327, 'rewards/accuracies': 1.0, 'rewards/margins': 0.01889505609869957, 'logps/chosen': -0.7924125790596008, 'logps/rejected': -0.9882894158363342, 'logits/chosen': -3.725660800933838, 'logits/rejected': -3.692019462585449, 'epoch': 60.0}\n",
      "{'loss': 23.0727, 'grad_norm': 60.726444244384766, 'learning_rate': 1.7274575140626315e-07, 'rewards/chosen': 0.11129535734653473, 'rewards/rejected': 0.09139233082532883, 'rewards/accuracies': 1.0, 'rewards/margins': 0.019903020933270454, 'logps/chosen': -0.7950571775436401, 'logps/rejected': -1.0010135173797607, 'logits/chosen': -3.722578763961792, 'logits/rejected': -3.6894586086273193, 'epoch': 65.0}\n",
      "{'loss': 22.997, 'grad_norm': 54.594120025634766, 'learning_rate': 1.3263210930352737e-07, 'rewards/chosen': 0.11124567687511444, 'rewards/rejected': 0.09052767604589462, 'rewards/accuracies': 1.0, 'rewards/margins': 0.020718002691864967, 'logps/chosen': -0.7955537438392639, 'logps/rejected': -1.009660005569458, 'logits/chosen': -3.725682497024536, 'logits/rejected': -3.6933743953704834, 'epoch': 70.0}\n",
      "{'loss': 22.931, 'grad_norm': 56.73577880859375, 'learning_rate': 9.608463116858542e-08, 'rewards/chosen': 0.11088664829730988, 'rewards/rejected': 0.08946139365434647, 'rewards/accuracies': 1.0, 'rewards/margins': 0.021425243467092514, 'logps/chosen': -0.7991441488265991, 'logps/rejected': -1.0203230381011963, 'logits/chosen': -3.7261056900024414, 'logits/rejected': -3.6940653324127197, 'epoch': 75.0}\n",
      "{'loss': 22.8715, 'grad_norm': 59.249122619628906, 'learning_rate': 6.42137936306514e-08, 'rewards/chosen': 0.11143377423286438, 'rewards/rejected': 0.08936727792024612, 'rewards/accuracies': 1.0, 'rewards/margins': 0.022066505625844002, 'logps/chosen': -0.7936728596687317, 'logps/rejected': -1.0212641954421997, 'logits/chosen': -3.7150838375091553, 'logits/rejected': -3.682264566421509, 'epoch': 80.0}\n",
      "{'loss': 22.8688, 'grad_norm': 58.16802978515625, 'learning_rate': 3.798797596089351e-08, 'rewards/chosen': 0.11013249307870865, 'rewards/rejected': 0.0880301222205162, 'rewards/accuracies': 1.0, 'rewards/margins': 0.022102374583482742, 'logps/chosen': -0.806685745716095, 'logps/rejected': -1.0346356630325317, 'logits/chosen': -3.7286343574523926, 'logits/rejected': -3.6964094638824463, 'epoch': 85.0}\n",
      "{'loss': 22.8651, 'grad_norm': 58.577796936035156, 'learning_rate': 1.8204036358303172e-08, 'rewards/chosen': 0.1104787141084671, 'rewards/rejected': 0.08834698796272278, 'rewards/accuracies': 1.0, 'rewards/margins': 0.022131728008389473, 'logps/chosen': -0.8032234311103821, 'logps/rejected': -1.0314668416976929, 'logits/chosen': -3.7343592643737793, 'logits/rejected': -3.7017171382904053, 'epoch': 90.0}\n",
      "{'loss': 22.8472, 'grad_norm': 62.437286376953125, 'learning_rate': 5.463099816548577e-09, 'rewards/chosen': 0.11028178781270981, 'rewards/rejected': 0.08796131610870361, 'rewards/accuracies': 1.0, 'rewards/margins': 0.022320475429296494, 'logps/chosen': -0.8051928281784058, 'logps/rejected': -1.035323977470398, 'logits/chosen': -3.7297699451446533, 'logits/rejected': -3.6967453956604004, 'epoch': 95.0}\n",
      "{'loss': 22.8223, 'grad_norm': 61.28019714355469, 'learning_rate': 1.5229324522605947e-10, 'rewards/chosen': 0.11191325634717941, 'rewards/rejected': 0.08932823687791824, 'rewards/accuracies': 1.0, 'rewards/margins': 0.022585023194551468, 'logps/chosen': -0.7888781428337097, 'logps/rejected': -1.021654486656189, 'logits/chosen': -3.7280564308166504, 'logits/rejected': -3.6958670616149902, 'epoch': 100.0}\n",
      "{'train_runtime': 963.2, 'train_samples_per_second': 1.661, 'train_steps_per_second': 0.104, 'train_loss': 23.5415975189209, 'epoch': 100.0}\n",
      "100% 100/100 [16:03<00:00,  9.63s/it]\n",
      "Saved lora to checkpoints/core_agent_ipo/final\n",
      "\n",
      "IPO training complete!\n",
      "  Adapter saved to: checkpoints/core_agent_ipo/final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"storageQuotaExceeded\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint backed up to Drive.\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6a7b9c0d1",
   "metadata": {},
   "source": [
    "### 4.2 Verify IPO"
   ]
  },
  {
   "cell_type": "code",
   "id": "d4e5f6a7b9c0d1e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T07:26:17.129340Z",
     "start_time": "2026-02-14T07:26:16.904335Z"
    }
   },
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "elif not CONFIG[\"include_ipo\"]:\n",
    "    print(\"Skipping \\u2014 IPO disabled\")\n",
    "else:\n",
    "    ckpt_path = \"checkpoints/core_agent_ipo/final\"\n",
    "\n",
    "    print(\"IPO Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(ckpt_path):\n",
    "        files = os.listdir(ckpt_path)\n",
    "        print(f\"  \\u2713 IPO checkpoint: {ckpt_path} ({len(files)} files)\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 IPO checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "    tb_dir = \"checkpoints/core_agent_ipo\"\n",
    "    tb_files = []\n",
    "    if os.path.exists(tb_dir):\n",
    "        for root, dirs, fnames in os.walk(tb_dir):\n",
    "            for fn in fnames:\n",
    "                if fn.startswith(\"events.out.tfevents\"):\n",
    "                    tb_files.append(os.path.join(root, fn))\n",
    "    if tb_files:\n",
    "        print(f\"  \\u2713 TensorBoard logs found ({len(tb_files)} event files)\")\n",
    "        print(f\"    Monitor KL divergence: warn >0.3, abort >0.5\")\n",
    "    else:\n",
    "        print(f\"  \\u2014 No TensorBoard logs found\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPO Verification:\n",
      "============================================================\n",
      "  ‚úì IPO checkpoint: checkpoints/core_agent_ipo/final (7 files)\n",
      "  ‚úì TensorBoard logs found (1 event files)\n",
      "    Monitor KL divergence: warn >0.3, abort >0.5\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b9c0d1e2f3",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Test Model\n",
    "\n",
    "Load the trained model and generate Rust code interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b9c0d1e2f3a4",
   "metadata": {},
   "source": [
    "### 5.1 Load Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "a7b9c0d1e2f3a4b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T07:26:17.691015Z",
     "start_time": "2026-02-14T07:26:17.140503Z"
    }
   },
   "source": "from unsloth import FastLanguageModel\nimport torch\n\nCHECKPOINT_PRIORITY = [\n    \"checkpoints/core_agent_ipo/final\",\n    \"checkpoints/core_agent/final\",\n    \"checkpoints/gpt-oss-20b-rust-merged\",\n]\n\nMODEL_PATH = None\nfor path in CHECKPOINT_PRIORITY:\n    if os.path.exists(path):\n        MODEL_PATH = path\n        break\n\nif MODEL_PATH is None:\n    print(\"\\u2717 No checkpoint found. Train the model first.\")\nelse:\n    print(f\"Loading model from: {MODEL_PATH}\")\n\n    load_kwargs = {\n        \"max_seq_length\": 4096,\n        \"dtype\": torch.bfloat16,\n    }\n    if CONFIG.get(\"load_mode\") == \"fp8\" and CONFIG.get(\"use_fp8\"):\n        load_kwargs[\"load_in_4bit\"] = False\n        load_kwargs[\"load_in_fp8\"] = True\n        print(\"  Mode: FP8 (H100)\")\n    else:\n        load_kwargs[\"load_in_4bit\"] = True\n        print(\"  Mode: 4-bit QLoRA\")\n\n    if CONFIG.get(\"fast_inference\"):\n        load_kwargs[\"fast_inference\"] = True\n        print(\"  Inference: vLLM backend\")\n\n    print(\"=\" * 60)\n\n    model, tokenizer = FastLanguageModel.from_pretrained(MODEL_PATH, **load_kwargs)\n    FastLanguageModel.for_inference(model)\n\n    print(\"\\u2713 Model loaded!\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b9c0d1e2f3a4b5c6",
   "metadata": {},
   "source": [
    "### 5.2 Generate Rust Code"
   ]
  },
  {
   "cell_type": "code",
   "id": "c0d1e2f3a4b5c6d7",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"scripts\")\n",
    "from dataset_formatters.harmony import encode_harmony_messages\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    \"Write a Rust function `fn merge_sorted(a: &[i32], b: &[i32]) -> Vec<i32>` that merges two sorted slices into a single sorted vector.\",\n",
    "    \"This Rust code fails the borrow checker. Fix it:\\n```rust\\nfn main() {\\n    let mut v = vec![1, 2, 3];\\n    let first = &v[0];\\n    v.push(4);\\n    println!(\\\"{}\\\", first);\\n}\\n```\",\n",
    "    \"Write an async Rust function using tokio that fetches a URL with reqwest, retries up to 3 times on failure, and returns the response body as a String.\",\n",
    "]\n",
    "\n",
    "def generate_rust(prompt, max_tokens=1024):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted = encode_harmony_messages(\n",
    "        messages,\n",
    "        developer_instructions=\"You are a Rust programming expert. Write correct, idiomatic code.\",\n",
    "    )\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.3,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "for i, prompt in enumerate(TEST_PROMPTS, 1):\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Test {i}: {prompt[:80]}...\")\n",
    "    print(\"=\" * 60)\n",
    "    response = generate_rust(prompt)\n",
    "    print(response)\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d1e2f3a4b5c6d7e8",
   "metadata": {},
   "source": [
    "### 5.3 Custom Prompt"
   ]
  },
  {
   "cell_type": "code",
   "id": "e2f3a4b5c6d7e8f9",
   "metadata": {},
   "source": [
    "CUSTOM_PROMPT = \"Write a Rust function that reads a CSV file and returns the sum of a specified column.\"\n",
    "\n",
    "print(f\"Prompt: {CUSTOM_PROMPT}\")\n",
    "print(\"=\" * 60)\n",
    "print(generate_rust(CUSTOM_PROMPT))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f3a4b5c6d7e8f9a0",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Export\n",
    "\n",
    "Merge the final adapter and export to HuggingFace + GGUF formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5c6d7e8f9a0b1",
   "metadata": {},
   "source": [
    "### 6.1 Export to GGUF"
   ]
  },
  {
   "cell_type": "code",
   "id": "b5c6d7e8f9a0b1c2",
   "metadata": {},
   "source": [
    "ADAPTER_PRIORITY = [\n",
    "    \"checkpoints/core_agent_ipo/final\",\n",
    "    \"checkpoints/core_agent/final\",\n",
    "    \"checkpoints/lang_rust/final\",\n",
    "]\n",
    "\n",
    "adapter_path = None\n",
    "for path in ADAPTER_PRIORITY:\n",
    "    if os.path.exists(path):\n",
    "        adapter_path = path\n",
    "        break\n",
    "\n",
    "if adapter_path is None:\n",
    "    print(\"\\u2717 No adapter checkpoint found.\")\n",
    "else:\n",
    "    export_dir = \"checkpoints/gpt-oss-20b-rust-export-v3\"\n",
    "    print(f\"Exporting adapter: {adapter_path}\")\n",
    "    print(f\"Output: {export_dir}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/19_merge_adapter.py \\\n",
    "        --adapter_path {adapter_path} \\\n",
    "        --output_dir {export_dir} \\\n",
    "        --export_formats hf gguf_q4\n",
    "\n",
    "    drive_helper.backup(export_dir, \"checkpoints/gpt-oss-20b-rust-export-v3\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nExport backed up to Drive.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c6d7e8f9a0b1c2d3",
   "metadata": {},
   "source": [
    "### 6.2 Download GGUF"
   ]
  },
  {
   "cell_type": "code",
   "id": "d7e8f9a0b1c2d3e4",
   "metadata": {},
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    import glob\n",
    "\n",
    "    export_dir = \"checkpoints/gpt-oss-20b-rust-export-v3\"\n",
    "    gguf_files = glob.glob(os.path.join(export_dir, \"*.gguf\"))\n",
    "\n",
    "    if gguf_files:\n",
    "        gguf_path = gguf_files[0]\n",
    "        size_gb = os.path.getsize(gguf_path) / (1024**3)\n",
    "        print(f\"Downloading: {os.path.basename(gguf_path)} ({size_gb:.1f} GB)\")\n",
    "        files.download(gguf_path)\n",
    "    else:\n",
    "        print(\"\\u2717 No GGUF file found. Run export (6.1) first.\")\n",
    "else:\n",
    "    print(\"Download not available outside Colab.\")\n",
    "    print(\"GGUF file is at: checkpoints/gpt-oss-20b-rust-export-v3/\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e8f9a0b1c2d3e4f5",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Complete!\n",
    "\n",
    "Your GPT-OSS 20B Rust coding agent (v3 \\u2014 Strandset) is trained and ready to use.\n",
    "\n",
    "**Data source:** [Strandset-Rust-v1](https://huggingface.co/datasets/Fortytwo-Network/Strandset-Rust-v1) (191K examples, Apache 2.0)\n",
    "\n",
    "**Pipeline:**\n",
    "1. Lang Adapter: Rust domain specialisation from code generation/completion examples\n",
    "2. Core Agent SFT: Debug and review training from bug_detection/code_review examples\n",
    "3. IPO: Synthetic preference pairs from bug_detection (if enabled)\n",
    "\n",
    "**Outputs:**\n",
    "- Checkpoints: `checkpoints/core_agent_{ipo}/final`\n",
    "- Exported model: `checkpoints/gpt-oss-20b-rust-export-v3/`\n",
    "- All backed up to Google Drive: `gpt-oss-20b-rust-agent-v3/`\n",
    "\n",
    "**Compared to v2:**\n",
    "- No Rust toolchain required \\u2014 runs on any Colab GPU instance\n",
    "- No cargo-mutants or trajectory generation \\u2014 faster setup\n",
    "- No GRPO RL \\u2014 no execution-based rewards\n",
    "- For better results, consider upgrading to v2 with mutation data + GRPO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}