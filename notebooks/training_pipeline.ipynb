{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Training Pipeline\n",
    "\n",
    "This notebook provides an interactive interface for training a 7B parameter language model through the complete pipeline:\n",
    "\n",
    "1. **Data Preparation** - Download, clean, and tokenize training data\n",
    "2. **Pretraining** - Train on large text corpora with curriculum learning\n",
    "3. **SFT** - Supervised fine-tuning on instruction-response pairs\n",
    "4. **DPO** - Direct preference optimization for alignment\n",
    "5. **LoRA** - Optional domain-specific fine-tuning\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- NVIDIA GPU (A100 80GB recommended, H100 for FP8 support)\n",
    "- Training scripts installed in the same environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set the project root directory\n",
    "PROJECT_ROOT = os.path.dirname(os.getcwd()) if 'notebooks' in os.getcwd() else os.getcwd()\n",
    "os.chdir(PROJECT_ROOT)\n",
    "sys.path.insert(0, os.path.join(PROJECT_ROOT, 'scripts'))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # Check for H100/FP8 support\n",
    "    capability = torch.cuda.get_device_capability()\n",
    "    if capability[0] >= 9:\n",
    "        print(\"FP8 Support: Available (H100)\")\n",
    "    else:\n",
    "        print(f\"FP8 Support: Not available (compute capability {capability[0]}.{capability[1]})\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected! Training will be extremely slow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "# Modify these settings as needed\n",
    "\n",
    "CONFIG = {\n",
    "    # General settings\n",
    "    'use_fp8': None,  # None = auto-detect, True = force FP8, False = force BF16\n",
    "    'seed': 42,\n",
    "    'enable_oom_recovery': True,\n",
    "    \n",
    "    # Pretraining\n",
    "    'pretrain_max_steps': 100000,\n",
    "    'pretrain_save_steps': 1000,\n",
    "    'pretrain_eval_steps': 1000,\n",
    "    \n",
    "    # SFT\n",
    "    'sft_max_steps': 5000,\n",
    "    'sft_save_steps': 500,\n",
    "    \n",
    "    # DPO\n",
    "    'dpo_max_steps': 2000,\n",
    "    'dpo_save_steps': 200,\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pre-flight Validation\n",
    "\n",
    "Before starting training, validate that all prerequisites are in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pre-flight checks for a specific stage\n",
    "# Options: 'pretrain', 'sft', 'dpo', 'lora'\n",
    "\n",
    "STAGE_TO_CHECK = 'pretrain'  # Change this to check different stages\n",
    "\n",
    "!python scripts/preflight_check.py {STAGE_TO_CHECK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all pre-flight checks\n",
    "!python scripts/preflight_check.py --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 1: Data Preparation\n",
    "\n",
    "Download, clean, and prepare training data. Skip this section if data is already prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.1: Download raw data\n",
    "# This downloads data from configured sources (HuggingFace, etc.)\n",
    "\n",
    "!python scripts/01_download_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.2: Clean and deduplicate data\n",
    "# Removes duplicates, filters low-quality content\n",
    "\n",
    "!python scripts/02_clean_deduplicate_optimized.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.3: Tokenize and pack sequences\n",
    "# Creates packed sequences for efficient training\n",
    "\n",
    "!python scripts/03_tokenize_and_pack.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.4: Initialize model\n",
    "# Creates the initial 7B model checkpoint\n",
    "\n",
    "!python scripts/04_init_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data preparation\n",
    "import os\n",
    "\n",
    "paths_to_check = [\n",
    "    ('Tokenizer', 'configs/tokenizer'),\n",
    "    ('Initial model', 'checkpoints/init'),\n",
    "    ('Training data', 'data/packed/train'),\n",
    "    ('Validation data', 'data/packed/val'),\n",
    "]\n",
    "\n",
    "print(\"Data preparation status:\")\n",
    "print(\"=\" * 50)\n",
    "all_ready = True\n",
    "for name, path in paths_to_check:\n",
    "    exists = os.path.exists(path)\n",
    "    status = \"OK\" if exists else \"MISSING\"\n",
    "    print(f\"  {name}: {status}\")\n",
    "    all_ready = all_ready and exists\n",
    "\n",
    "print(\"=\" * 50)\n",
    "if all_ready:\n",
    "    print(\"All data preparation complete! Ready for pretraining.\")\n",
    "else:\n",
    "    print(\"Some data is missing. Run the preparation steps above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 2: Pretraining\n",
    "\n",
    "Train the base model on large text corpora. This is the longest stage.\n",
    "\n",
    "**Estimated time:** 25-50 hours depending on GPU (H100 FP8 fastest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pretraining command\n",
    "pretrain_cmd = \"python scripts/05_pretrain.py\"\n",
    "\n",
    "if CONFIG['use_fp8'] is True:\n",
    "    pretrain_cmd += \" --fp8\"\n",
    "elif CONFIG['use_fp8'] is False:\n",
    "    pretrain_cmd += \" --no-fp8\"\n",
    "\n",
    "pretrain_cmd += f\" --max_steps {CONFIG['pretrain_max_steps']}\"\n",
    "pretrain_cmd += f\" --save_steps {CONFIG['pretrain_save_steps']}\"\n",
    "pretrain_cmd += f\" --eval_steps {CONFIG['pretrain_eval_steps']}\"\n",
    "pretrain_cmd += f\" --seed {CONFIG['seed']}\"\n",
    "\n",
    "if CONFIG['enable_oom_recovery']:\n",
    "    pretrain_cmd += \" --enable-oom-recovery\"\n",
    "\n",
    "print(\"Pretraining command:\")\n",
    "print(pretrain_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start pretraining\n",
    "# This will take a long time - monitor progress in the output\n",
    "\n",
    "!{pretrain_cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume pretraining from checkpoint (if interrupted)\n",
    "# Uncomment and modify the checkpoint path as needed\n",
    "\n",
    "# CHECKPOINT_PATH = \"checkpoints/pretrain/checkpoint-5000\"\n",
    "# !python scripts/05_pretrain.py --resume_from_checkpoint {CHECKPOINT_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 3: Supervised Fine-Tuning (SFT)\n",
    "\n",
    "Fine-tune on instruction-response pairs to create a helpful assistant.\n",
    "\n",
    "**Estimated time:** 2-5 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare SFT data (if not already done)\n",
    "!python scripts/06_prepare_sft_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify pretrained checkpoint exists\n",
    "import os\n",
    "\n",
    "if os.path.exists('checkpoints/pretrain_final'):\n",
    "    print(\"Pretrained checkpoint found. Ready for SFT.\")\n",
    "else:\n",
    "    print(\"ERROR: Pretrained checkpoint not found!\")\n",
    "    print(\"Complete pretraining before starting SFT.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build SFT command\n",
    "sft_cmd = \"python scripts/07_sft.py\"\n",
    "\n",
    "if CONFIG['use_fp8'] is True:\n",
    "    sft_cmd += \" --fp8\"\n",
    "elif CONFIG['use_fp8'] is False:\n",
    "    sft_cmd += \" --no-fp8\"\n",
    "\n",
    "sft_cmd += f\" --max_steps {CONFIG['sft_max_steps']}\"\n",
    "sft_cmd += f\" --save_steps {CONFIG['sft_save_steps']}\"\n",
    "sft_cmd += f\" --seed {CONFIG['seed']}\"\n",
    "\n",
    "if CONFIG['enable_oom_recovery']:\n",
    "    sft_cmd += \" --enable-oom-recovery\"\n",
    "\n",
    "print(\"SFT command:\")\n",
    "print(sft_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start SFT training\n",
    "!{sft_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 4: Direct Preference Optimization (DPO)\n",
    "\n",
    "Align the model with human preferences using chosen/rejected response pairs.\n",
    "\n",
    "**Estimated time:** 1-3 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DPO data\n",
    "!python scripts/08_prepare_dpo_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify SFT checkpoint exists\n",
    "import os\n",
    "\n",
    "if os.path.exists('checkpoints/sft_final'):\n",
    "    print(\"SFT checkpoint found. Ready for DPO.\")\n",
    "else:\n",
    "    print(\"ERROR: SFT checkpoint not found!\")\n",
    "    print(\"Complete SFT before starting DPO.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build DPO command\n",
    "dpo_cmd = \"python scripts/09_dpo.py\"\n",
    "\n",
    "if CONFIG['use_fp8'] is True:\n",
    "    dpo_cmd += \" --fp8\"\n",
    "elif CONFIG['use_fp8'] is False:\n",
    "    dpo_cmd += \" --no-fp8\"\n",
    "\n",
    "dpo_cmd += f\" --max_steps {CONFIG['dpo_max_steps']}\"\n",
    "dpo_cmd += f\" --save_steps {CONFIG['dpo_save_steps']}\"\n",
    "dpo_cmd += f\" --seed {CONFIG['seed']}\"\n",
    "\n",
    "if CONFIG['enable_oom_recovery']:\n",
    "    dpo_cmd += \" --enable-oom-recovery\"\n",
    "\n",
    "print(\"DPO command:\")\n",
    "print(dpo_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start DPO training\n",
    "!{dpo_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 5: LoRA Fine-Tuning (Optional)\n",
    "\n",
    "Domain-specific adaptation using LoRA for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA fine-tuning (optional)\n",
    "# Uncomment to run LoRA training\n",
    "\n",
    "# !python scripts/10_lora_finetune.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluation\n",
    "\n",
    "Evaluate the trained model on various benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full evaluation suite\n",
    "CHECKPOINT_TO_EVAL = \"checkpoints/dpo_final\"  # Change as needed\n",
    "\n",
    "!python scripts/11_evaluate.py {CHECKPOINT_TO_EVAL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check promotion gates\n",
    "# Verify model meets quality thresholds\n",
    "\n",
    "STAGE_TO_CHECK = \"dpo\"  # Options: pretrain, sft, dpo\n",
    "\n",
    "!python scripts/12_check_gates.py {STAGE_TO_CHECK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Monitoring & Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor GPU utilization\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all checkpoints\n",
    "!bash scripts/checkpoint_manager.sh list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show disk usage\n",
    "!bash scripts/checkpoint_manager.sh disk-usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup old checkpoints (keep latest 3)\n",
    "# Uncomment to run\n",
    "\n",
    "# !bash scripts/checkpoint_manager.sh cleanup pretrain 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Inference\n",
    "\n",
    "Test the trained model with interactive generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model for inference\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "MODEL_PATH = \"checkpoints/dpo_final\"  # Change to your checkpoint\n",
    "\n",
    "print(f\"Loading model from {MODEL_PATH}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"configs/tokenizer\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "def generate(prompt, max_new_tokens=256, temperature=0.7):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test generation\n",
    "prompt = \"Explain machine learning in simple terms:\"\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(f\"Response: {generate(prompt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive generation cell\n",
    "# Modify the prompt and run to test different inputs\n",
    "\n",
    "PROMPT = \"Write a Python function to calculate fibonacci numbers:\"\n",
    "\n",
    "print(f\"Prompt: {PROMPT}\\n\")\n",
    "print(\"=\" * 50)\n",
    "print(generate(PROMPT, max_new_tokens=512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Summary\n",
    "\n",
    "After completing all stages, review the training summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training report\n",
    "import os\n",
    "import json\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING PIPELINE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "stages = [\n",
    "    ('Pretrain', 'checkpoints/pretrain_final'),\n",
    "    ('SFT', 'checkpoints/sft_final'),\n",
    "    ('DPO', 'checkpoints/dpo_final'),\n",
    "    ('LoRA', 'checkpoints/lora_final'),\n",
    "]\n",
    "\n",
    "print(\"\\nCheckpoint Status:\")\n",
    "for name, path in stages:\n",
    "    if os.path.exists(path):\n",
    "        # Get checkpoint size\n",
    "        size = sum(os.path.getsize(os.path.join(path, f)) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)))\n",
    "        size_gb = size / (1024**3)\n",
    "        print(f\"  {name}: COMPLETE ({size_gb:.2f} GB)\")\n",
    "    else:\n",
    "        print(f\"  {name}: Not completed\")\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "eval_path = \"evals/\"\n",
    "if os.path.exists(eval_path):\n",
    "    for f in os.listdir(eval_path):\n",
    "        if f.endswith('.json'):\n",
    "            with open(os.path.join(eval_path, f)) as file:\n",
    "                results = json.load(file)\n",
    "                print(f\"  {f}: {results}\")\n",
    "else:\n",
    "    print(\"  No evaluation results found. Run evaluation first.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
