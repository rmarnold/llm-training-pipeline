{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# LLM Training Pipeline\n\nThis notebook provides an interactive interface for training a 7B parameter language model through the complete pipeline:\n\n1. **Data Preparation** - Download, clean, and tokenize training data\n2. **Pretraining** - Train on large text corpora with curriculum learning\n3. **SFT** - Supervised fine-tuning on instruction-response pairs\n4. **DPO** - Direct preference optimization for alignment\n5. **LoRA** - Optional domain-specific fine-tuning\n\n## Requirements\n\n- **GPU**: NVIDIA A100 80GB recommended, H100 for FP8 support\n- **Colab**: Pro/Pro+ recommended for longer training sessions\n- **Storage**: Google Drive for persistent checkpoints (recommended)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Step 0: Environment Setup\n\nRun these cells first to set up the training environment. Choose your deployment method below."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title ### 0.1 Detect Environment & Mount Google Drive\n#@markdown Run this cell to detect if running in Colab and mount Google Drive for persistent storage.\n\nimport os\nimport sys\n\n# Detect if running in Google Colab\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    print(\"Running in Google Colab\")\n    \n    # Mount Google Drive for persistent storage\n    from google.colab import drive\n    drive.mount('/content/drive')\n    \n    # Set up persistent directories on Google Drive\n    DRIVE_BASE = \"/content/drive/MyDrive/llm-training-pipeline\"\n    os.makedirs(DRIVE_BASE, exist_ok=True)\n    os.makedirs(f\"{DRIVE_BASE}/checkpoints\", exist_ok=True)\n    os.makedirs(f\"{DRIVE_BASE}/data\", exist_ok=True)\n    \n    print(f\"Google Drive mounted. Persistent storage at: {DRIVE_BASE}\")\nelse:\n    print(\"Running locally (not in Colab)\")\n    DRIVE_BASE = None"
  },
  {
   "cell_type": "code",
   "source": "#@title ### 0.2 Clone Repository & Install Dependencies\n#@markdown Choose your repository source and install dependencies.\n\n#@markdown ---\n#@markdown **Repository Settings:**\nREPO_URL = \"https://github.com/rmarnold/llm-training-pipeline.git\"  #@param {type:\"string\"}\nBRANCH = \"main\"  #@param {type:\"string\"}\n\nimport os\nimport subprocess\n\n# Clone or update repository\nREPO_DIR = \"/content/llm-training-pipeline\"\n\nif IN_COLAB:\n    if os.path.exists(REPO_DIR):\n        print(f\"Repository exists. Pulling latest changes...\")\n        %cd {REPO_DIR}\n        !git pull origin {BRANCH}\n    else:\n        print(f\"Cloning repository from {REPO_URL}...\")\n        !git clone -b {BRANCH} {REPO_URL} {REPO_DIR}\n        %cd {REPO_DIR}\n    \n    # Install core dependencies\n    print(\"\\n\" + \"=\"*50)\n    print(\"Installing dependencies...\")\n    print(\"=\"*50)\n    \n    # Install the package\n    !pip install -q -e .\n    \n    # Install flash-attn separately (requires special handling)\n    print(\"\\nInstalling flash-attention (this may take a few minutes)...\")\n    try:\n        # Try installing pre-built wheel first (faster)\n        !pip install -q flash-attn --no-build-isolation 2>/dev/null || \\\n         pip install -q flash-attn --no-build-isolation --no-cache-dir\n        print(\"flash-attn installed successfully!\")\n    except:\n        print(\"Warning: flash-attn installation failed. Training will use standard attention.\")\n        print(\"This is OK - training will still work, just slightly slower.\")\n    \n    # Install kernel optimizations (Liger Kernel + Cut Cross-Entropy)\n    print(\"\\nInstalling kernel optimizations...\")\n    !pip install -q liger-kernel cut-cross-entropy 2>/dev/null || true\n    print(\"Kernel optimizations installed (Liger Kernel, Cut Cross-Entropy)\")\n    \n    PROJECT_ROOT = REPO_DIR\n    print(\"\\n\" + \"=\"*50)\n    print(\"Installation complete!\")\n    print(\"=\"*50)\nelse:\n    # Local development - assume we're in the repo\n    PROJECT_ROOT = os.path.dirname(os.getcwd()) if 'notebooks' in os.getcwd() else os.getcwd()\n    \nprint(f\"\\nProject root: {PROJECT_ROOT}\")\nos.chdir(PROJECT_ROOT)\nsys.path.insert(0, os.path.join(PROJECT_ROOT, 'scripts'))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#@title ### 0.3 Set Up Persistent Storage (Google Drive)\n#@markdown Link checkpoints and data directories to Google Drive for persistence across sessions.\n\n#@markdown ---\nUSE_DRIVE_STORAGE = True  #@param {type:\"boolean\"}\n\nimport os\n\nif IN_COLAB and USE_DRIVE_STORAGE and DRIVE_BASE:\n    print(\"Setting up persistent storage on Google Drive...\")\n    \n    # Create symlinks for checkpoints and data\n    local_dirs = ['checkpoints', 'data', 'logs', 'evals']\n    \n    for dir_name in local_dirs:\n        local_path = os.path.join(PROJECT_ROOT, dir_name)\n        drive_path = os.path.join(DRIVE_BASE, dir_name)\n        \n        # Create drive directory if it doesn't exist\n        os.makedirs(drive_path, exist_ok=True)\n        \n        # Remove local dir if it exists (but not if it's already a symlink)\n        if os.path.exists(local_path) and not os.path.islink(local_path):\n            # Move existing contents to drive\n            if os.listdir(local_path):\n                print(f\"  Moving existing {dir_name} to Drive...\")\n                !cp -r {local_path}/* {drive_path}/ 2>/dev/null || true\n            !rm -rf {local_path}\n        \n        # Create symlink\n        if not os.path.exists(local_path):\n            os.symlink(drive_path, local_path)\n            print(f\"  {dir_name} -> {drive_path}\")\n    \n    print(\"\\nPersistent storage configured!\")\n    print(\"Your checkpoints and data will survive Colab disconnections.\")\nelse:\n    print(\"Using local storage (not persistent in Colab)\")\n    # Create local directories\n    for dir_name in ['checkpoints', 'data', 'logs', 'evals']:\n        os.makedirs(os.path.join(PROJECT_ROOT, dir_name), exist_ok=True)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Alternative: Install from pip (if repo is published)\n\nIf the package is published to PyPI or you prefer pip installation:\n\n```python\n# From PyPI (when published)\n!pip install llm-training-pipeline\n\n# From GitHub directly\n!pip install git+https://github.com/rmarnold/llm-training-pipeline.git\n\n# With FP8 support (H100 only)\n!pip install \"llm-training-pipeline[fp8]\"\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title ### 0.4 Check GPU Availability\n#@markdown Verify GPU is available and check for FP8 support.\n\nimport torch\n\nprint(\"=\" * 50)\nprint(\"GPU INFORMATION\")\nprint(\"=\" * 50)\n\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n    print(f\"GPU: {gpu_name}\")\n    print(f\"Memory: {gpu_memory:.1f} GB\")\n    \n    # Check for H100/FP8 support\n    capability = torch.cuda.get_device_capability()\n    print(f\"Compute Capability: {capability[0]}.{capability[1]}\")\n    \n    if capability[0] >= 9:\n        print(\"FP8 Support: AVAILABLE (H100)\")\n        RECOMMENDED_PRECISION = \"fp8\"\n    elif capability[0] >= 8:\n        print(\"FP8 Support: Not available (use BF16)\")\n        RECOMMENDED_PRECISION = \"bf16\"\n    else:\n        print(\"FP8 Support: Not available\")\n        RECOMMENDED_PRECISION = \"fp16\"\n    \n    # Memory recommendation\n    if gpu_memory >= 80:\n        print(f\"\\nRecommendation: Full 7B training supported\")\n    elif gpu_memory >= 40:\n        print(f\"\\nRecommendation: Use gradient checkpointing, smaller batch size\")\n    else:\n        print(f\"\\nWarning: GPU memory may be insufficient for 7B model\")\n        print(\"Consider using LoRA or a smaller model\")\nelse:\n    print(\"WARNING: No GPU detected!\")\n    print(\"Training will be extremely slow on CPU.\")\n    RECOMMENDED_PRECISION = \"fp32\"\n\nprint(\"=\" * 50)"
  },
  {
   "cell_type": "code",
   "source": "#@title ### 0.5 Start GPU Keepalive (Prevents Idle Timeout) [DISABLED]\n#@markdown **Note:** GPU keepalive is currently disabled. Uncomment the code below to enable if needed.\n#@markdown \n#@markdown This starts a background process that periodically pings the GPU to prevent Colab from timing out during long CPU-bound operations (like data cleaning).\n\n# GPU Keepalive is DISABLED - uncomment below to enable\n# import subprocess\n# import os\n# \n# if IN_COLAB:\n#     # Kill any existing keepalive process\n#     !pkill -f gpu_keepalive.py 2>/dev/null || true\n#     \n#     # Start GPU keepalive in background\n#     keepalive_script = os.path.join(PROJECT_ROOT, 'scripts', 'gpu_keepalive.py')\n#     \n#     if os.path.exists(keepalive_script):\n#         process = subprocess.Popen(\n#             ['python', keepalive_script],\n#             stdout=subprocess.DEVNULL,\n#             stderr=subprocess.DEVNULL,\n#             start_new_session=True\n#         )\n#         print(f\"GPU Keepalive started (PID: {process.pid})\")\n#         print(\"  - Checks GPU every 60 seconds\")\n#         print(\"  - Sends keepalive spike if idle for 5+ minutes\")\n#         print(\"  - Prevents Colab idle timeout during CPU-bound tasks\")\n#         print(f\"\\nTo stop: !pkill -f gpu_keepalive.py\")\n#     else:\n#         print(\"Warning: gpu_keepalive.py not found. Run 'git pull' to get latest code.\")\n# else:\n#     print(\"GPU keepalive not needed outside of Colab\")\n\nprint(\"GPU Keepalive: DISABLED\")\nprint(\"To enable, uncomment the code in this cell.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title ### 0.5 Training Configuration {run: \"auto\"}\n#@markdown Adjust these settings based on your GPU and requirements.\n\n#@markdown ---\n#@markdown **General Settings:**\nuse_fp8 = \"auto\"  #@param [\"auto\", \"true\", \"false\"]\nseed = 42  #@param {type:\"integer\"}\nenable_oom_recovery = True  #@param {type:\"boolean\"}\n\n#@markdown ---\n#@markdown **Kernel Optimizations (Recommended):**\nuse_liger_kernel = True  #@param {type:\"boolean\"}\n#@markdown *Liger Kernel: ~20% speedup, ~60% memory reduction*\nuse_cce = True  #@param {type:\"boolean\"}\n#@markdown *Cut Cross-Entropy: ~95% memory reduction on loss computation*\n\n#@markdown ---\n#@markdown **Data Cleaning:**\npipeline_mode = \"native\"  #@param [\"native\", \"legacy\"]\n#@markdown *Pipeline modes:*\n#@markdown - **native**: Uses datatrove's optimized pipeline (3-5x faster, recommended)\n#@markdown - **legacy**: Uses multiprocessing Pool (fallback if native fails)\n\nuse_full_clean = False  #@param {type:\"boolean\"}\n#@markdown *Full clean uses plsfix (Rust) for Unicode/mojibake fixing. Slower but more thorough.*\n\nquality_filter_mode = \"default\"  #@param [\"default\", \"fast\", \"no-repetition\", \"no-fineweb\"]\n#@markdown *Quality filter modes:*\n#@markdown - **default**: All filters (GopherQuality + FineWeb + GopherRepetition)\n#@markdown - **fast**: Only GopherQuality (~3x faster, basic filtering)\n#@markdown - **no-repetition**: Skip n-gram analysis (~2x faster, may miss spam)\n#@markdown - **no-fineweb**: Skip line structure checks (~15% faster)\n\n#@markdown ---\n#@markdown **Pretraining:**\npretrain_max_steps = 100000  #@param {type:\"integer\"}\npretrain_save_steps = 1000  #@param {type:\"integer\"}\npretrain_eval_steps = 1000  #@param {type:\"integer\"}\n\n#@markdown ---\n#@markdown **SFT:**\nsft_max_steps = 5000  #@param {type:\"integer\"}\nsft_save_steps = 500  #@param {type:\"integer\"}\n\n#@markdown ---\n#@markdown **DPO:**\ndpo_max_steps = 2000  #@param {type:\"integer\"}\ndpo_save_steps = 200  #@param {type:\"integer\"}\n\n# Build config dict\nCONFIG = {\n    'use_fp8': None if use_fp8 == \"auto\" else (use_fp8 == \"true\"),\n    'seed': seed,\n    'enable_oom_recovery': enable_oom_recovery,\n    'use_liger_kernel': use_liger_kernel,\n    'use_cce': use_cce,\n    'pipeline_mode': pipeline_mode,\n    'use_full_clean': use_full_clean,\n    'quality_filter_mode': quality_filter_mode,\n    'pretrain_max_steps': pretrain_max_steps,\n    'pretrain_save_steps': pretrain_save_steps,\n    'pretrain_eval_steps': pretrain_eval_steps,\n    'sft_max_steps': sft_max_steps,\n    'sft_save_steps': sft_save_steps,\n    'dpo_max_steps': dpo_max_steps,\n    'dpo_save_steps': dpo_save_steps,\n}\n\nprint(\"Configuration:\")\nfor key, value in CONFIG.items():\n    print(f\"  {key}: {value}\")\n\nif use_liger_kernel or use_cce:\n    print(\"\\nKernel Optimizations Enabled:\")\n    if use_liger_kernel:\n        print(\"  - Liger Kernel: Fused Triton kernels for ~20% speedup\")\n    if use_cce:\n        print(\"  - Cut Cross-Entropy: Memory-efficient loss (~95% reduction)\")\n\nprint(f\"\\nPipeline Mode: {'NATIVE DATATROVE (optimized)' if pipeline_mode == 'native' else 'LEGACY (multiprocessing)'}\")\nprint(f\"Data Cleaning Mode: {'FULL (plsfix)' if use_full_clean else 'FAST (skip Unicode fixing)'}\")\nprint(f\"Quality Filter Mode: {quality_filter_mode}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pre-flight Validation\n",
    "\n",
    "Before starting training, validate that all prerequisites are in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pre-flight checks for a specific stage\n",
    "# Options: 'pretrain', 'sft', 'dpo', 'lora'\n",
    "\n",
    "STAGE_TO_CHECK = 'pretrain'  # Change this to check different stages\n",
    "\n",
    "!python scripts/preflight_check.py {STAGE_TO_CHECK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all pre-flight checks\n",
    "!python scripts/preflight_check.py --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 1: Data Preparation\n",
    "\n",
    "Download, clean, and prepare training data. Skip this section if data is already prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.1: Download raw data\n",
    "# This downloads data from configured sources (HuggingFace, etc.)\n",
    "\n",
    "!python scripts/01_download_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1.2: Clean and deduplicate data\n# Removes duplicates, filters low-quality content\n\nclean_cmd = \"python scripts/02_clean_deduplicate_optimized.py\"\n\n# Pipeline mode (native datatrove vs legacy multiprocessing)\npipeline_mode = CONFIG.get('pipeline_mode', 'native')\nif pipeline_mode == 'native':\n    clean_cmd += \" --native-pipeline\"\nelse:\n    clean_cmd += \" --legacy\"\n\n# Unicode cleaning mode\nif CONFIG.get('use_full_clean', False):\n    clean_cmd += \" --full-clean\"\n\n# Quality filter mode\nfilter_mode = CONFIG.get('quality_filter_mode', 'default')\nif filter_mode == 'fast':\n    clean_cmd += \" --fast-quality\"\nelif filter_mode == 'no-repetition':\n    clean_cmd += \" --no-repetition-filter\"\nelif filter_mode == 'no-fineweb':\n    clean_cmd += \" --no-fineweb-filter\"\n# 'default' uses all filters, no extra flags needed\n\nprint(f\"Cleaning command: {clean_cmd}\")\nprint(f\"Pipeline mode: {'NATIVE DATATROVE (optimized)' if pipeline_mode == 'native' else 'LEGACY (multiprocessing)'}\")\nprint(f\"Unicode cleaning: {'FULL (plsfix)' if CONFIG.get('use_full_clean') else 'FAST (skip Unicode fixing)'}\")\nprint(f\"Quality filters: {filter_mode}\")\nprint()\n\n!{clean_cmd}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.3: Tokenize and pack sequences\n",
    "# Creates packed sequences for efficient training\n",
    "\n",
    "!python scripts/03_tokenize_and_pack.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.4: Initialize model\n",
    "# Creates the initial 7B model checkpoint\n",
    "\n",
    "!python scripts/04_init_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data preparation\n",
    "import os\n",
    "\n",
    "paths_to_check = [\n",
    "    ('Tokenizer', 'configs/tokenizer'),\n",
    "    ('Initial model', 'checkpoints/init'),\n",
    "    ('Training data', 'data/packed/train'),\n",
    "    ('Validation data', 'data/packed/val'),\n",
    "]\n",
    "\n",
    "print(\"Data preparation status:\")\n",
    "print(\"=\" * 50)\n",
    "all_ready = True\n",
    "for name, path in paths_to_check:\n",
    "    exists = os.path.exists(path)\n",
    "    status = \"OK\" if exists else \"MISSING\"\n",
    "    print(f\"  {name}: {status}\")\n",
    "    all_ready = all_ready and exists\n",
    "\n",
    "print(\"=\" * 50)\n",
    "if all_ready:\n",
    "    print(\"All data preparation complete! Ready for pretraining.\")\n",
    "else:\n",
    "    print(\"Some data is missing. Run the preparation steps above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 2: Pretraining\n",
    "\n",
    "Train the base model on large text corpora. This is the longest stage.\n",
    "\n",
    "**Estimated time:** 25-50 hours depending on GPU (H100 FP8 fastest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build pretraining command\npretrain_cmd = \"python scripts/05_pretrain.py\"\n\nif CONFIG['use_fp8'] is True:\n    pretrain_cmd += \" --fp8\"\nelif CONFIG['use_fp8'] is False:\n    pretrain_cmd += \" --no-fp8\"\n\npretrain_cmd += f\" --max_steps {CONFIG['pretrain_max_steps']}\"\npretrain_cmd += f\" --save_steps {CONFIG['pretrain_save_steps']}\"\npretrain_cmd += f\" --eval_steps {CONFIG['pretrain_eval_steps']}\"\npretrain_cmd += f\" --seed {CONFIG['seed']}\"\n\nif CONFIG['enable_oom_recovery']:\n    pretrain_cmd += \" --enable-oom-recovery\"\n\n# Kernel optimizations (enabled by default)\nif CONFIG.get('use_liger_kernel', True):\n    pretrain_cmd += \" --use-liger-kernel\"\nelse:\n    pretrain_cmd += \" --no-liger-kernel\"\n\nif CONFIG.get('use_cce', True):\n    pretrain_cmd += \" --use-cce\"\nelse:\n    pretrain_cmd += \" --no-cce\"\n\nprint(\"Pretraining command:\")\nprint(pretrain_cmd)\n\nif CONFIG.get('use_liger_kernel') or CONFIG.get('use_cce'):\n    print(\"\\nKernel optimizations enabled for faster training!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start pretraining\n",
    "# This will take a long time - monitor progress in the output\n",
    "\n",
    "!{pretrain_cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume pretraining from checkpoint (if interrupted)\n",
    "# Uncomment and modify the checkpoint path as needed\n",
    "\n",
    "# CHECKPOINT_PATH = \"checkpoints/pretrain/checkpoint-5000\"\n",
    "# !python scripts/05_pretrain.py --resume_from_checkpoint {CHECKPOINT_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 3: Supervised Fine-Tuning (SFT)\n",
    "\n",
    "Fine-tune on instruction-response pairs to create a helpful assistant.\n",
    "\n",
    "**Estimated time:** 2-5 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare SFT data (if not already done)\n",
    "!python scripts/06_prepare_sft_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify pretrained checkpoint exists\n",
    "import os\n",
    "\n",
    "if os.path.exists('checkpoints/pretrain_final'):\n",
    "    print(\"Pretrained checkpoint found. Ready for SFT.\")\n",
    "else:\n",
    "    print(\"ERROR: Pretrained checkpoint not found!\")\n",
    "    print(\"Complete pretraining before starting SFT.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build SFT command\n",
    "sft_cmd = \"python scripts/07_sft.py\"\n",
    "\n",
    "if CONFIG['use_fp8'] is True:\n",
    "    sft_cmd += \" --fp8\"\n",
    "elif CONFIG['use_fp8'] is False:\n",
    "    sft_cmd += \" --no-fp8\"\n",
    "\n",
    "sft_cmd += f\" --max_steps {CONFIG['sft_max_steps']}\"\n",
    "sft_cmd += f\" --save_steps {CONFIG['sft_save_steps']}\"\n",
    "sft_cmd += f\" --seed {CONFIG['seed']}\"\n",
    "\n",
    "if CONFIG['enable_oom_recovery']:\n",
    "    sft_cmd += \" --enable-oom-recovery\"\n",
    "\n",
    "print(\"SFT command:\")\n",
    "print(sft_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start SFT training\n",
    "!{sft_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 4: Direct Preference Optimization (DPO)\n",
    "\n",
    "Align the model with human preferences using chosen/rejected response pairs.\n",
    "\n",
    "**Estimated time:** 1-3 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DPO data\n",
    "!python scripts/08_prepare_dpo_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify SFT checkpoint exists\n",
    "import os\n",
    "\n",
    "if os.path.exists('checkpoints/sft_final'):\n",
    "    print(\"SFT checkpoint found. Ready for DPO.\")\n",
    "else:\n",
    "    print(\"ERROR: SFT checkpoint not found!\")\n",
    "    print(\"Complete SFT before starting DPO.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build DPO command\n",
    "dpo_cmd = \"python scripts/09_dpo.py\"\n",
    "\n",
    "if CONFIG['use_fp8'] is True:\n",
    "    dpo_cmd += \" --fp8\"\n",
    "elif CONFIG['use_fp8'] is False:\n",
    "    dpo_cmd += \" --no-fp8\"\n",
    "\n",
    "dpo_cmd += f\" --max_steps {CONFIG['dpo_max_steps']}\"\n",
    "dpo_cmd += f\" --save_steps {CONFIG['dpo_save_steps']}\"\n",
    "dpo_cmd += f\" --seed {CONFIG['seed']}\"\n",
    "\n",
    "if CONFIG['enable_oom_recovery']:\n",
    "    dpo_cmd += \" --enable-oom-recovery\"\n",
    "\n",
    "print(\"DPO command:\")\n",
    "print(dpo_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start DPO training\n",
    "!{dpo_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 5: LoRA Fine-Tuning (Optional)\n",
    "\n",
    "Domain-specific adaptation using LoRA for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA fine-tuning (optional)\n",
    "# Uncomment to run LoRA training\n",
    "\n",
    "# !python scripts/10_lora_finetune.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluation\n",
    "\n",
    "Evaluate the trained model on various benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full evaluation suite\n",
    "CHECKPOINT_TO_EVAL = \"checkpoints/dpo_final\"  # Change as needed\n",
    "\n",
    "!python scripts/11_evaluate.py {CHECKPOINT_TO_EVAL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check promotion gates\n",
    "# Verify model meets quality thresholds\n",
    "\n",
    "STAGE_TO_CHECK = \"dpo\"  # Options: pretrain, sft, dpo\n",
    "\n",
    "!python scripts/12_check_gates.py {STAGE_TO_CHECK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Monitoring & Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor GPU utilization\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all checkpoints\n",
    "!bash scripts/checkpoint_manager.sh list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show disk usage\n",
    "!bash scripts/checkpoint_manager.sh disk-usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup old checkpoints (keep latest 3)\n",
    "# Uncomment to run\n",
    "\n",
    "# !bash scripts/checkpoint_manager.sh cleanup pretrain 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Inference\n",
    "\n",
    "Test the trained model with interactive generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model for inference\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "MODEL_PATH = \"checkpoints/dpo_final\"  # Change to your checkpoint\n",
    "\n",
    "print(f\"Loading model from {MODEL_PATH}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"configs/tokenizer\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "def generate(prompt, max_new_tokens=256, temperature=0.7):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test generation\n",
    "prompt = \"Explain machine learning in simple terms:\"\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(f\"Response: {generate(prompt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive generation cell\n",
    "# Modify the prompt and run to test different inputs\n",
    "\n",
    "PROMPT = \"Write a Python function to calculate fibonacci numbers:\"\n",
    "\n",
    "print(f\"Prompt: {PROMPT}\\n\")\n",
    "print(\"=\" * 50)\n",
    "print(generate(PROMPT, max_new_tokens=512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Summary\n",
    "\n",
    "After completing all stages, review the training summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training report\n",
    "import os\n",
    "import json\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING PIPELINE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "stages = [\n",
    "    ('Pretrain', 'checkpoints/pretrain_final'),\n",
    "    ('SFT', 'checkpoints/sft_final'),\n",
    "    ('DPO', 'checkpoints/dpo_final'),\n",
    "    ('LoRA', 'checkpoints/lora_final'),\n",
    "]\n",
    "\n",
    "print(\"\\nCheckpoint Status:\")\n",
    "for name, path in stages:\n",
    "    if os.path.exists(path):\n",
    "        # Get checkpoint size\n",
    "        size = sum(os.path.getsize(os.path.join(path, f)) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)))\n",
    "        size_gb = size / (1024**3)\n",
    "        print(f\"  {name}: COMPLETE ({size_gb:.2f} GB)\")\n",
    "    else:\n",
    "        print(f\"  {name}: Not completed\")\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "eval_path = \"evals/\"\n",
    "if os.path.exists(eval_path):\n",
    "    for f in os.listdir(eval_path):\n",
    "        if f.endswith('.json'):\n",
    "            with open(os.path.join(eval_path, f)) as file:\n",
    "                results = json.load(file)\n",
    "                print(f\"  {f}: {results}\")\n",
    "else:\n",
    "    print(\"  No evaluation results found. Run evaluation first.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}