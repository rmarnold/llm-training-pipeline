{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# LLM Training Pipeline\n\nThis notebook provides an interactive interface for training a 7B parameter language model through the complete pipeline:\n\n1. **Data Preparation** - Download, clean, and tokenize training data\n2. **Pretraining** - Train on large text corpora with curriculum learning\n3. **SFT** - Supervised fine-tuning on instruction-response pairs\n4. **DPO** - Direct preference optimization for alignment\n5. **LoRA** - Optional domain-specific fine-tuning\n\n## Requirements\n\n- **GPU**: NVIDIA A100 80GB recommended, H100 for FP8 support\n- **Colab**: Pro/Pro+ recommended for longer training sessions\n- **Storage**: Google Drive for persistent checkpoints (recommended)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Step 0: Environment Setup\n\nRun these cells first to set up the training environment. Choose your deployment method below."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title ### 0.1 Detect Environment & Mount Google Drive\n#@markdown Run this cell to detect if running in Colab and mount Google Drive for persistent storage.\n\nimport os\nimport sys\n\n# Detect if running in Google Colab\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    print(\"Running in Google Colab\")\n    \n    # Mount Google Drive for persistent storage\n    from google.colab import drive\n    drive.mount('/content/drive')\n    \n    # Set up persistent directories on Google Drive\n    DRIVE_BASE = \"/content/drive/MyDrive/llm-training-pipeline\"\n    os.makedirs(DRIVE_BASE, exist_ok=True)\n    os.makedirs(f\"{DRIVE_BASE}/checkpoints\", exist_ok=True)\n    os.makedirs(f\"{DRIVE_BASE}/data\", exist_ok=True)\n    \n    print(f\"Google Drive mounted. Persistent storage at: {DRIVE_BASE}\")\nelse:\n    print(\"Running locally (not in Colab)\")\n    DRIVE_BASE = None"
  },
  {
   "cell_type": "code",
   "source": "#@title ### 0.2 Clone Repository & Install Dependencies\n#@markdown Choose your repository source and install dependencies.\n\n#@markdown ---\n#@markdown **Repository Settings:**\nREPO_URL = \"https://github.com/rmarnold/llm-training-pipeline.git\"  #@param {type:\"string\"}\nBRANCH = \"main\"  #@param {type:\"string\"}\n\nimport os\n\n# Clone or update repository\nREPO_DIR = \"/content/llm-training-pipeline\"\n\nif IN_COLAB:\n    if os.path.exists(REPO_DIR):\n        print(f\"Repository exists. Pulling latest changes...\")\n        %cd {REPO_DIR}\n        !git pull origin {BRANCH}\n    else:\n        print(f\"Cloning repository from {REPO_URL}...\")\n        !git clone -b {BRANCH} {REPO_URL} {REPO_DIR}\n        %cd {REPO_DIR}\n    \n    # Install dependencies\n    print(\"\\nInstalling dependencies...\")\n    !pip install -q -e .\n    \n    # Install flash-attn (may need special handling)\n    !pip install -q flash-attn --no-build-isolation\n    \n    PROJECT_ROOT = REPO_DIR\nelse:\n    # Local development - assume we're in the repo\n    PROJECT_ROOT = os.path.dirname(os.getcwd()) if 'notebooks' in os.getcwd() else os.getcwd()\n    \nprint(f\"\\nProject root: {PROJECT_ROOT}\")\nos.chdir(PROJECT_ROOT)\nsys.path.insert(0, os.path.join(PROJECT_ROOT, 'scripts'))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#@title ### 0.3 Set Up Persistent Storage (Google Drive)\n#@markdown Link checkpoints and data directories to Google Drive for persistence across sessions.\n\n#@markdown ---\nUSE_DRIVE_STORAGE = True  #@param {type:\"boolean\"}\n\nimport os\n\nif IN_COLAB and USE_DRIVE_STORAGE and DRIVE_BASE:\n    print(\"Setting up persistent storage on Google Drive...\")\n    \n    # Create symlinks for checkpoints and data\n    local_dirs = ['checkpoints', 'data', 'logs', 'evals']\n    \n    for dir_name in local_dirs:\n        local_path = os.path.join(PROJECT_ROOT, dir_name)\n        drive_path = os.path.join(DRIVE_BASE, dir_name)\n        \n        # Create drive directory if it doesn't exist\n        os.makedirs(drive_path, exist_ok=True)\n        \n        # Remove local dir if it exists (but not if it's already a symlink)\n        if os.path.exists(local_path) and not os.path.islink(local_path):\n            # Move existing contents to drive\n            if os.listdir(local_path):\n                print(f\"  Moving existing {dir_name} to Drive...\")\n                !cp -r {local_path}/* {drive_path}/ 2>/dev/null || true\n            !rm -rf {local_path}\n        \n        # Create symlink\n        if not os.path.exists(local_path):\n            os.symlink(drive_path, local_path)\n            print(f\"  {dir_name} -> {drive_path}\")\n    \n    print(\"\\nPersistent storage configured!\")\n    print(\"Your checkpoints and data will survive Colab disconnections.\")\nelse:\n    print(\"Using local storage (not persistent in Colab)\")\n    # Create local directories\n    for dir_name in ['checkpoints', 'data', 'logs', 'evals']:\n        os.makedirs(os.path.join(PROJECT_ROOT, dir_name), exist_ok=True)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Alternative: Install from pip (if repo is published)\n\nIf the package is published to PyPI or you prefer pip installation:\n\n```python\n# From PyPI (when published)\n!pip install llm-training-pipeline\n\n# From GitHub directly\n!pip install git+https://github.com/rmarnold/llm-training-pipeline.git\n\n# With FP8 support (H100 only)\n!pip install \"llm-training-pipeline[fp8]\"\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title ### 0.4 Check GPU Availability\n#@markdown Verify GPU is available and check for FP8 support.\n\nimport torch\n\nprint(\"=\" * 50)\nprint(\"GPU INFORMATION\")\nprint(\"=\" * 50)\n\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n    print(f\"GPU: {gpu_name}\")\n    print(f\"Memory: {gpu_memory:.1f} GB\")\n    \n    # Check for H100/FP8 support\n    capability = torch.cuda.get_device_capability()\n    print(f\"Compute Capability: {capability[0]}.{capability[1]}\")\n    \n    if capability[0] >= 9:\n        print(\"FP8 Support: AVAILABLE (H100)\")\n        RECOMMENDED_PRECISION = \"fp8\"\n    elif capability[0] >= 8:\n        print(\"FP8 Support: Not available (use BF16)\")\n        RECOMMENDED_PRECISION = \"bf16\"\n    else:\n        print(\"FP8 Support: Not available\")\n        RECOMMENDED_PRECISION = \"fp16\"\n    \n    # Memory recommendation\n    if gpu_memory >= 80:\n        print(f\"\\nRecommendation: Full 7B training supported\")\n    elif gpu_memory >= 40:\n        print(f\"\\nRecommendation: Use gradient checkpointing, smaller batch size\")\n    else:\n        print(f\"\\nWarning: GPU memory may be insufficient for 7B model\")\n        print(\"Consider using LoRA or a smaller model\")\nelse:\n    print(\"WARNING: No GPU detected!\")\n    print(\"Training will be extremely slow on CPU.\")\n    RECOMMENDED_PRECISION = \"fp32\"\n\nprint(\"=\" * 50)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title ### 0.5 Training Configuration {run: \"auto\"}\n#@markdown Adjust these settings based on your GPU and requirements.\n\n#@markdown ---\n#@markdown **General Settings:**\nuse_fp8 = \"auto\"  #@param [\"auto\", \"true\", \"false\"]\nseed = 42  #@param {type:\"integer\"}\nenable_oom_recovery = True  #@param {type:\"boolean\"}\n\n#@markdown ---\n#@markdown **Pretraining:**\npretrain_max_steps = 100000  #@param {type:\"integer\"}\npretrain_save_steps = 1000  #@param {type:\"integer\"}\npretrain_eval_steps = 1000  #@param {type:\"integer\"}\n\n#@markdown ---\n#@markdown **SFT:**\nsft_max_steps = 5000  #@param {type:\"integer\"}\nsft_save_steps = 500  #@param {type:\"integer\"}\n\n#@markdown ---\n#@markdown **DPO:**\ndpo_max_steps = 2000  #@param {type:\"integer\"}\ndpo_save_steps = 200  #@param {type:\"integer\"}\n\n# Build config dict\nCONFIG = {\n    'use_fp8': None if use_fp8 == \"auto\" else (use_fp8 == \"true\"),\n    'seed': seed,\n    'enable_oom_recovery': enable_oom_recovery,\n    'pretrain_max_steps': pretrain_max_steps,\n    'pretrain_save_steps': pretrain_save_steps,\n    'pretrain_eval_steps': pretrain_eval_steps,\n    'sft_max_steps': sft_max_steps,\n    'sft_save_steps': sft_save_steps,\n    'dpo_max_steps': dpo_max_steps,\n    'dpo_save_steps': dpo_save_steps,\n}\n\nprint(\"Configuration:\")\nfor key, value in CONFIG.items():\n    print(f\"  {key}: {value}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pre-flight Validation\n",
    "\n",
    "Before starting training, validate that all prerequisites are in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pre-flight checks for a specific stage\n",
    "# Options: 'pretrain', 'sft', 'dpo', 'lora'\n",
    "\n",
    "STAGE_TO_CHECK = 'pretrain'  # Change this to check different stages\n",
    "\n",
    "!python scripts/preflight_check.py {STAGE_TO_CHECK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all pre-flight checks\n",
    "!python scripts/preflight_check.py --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 1: Data Preparation\n",
    "\n",
    "Download, clean, and prepare training data. Skip this section if data is already prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.1: Download raw data\n",
    "# This downloads data from configured sources (HuggingFace, etc.)\n",
    "\n",
    "!python scripts/01_download_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.2: Clean and deduplicate data\n",
    "# Removes duplicates, filters low-quality content\n",
    "\n",
    "!python scripts/02_clean_deduplicate_optimized.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.3: Tokenize and pack sequences\n",
    "# Creates packed sequences for efficient training\n",
    "\n",
    "!python scripts/03_tokenize_and_pack.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.4: Initialize model\n",
    "# Creates the initial 7B model checkpoint\n",
    "\n",
    "!python scripts/04_init_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data preparation\n",
    "import os\n",
    "\n",
    "paths_to_check = [\n",
    "    ('Tokenizer', 'configs/tokenizer'),\n",
    "    ('Initial model', 'checkpoints/init'),\n",
    "    ('Training data', 'data/packed/train'),\n",
    "    ('Validation data', 'data/packed/val'),\n",
    "]\n",
    "\n",
    "print(\"Data preparation status:\")\n",
    "print(\"=\" * 50)\n",
    "all_ready = True\n",
    "for name, path in paths_to_check:\n",
    "    exists = os.path.exists(path)\n",
    "    status = \"OK\" if exists else \"MISSING\"\n",
    "    print(f\"  {name}: {status}\")\n",
    "    all_ready = all_ready and exists\n",
    "\n",
    "print(\"=\" * 50)\n",
    "if all_ready:\n",
    "    print(\"All data preparation complete! Ready for pretraining.\")\n",
    "else:\n",
    "    print(\"Some data is missing. Run the preparation steps above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 2: Pretraining\n",
    "\n",
    "Train the base model on large text corpora. This is the longest stage.\n",
    "\n",
    "**Estimated time:** 25-50 hours depending on GPU (H100 FP8 fastest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pretraining command\n",
    "pretrain_cmd = \"python scripts/05_pretrain.py\"\n",
    "\n",
    "if CONFIG['use_fp8'] is True:\n",
    "    pretrain_cmd += \" --fp8\"\n",
    "elif CONFIG['use_fp8'] is False:\n",
    "    pretrain_cmd += \" --no-fp8\"\n",
    "\n",
    "pretrain_cmd += f\" --max_steps {CONFIG['pretrain_max_steps']}\"\n",
    "pretrain_cmd += f\" --save_steps {CONFIG['pretrain_save_steps']}\"\n",
    "pretrain_cmd += f\" --eval_steps {CONFIG['pretrain_eval_steps']}\"\n",
    "pretrain_cmd += f\" --seed {CONFIG['seed']}\"\n",
    "\n",
    "if CONFIG['enable_oom_recovery']:\n",
    "    pretrain_cmd += \" --enable-oom-recovery\"\n",
    "\n",
    "print(\"Pretraining command:\")\n",
    "print(pretrain_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start pretraining\n",
    "# This will take a long time - monitor progress in the output\n",
    "\n",
    "!{pretrain_cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume pretraining from checkpoint (if interrupted)\n",
    "# Uncomment and modify the checkpoint path as needed\n",
    "\n",
    "# CHECKPOINT_PATH = \"checkpoints/pretrain/checkpoint-5000\"\n",
    "# !python scripts/05_pretrain.py --resume_from_checkpoint {CHECKPOINT_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 3: Supervised Fine-Tuning (SFT)\n",
    "\n",
    "Fine-tune on instruction-response pairs to create a helpful assistant.\n",
    "\n",
    "**Estimated time:** 2-5 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare SFT data (if not already done)\n",
    "!python scripts/06_prepare_sft_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify pretrained checkpoint exists\n",
    "import os\n",
    "\n",
    "if os.path.exists('checkpoints/pretrain_final'):\n",
    "    print(\"Pretrained checkpoint found. Ready for SFT.\")\n",
    "else:\n",
    "    print(\"ERROR: Pretrained checkpoint not found!\")\n",
    "    print(\"Complete pretraining before starting SFT.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build SFT command\n",
    "sft_cmd = \"python scripts/07_sft.py\"\n",
    "\n",
    "if CONFIG['use_fp8'] is True:\n",
    "    sft_cmd += \" --fp8\"\n",
    "elif CONFIG['use_fp8'] is False:\n",
    "    sft_cmd += \" --no-fp8\"\n",
    "\n",
    "sft_cmd += f\" --max_steps {CONFIG['sft_max_steps']}\"\n",
    "sft_cmd += f\" --save_steps {CONFIG['sft_save_steps']}\"\n",
    "sft_cmd += f\" --seed {CONFIG['seed']}\"\n",
    "\n",
    "if CONFIG['enable_oom_recovery']:\n",
    "    sft_cmd += \" --enable-oom-recovery\"\n",
    "\n",
    "print(\"SFT command:\")\n",
    "print(sft_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start SFT training\n",
    "!{sft_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 4: Direct Preference Optimization (DPO)\n",
    "\n",
    "Align the model with human preferences using chosen/rejected response pairs.\n",
    "\n",
    "**Estimated time:** 1-3 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DPO data\n",
    "!python scripts/08_prepare_dpo_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify SFT checkpoint exists\n",
    "import os\n",
    "\n",
    "if os.path.exists('checkpoints/sft_final'):\n",
    "    print(\"SFT checkpoint found. Ready for DPO.\")\n",
    "else:\n",
    "    print(\"ERROR: SFT checkpoint not found!\")\n",
    "    print(\"Complete SFT before starting DPO.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build DPO command\n",
    "dpo_cmd = \"python scripts/09_dpo.py\"\n",
    "\n",
    "if CONFIG['use_fp8'] is True:\n",
    "    dpo_cmd += \" --fp8\"\n",
    "elif CONFIG['use_fp8'] is False:\n",
    "    dpo_cmd += \" --no-fp8\"\n",
    "\n",
    "dpo_cmd += f\" --max_steps {CONFIG['dpo_max_steps']}\"\n",
    "dpo_cmd += f\" --save_steps {CONFIG['dpo_save_steps']}\"\n",
    "dpo_cmd += f\" --seed {CONFIG['seed']}\"\n",
    "\n",
    "if CONFIG['enable_oom_recovery']:\n",
    "    dpo_cmd += \" --enable-oom-recovery\"\n",
    "\n",
    "print(\"DPO command:\")\n",
    "print(dpo_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start DPO training\n",
    "!{dpo_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 5: LoRA Fine-Tuning (Optional)\n",
    "\n",
    "Domain-specific adaptation using LoRA for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA fine-tuning (optional)\n",
    "# Uncomment to run LoRA training\n",
    "\n",
    "# !python scripts/10_lora_finetune.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluation\n",
    "\n",
    "Evaluate the trained model on various benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full evaluation suite\n",
    "CHECKPOINT_TO_EVAL = \"checkpoints/dpo_final\"  # Change as needed\n",
    "\n",
    "!python scripts/11_evaluate.py {CHECKPOINT_TO_EVAL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check promotion gates\n",
    "# Verify model meets quality thresholds\n",
    "\n",
    "STAGE_TO_CHECK = \"dpo\"  # Options: pretrain, sft, dpo\n",
    "\n",
    "!python scripts/12_check_gates.py {STAGE_TO_CHECK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Monitoring & Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor GPU utilization\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all checkpoints\n",
    "!bash scripts/checkpoint_manager.sh list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show disk usage\n",
    "!bash scripts/checkpoint_manager.sh disk-usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup old checkpoints (keep latest 3)\n",
    "# Uncomment to run\n",
    "\n",
    "# !bash scripts/checkpoint_manager.sh cleanup pretrain 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Inference\n",
    "\n",
    "Test the trained model with interactive generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model for inference\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "MODEL_PATH = \"checkpoints/dpo_final\"  # Change to your checkpoint\n",
    "\n",
    "print(f\"Loading model from {MODEL_PATH}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"configs/tokenizer\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "def generate(prompt, max_new_tokens=256, temperature=0.7):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test generation\n",
    "prompt = \"Explain machine learning in simple terms:\"\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(f\"Response: {generate(prompt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive generation cell\n",
    "# Modify the prompt and run to test different inputs\n",
    "\n",
    "PROMPT = \"Write a Python function to calculate fibonacci numbers:\"\n",
    "\n",
    "print(f\"Prompt: {PROMPT}\\n\")\n",
    "print(\"=\" * 50)\n",
    "print(generate(PROMPT, max_new_tokens=512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Summary\n",
    "\n",
    "After completing all stages, review the training summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training report\n",
    "import os\n",
    "import json\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING PIPELINE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "stages = [\n",
    "    ('Pretrain', 'checkpoints/pretrain_final'),\n",
    "    ('SFT', 'checkpoints/sft_final'),\n",
    "    ('DPO', 'checkpoints/dpo_final'),\n",
    "    ('LoRA', 'checkpoints/lora_final'),\n",
    "]\n",
    "\n",
    "print(\"\\nCheckpoint Status:\")\n",
    "for name, path in stages:\n",
    "    if os.path.exists(path):\n",
    "        # Get checkpoint size\n",
    "        size = sum(os.path.getsize(os.path.join(path, f)) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)))\n",
    "        size_gb = size / (1024**3)\n",
    "        print(f\"  {name}: COMPLETE ({size_gb:.2f} GB)\")\n",
    "    else:\n",
    "        print(f\"  {name}: Not completed\")\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "eval_path = \"evals/\"\n",
    "if os.path.exists(eval_path):\n",
    "    for f in os.listdir(eval_path):\n",
    "        if f.endswith('.json'):\n",
    "            with open(os.path.join(eval_path, f)) as file:\n",
    "                results = json.load(file)\n",
    "                print(f\"  {f}: {results}\")\n",
    "else:\n",
    "    print(\"  No evaluation results found. Run evaluation first.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}