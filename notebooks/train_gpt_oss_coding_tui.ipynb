{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac31bce43c5cfb0c",
   "metadata": {},
   "source": "# Train GPT-OSS 20B Coding TUI Agent\n\n**Combined pipeline** \u2014 Tool calling SFT + agent trajectory training + proxy log extraction + IPO preference + GRPO RL.\n\n**Target failure modes to fix:**\n- Tool calling errors (invalid params, non-existent MCP servers)\n- No follow-through (analysis loops, never writes code)\n- Circular reasoning (repeating the same analysis)\n- Context loss (forgetting task state mid-session)\n\n**Pipeline:**\n1. Tool calling SFT (rank 64 LoRA)\n2. Merge \u2192 Agent SFT from proxy log trajectories (rank 128 LoRA)\n3. IPO preference optimisation (decisive action > endless analysis)\n4. GRPO RL (execution-grounded: compiles / tests pass / no loops)\n5. Eval \u2192 Export\n\n**Base model:** [openai/gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b) (20.9B MoE, 3.6B active)\n\n**Data sources:**\n- [glaiveai/glaive-function-calling-v2](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2) (113K)\n- [Salesforce/xlam-function-calling-60k](https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k) (60K)\n- [NousResearch/hermes-function-calling-v1](https://huggingface.co/datasets/NousResearch/hermes-function-calling-v1)\n- [xingyaoww/code-act](https://huggingface.co/datasets/xingyaoww/code-act)\n- [bigcode/commitpack](https://huggingface.co/datasets/bigcode/commitpack) (50K subsample)\n- [bigcode/editpackft](https://huggingface.co/datasets/bigcode/editpackft) (50K subsample)\n- [Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf)\n- [m-a-p/CodeFeedback-Filtered-Instruction](https://huggingface.co/datasets/m-a-p/CodeFeedback-Filtered-Instruction)\n- MacLean AI proxy logs (real Codex CLI agent sessions \u2014 most valuable source)"
  },
  {
   "cell_type": "markdown",
   "id": "8c223b20d8c73506",
   "metadata": {},
   "source": "## Step 0: Environment Setup"
  },
  {
   "cell_type": "markdown",
   "id": "d5b7cc89e998088b",
   "metadata": {},
   "source": "### 0.1 Mount Google Drive & Clone Repository"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f8caf22f35ebd8",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nIN_COLAB = \"COLAB_GPU\" in os.environ or os.path.exists(\"/content\")\n\nDRIVE_BASE = \"\"\nDRIVE_MODE = \"local\"\n\nif IN_COLAB:\n    from google.colab import drive\n    drive.mount(\"/content/drive\")\n    DRIVE_BASE = \"/content/drive/MyDrive/gpt-oss-20b-coding-tui\"\n    DRIVE_MODE = \"mounted\"\n    os.makedirs(DRIVE_BASE, exist_ok=True)\n\n    if not os.path.exists(\"llm-training-pipeline\"):\n        !git clone https://github.com/rmarnold/llm-training-pipeline.git\n    os.chdir(\"llm-training-pipeline\")\n    !git pull --ff-only\n    print(f\"Working directory: {os.getcwd()}\")\nelse:\n    print(\"Running locally (not in Colab).\")\n    print(f\"Working directory: {os.getcwd()}\")"
  },
  {
   "cell_type": "markdown",
   "id": "32511fbe9341b45c",
   "metadata": {},
   "source": "### 0.2 Install Dependencies"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565fd802759877f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys, os\n",
    "\n",
    "IN_COLAB = \"COLAB_GPU\" in os.environ or os.path.exists(\"/content\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Core + GPT-OSS deps\n",
    "    !pip install -q -e \".[gpt_oss]\"\n",
    "\n",
    "    # Unsloth (Colab optimised)\n",
    "    !pip install -q unsloth\n",
    "\n",
    "    # vLLM for fast inference in GRPO\n",
    "    !pip install -q vllm\n",
    "\n",
    "    # ipywidgets for config UI\n",
    "    !pip install -q ipywidgets\n",
    "\n",
    "    # Datasets for HuggingFace downloads\n",
    "    !pip install -q datasets huggingface_hub\n",
    "\n",
    "    # Code execution evaluation deps\n",
    "    !pip install -q pyflakes astunparse\n",
    "\n",
    "    print(\"\\nDependencies installed.\")\n",
    "else:\n",
    "    print(\"Assuming local dependencies are already installed.\")\n",
    "    print(\"Run: pip install -e '.[gpt_oss]'\")\n",
    "\n",
    "# Reduce CUDA memory fragmentation (helps with expert LoRA's large param count)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d256e0fbc370a68",
   "metadata": {},
   "source": "### 0.3 Configure Pipeline\n\nToggle the form view (click the \"...\" menu on this cell) to see the interactive configuration panel.\nAdjust settings, then **run this cell** to apply them."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e618b72f6afe400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### Pipeline Configuration { display-mode: \"form\" }\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown #### Core Settings\n",
    "\n",
    "training_scope = \"quick_test\"  #@param [\"full\", \"quick_test\", \"tool_calling_only\", \"skip_to_rl\"] {type: \"string\"}\n",
    "gpu_tier = \"h100_80gb\"  #@param [\"a100_40gb\", \"a100_80gb\", \"h100_80gb\"] {type: \"string\"}\n",
    "batch_profile = \"balanced\"  #@param [\"aggressive\", \"balanced\", \"conservative\"] {type: \"string\"}\n",
    "max_steps_override = 0  #@param {type: \"integer\"}\n",
    "\n",
    "#@markdown > **Batch Profile:**\n",
    "#@markdown > - **aggressive** \u2014 max GPU utilization, fastest training, may OOM on longest sequences\n",
    "#@markdown > - **balanced** \u2014 recommended default, good speed with expert LoRA headroom\n",
    "#@markdown > - **conservative** \u2014 safest for rank 128 or experimental configs, slowest\n",
    "#@markdown >\n",
    "#@markdown > *Max Steps Override: 0 = use GPU tier defaults. Set > 0 to cap all stages.*\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown #### Data Sources\n",
    "\n",
    "include_proxy_logs = True  #@param {type: \"boolean\"}\n",
    "proxy_log_dir = \"\"  #@param {type: \"string\"}\n",
    "\n",
    "#@markdown > *proxy_log_dir: path to MacLean AI proxy log directory (contains per-request JSON files).*\n",
    "\n",
    "include_tool_calling = True  #@param {type: \"boolean\"}\n",
    "include_agent_trajectories = True  #@param {type: \"boolean\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown #### Pipeline Phases\n",
    "\n",
    "include_preference = True  #@param {type: \"boolean\"}\n",
    "include_grpo = True  #@param {type: \"boolean\"}\n",
    "skip_data_generation = False  #@param {type: \"boolean\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown #### Export\n",
    "\n",
    "enable_qat_export = False  #@param {type: \"boolean\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown #### Advanced\n",
    "\n",
    "use_service_account = False  #@param {type: \"boolean\"}\n",
    "drive_folder_id = \"\"  #@param {type: \"string\"}\n",
    "\n",
    "# ======================================================================\n",
    "# GPU tier \u00d7 batch profile presets\n",
    "#\n",
    "# Expert LoRA trains 23x more params than attention-only (~740M at rank 64,\n",
    "# ~1.48B at rank 128). Unsloth's chunked CE and gradient offloading help,\n",
    "# but batch sizes still need to be lower than attention-only training.\n",
    "#\n",
    "# Effective batch sizes are kept consistent across profiles by adjusting\n",
    "# gradient_accumulation_steps inversely with per_device_train_batch_size.\n",
    "# ======================================================================\n",
    "import os, sys, json\n",
    "\n",
    "# Base GPU configs (non-batch settings)\n",
    "GPU_BASE = {\n",
    "    \"a100_40gb\": {\n",
    "        \"tool_calling_max_steps\": 3000, \"tool_calling_seq_len\": 4096,\n",
    "        \"agent_sft_max_steps\": 2000, \"agent_sft_seq_len\": 8192,\n",
    "        \"ipo_max_steps\": 1000, \"ipo_seq_len\": 4096,\n",
    "        \"grpo_max_steps\": 3000, \"grpo_seq_len\": 16384, \"grpo_num_gen\": 4,\n",
    "        \"eval_num_samples\": 100,\n",
    "        \"load_mode\": \"4bit\", \"moe_backend\": \"triton\", \"fast_inference\": False,\n",
    "    },\n",
    "    \"a100_80gb\": {\n",
    "        \"tool_calling_max_steps\": -1, \"tool_calling_seq_len\": 8192,\n",
    "        \"agent_sft_max_steps\": -1, \"agent_sft_seq_len\": 16384,\n",
    "        \"ipo_max_steps\": -1, \"ipo_seq_len\": 8192,\n",
    "        \"grpo_max_steps\": 5000, \"grpo_seq_len\": 32768, \"grpo_num_gen\": 4,\n",
    "        \"eval_num_samples\": 200,\n",
    "        \"load_mode\": \"4bit\", \"moe_backend\": \"triton\", \"fast_inference\": False,\n",
    "    },\n",
    "    \"h100_80gb\": {\n",
    "        \"tool_calling_max_steps\": -1, \"tool_calling_seq_len\": 8192,\n",
    "        \"agent_sft_max_steps\": -1, \"agent_sft_seq_len\": 16384,\n",
    "        \"ipo_max_steps\": -1, \"ipo_seq_len\": 8192,\n",
    "        \"grpo_max_steps\": 5000, \"grpo_seq_len\": 65536, \"grpo_num_gen\": 4,\n",
    "        \"eval_num_samples\": 200,\n",
    "        \"load_mode\": \"fp8\", \"moe_backend\": \"triton\", \"fast_inference\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Batch profiles per GPU tier: (batch, grad_accum) tuples\n",
    "# Effective batch = batch \u00d7 grad_accum (kept ~constant within each tier)\n",
    "BATCH_PROFILES = {\n",
    "    \"h100_80gb\": {\n",
    "        #                          tool_call    agent_sft    ipo          grpo\n",
    "        #                          b   ga  eff  b   ga  eff  b   ga  eff  b   ga  eff\n",
    "        \"aggressive\": {            # Fastest \u2014 max GPU util, may OOM on longest seqs\n",
    "            \"tool_calling_batch\": 10, \"tool_calling_grad_accum\": 5,   # eff=50\n",
    "            \"agent_sft_batch\": 4,    \"agent_sft_grad_accum\": 6,      # eff=24\n",
    "            \"ipo_batch\": 4,          \"ipo_grad_accum\": 8,            # eff=32\n",
    "            \"grpo_batch\": 2,         \"grpo_grad_accum\": 8,           # eff=16\n",
    "        },\n",
    "        \"balanced\": {              # Recommended \u2014 good speed + expert LoRA headroom\n",
    "            \"tool_calling_batch\": 8,  \"tool_calling_grad_accum\": 6,  # eff=48\n",
    "            \"agent_sft_batch\": 2,    \"agent_sft_grad_accum\": 12,     # eff=24\n",
    "            \"ipo_batch\": 2,          \"ipo_grad_accum\": 16,           # eff=32\n",
    "            \"grpo_batch\": 1,         \"grpo_grad_accum\": 16,          # eff=16\n",
    "        },\n",
    "        \"conservative\": {          # Safest \u2014 for rank 128 or long-context experiments\n",
    "            \"tool_calling_batch\": 4,  \"tool_calling_grad_accum\": 12, # eff=48\n",
    "            \"agent_sft_batch\": 1,    \"agent_sft_grad_accum\": 24,     # eff=24\n",
    "            \"ipo_batch\": 1,          \"ipo_grad_accum\": 32,           # eff=32\n",
    "            \"grpo_batch\": 1,         \"grpo_grad_accum\": 16,          # eff=16\n",
    "        },\n",
    "    },\n",
    "    \"a100_80gb\": {\n",
    "        \"aggressive\": {\n",
    "            \"tool_calling_batch\": 4,  \"tool_calling_grad_accum\": 8,  # eff=32\n",
    "            \"agent_sft_batch\": 2,    \"agent_sft_grad_accum\": 4,      # eff=8\n",
    "            \"ipo_batch\": 2,          \"ipo_grad_accum\": 8,            # eff=16\n",
    "            \"grpo_batch\": 1,         \"grpo_grad_accum\": 8,           # eff=8\n",
    "        },\n",
    "        \"balanced\": {\n",
    "            \"tool_calling_batch\": 2,  \"tool_calling_grad_accum\": 16, # eff=32\n",
    "            \"agent_sft_batch\": 1,    \"agent_sft_grad_accum\": 8,      # eff=8\n",
    "            \"ipo_batch\": 1,          \"ipo_grad_accum\": 16,           # eff=16\n",
    "            \"grpo_batch\": 1,         \"grpo_grad_accum\": 8,           # eff=8\n",
    "        },\n",
    "        \"conservative\": {\n",
    "            \"tool_calling_batch\": 1,  \"tool_calling_grad_accum\": 32, # eff=32\n",
    "            \"agent_sft_batch\": 1,    \"agent_sft_grad_accum\": 8,      # eff=8\n",
    "            \"ipo_batch\": 1,          \"ipo_grad_accum\": 16,           # eff=16\n",
    "            \"grpo_batch\": 1,         \"grpo_grad_accum\": 8,           # eff=8\n",
    "        },\n",
    "    },\n",
    "    \"a100_40gb\": {\n",
    "        \"aggressive\": {\n",
    "            \"tool_calling_batch\": 2,  \"tool_calling_grad_accum\": 16, # eff=32\n",
    "            \"agent_sft_batch\": 1,    \"agent_sft_grad_accum\": 8,      # eff=8\n",
    "            \"ipo_batch\": 1,          \"ipo_grad_accum\": 16,           # eff=16\n",
    "            \"grpo_batch\": 1,         \"grpo_grad_accum\": 8,           # eff=8\n",
    "        },\n",
    "        \"balanced\": {\n",
    "            \"tool_calling_batch\": 2,  \"tool_calling_grad_accum\": 16, # eff=32\n",
    "            \"agent_sft_batch\": 1,    \"agent_sft_grad_accum\": 8,      # eff=8\n",
    "            \"ipo_batch\": 1,          \"ipo_grad_accum\": 16,           # eff=16\n",
    "            \"grpo_batch\": 1,         \"grpo_grad_accum\": 8,           # eff=8\n",
    "        },\n",
    "        \"conservative\": {\n",
    "            \"tool_calling_batch\": 1,  \"tool_calling_grad_accum\": 32, # eff=32\n",
    "            \"agent_sft_batch\": 1,    \"agent_sft_grad_accum\": 8,      # eff=8\n",
    "            \"ipo_batch\": 1,          \"ipo_grad_accum\": 16,           # eff=16\n",
    "            \"grpo_batch\": 1,         \"grpo_grad_accum\": 8,           # eff=8\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# Merge base config + batch profile\n",
    "tier = {**GPU_BASE[gpu_tier], **BATCH_PROFILES[gpu_tier][batch_profile]}\n",
    "\n",
    "# ======================================================================\n",
    "# Build CONFIG dict from form values\n",
    "# ======================================================================\n",
    "CONFIG = {\n",
    "    \"training_scope\": training_scope,\n",
    "    \"gpu_tier\": gpu_tier,\n",
    "    \"batch_profile\": batch_profile,\n",
    "    **tier,\n",
    "    # Data sources\n",
    "    \"include_proxy_logs\": include_proxy_logs,\n",
    "    \"proxy_log_dir\": proxy_log_dir,\n",
    "    \"include_tool_calling\": include_tool_calling,\n",
    "    \"include_agent_trajectories\": include_agent_trajectories,\n",
    "    # Pipeline phases\n",
    "    \"include_preference\": include_preference,\n",
    "    \"include_grpo\": include_grpo,\n",
    "    \"enable_qat_export\": enable_qat_export,\n",
    "    \"skip_data_generation\": skip_data_generation,\n",
    "    # Advanced\n",
    "    \"use_service_account\": use_service_account,\n",
    "    \"drive_folder_id\": drive_folder_id,\n",
    "}\n",
    "\n",
    "# Apply max_steps override\n",
    "if max_steps_override > 0:\n",
    "    for key in list(CONFIG.keys()):\n",
    "        if key.endswith(\"_max_steps\"):\n",
    "            CONFIG[key] = max_steps_override\n",
    "\n",
    "# Quick test caps\n",
    "if CONFIG[\"training_scope\"] == \"quick_test\":\n",
    "    for key in list(CONFIG.keys()):\n",
    "        if key.endswith(\"_max_steps\"):\n",
    "            CONFIG[key] = min(CONFIG[key], 50)\n",
    "    CONFIG[\"eval_num_samples\"] = 10\n",
    "\n",
    "# Scope-based overrides\n",
    "if CONFIG[\"training_scope\"] == \"tool_calling_only\":\n",
    "    CONFIG[\"include_preference\"] = False\n",
    "    CONFIG[\"include_grpo\"] = False\n",
    "    CONFIG[\"include_agent_trajectories\"] = False\n",
    "elif CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "    CONFIG[\"include_tool_calling\"] = False\n",
    "    CONFIG[\"include_agent_trajectories\"] = False\n",
    "\n",
    "# ======================================================================\n",
    "# Set up DriveHelper\n",
    "# ======================================================================\n",
    "sys.path.insert(0, \"scripts\")\n",
    "from pipeline_lib.drive_utils import DriveHelper\n",
    "\n",
    "if \"DRIVE_BASE\" not in dir():\n",
    "    DRIVE_BASE = \"\"\n",
    "if \"DRIVE_MODE\" not in dir():\n",
    "    DRIVE_MODE = \"local\"\n",
    "\n",
    "if CONFIG[\"use_service_account\"] and CONFIG[\"drive_folder_id\"]:\n",
    "    sa_path = \"service_account.json\"\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        sa_json = userdata.get(\"SERVICE_ACCOUNT_JSON\")\n",
    "        with open(sa_path, \"w\") as f:\n",
    "            f.write(sa_json)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if os.path.exists(sa_path) and os.path.getsize(sa_path) > 10:\n",
    "        try:\n",
    "            drive_helper = DriveHelper(\n",
    "                mode=\"service_account\",\n",
    "                credentials_path=sa_path,\n",
    "                folder_id=CONFIG[\"drive_folder_id\"],\n",
    "            )\n",
    "            DRIVE_MODE = \"service_account\"\n",
    "        except Exception as e:\n",
    "            print(f\"Service account failed: {e}\")\n",
    "            drive_helper = DriveHelper(mode=\"local\")\n",
    "            DRIVE_MODE = \"local\"\n",
    "    else:\n",
    "        drive_helper = DriveHelper(mode=\"local\")\n",
    "        DRIVE_MODE = \"local\"\n",
    "elif DRIVE_BASE:\n",
    "    drive_helper = DriveHelper(mode=\"mounted\", drive_base=DRIVE_BASE)\n",
    "    DRIVE_MODE = \"mounted\"\n",
    "else:\n",
    "    drive_helper = DriveHelper(mode=\"local\")\n",
    "    DRIVE_MODE = \"local\"\n",
    "\n",
    "# Save for persistence across restarts\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "with open(\"data/config_coding_tui.json\", \"w\") as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "# ======================================================================\n",
    "# Print summary\n",
    "# ======================================================================\n",
    "# Compute effective batch sizes for display\n",
    "tc_eff = CONFIG[\"tool_calling_batch\"] * CONFIG[\"tool_calling_grad_accum\"]\n",
    "as_eff = CONFIG[\"agent_sft_batch\"] * CONFIG[\"agent_sft_grad_accum\"]\n",
    "ip_eff = CONFIG[\"ipo_batch\"] * CONFIG[\"ipo_grad_accum\"]\n",
    "gr_eff = CONFIG[\"grpo_batch\"] * CONFIG[\"grpo_grad_accum\"]\n",
    "\n",
    "profile_label = {\n",
    "    \"aggressive\": \"AGGRESSIVE (fastest, may OOM)\",\n",
    "    \"balanced\": \"BALANCED (recommended)\",\n",
    "    \"conservative\": \"CONSERVATIVE (safest, slowest)\",\n",
    "}[batch_profile]\n",
    "\n",
    "print(\"=\" * 62)\n",
    "print(\"  PIPELINE CONFIGURATION (Coding TUI Agent)\")\n",
    "print(\"=\" * 62)\n",
    "print(f\"  Scope:           {CONFIG['training_scope'].upper()}\")\n",
    "print(f\"  GPU tier:        {CONFIG['gpu_tier']}\")\n",
    "print(f\"  Batch profile:   {profile_label}\")\n",
    "print(f\"  MoE backend:     {CONFIG['moe_backend']}\")\n",
    "print(f\"  Load mode:       {CONFIG['load_mode']}\")\n",
    "print(f\"  Drive mode:      {DRIVE_MODE}\")\n",
    "print()\n",
    "print(f\"  Proxy logs:      {CONFIG['include_proxy_logs']}\")\n",
    "if CONFIG[\"include_proxy_logs\"] and CONFIG[\"proxy_log_dir\"]:\n",
    "    print(f\"    Log dir:       {CONFIG['proxy_log_dir']}\")\n",
    "print(f\"  Tool calling:    {CONFIG['include_tool_calling']}\")\n",
    "print(f\"  Agent traj:      {CONFIG['include_agent_trajectories']}\")\n",
    "print(f\"  IPO preference:  {CONFIG['include_preference']}\")\n",
    "print(f\"  GRPO:            {CONFIG['include_grpo']}\")\n",
    "print(f\"  QAT export:      {CONFIG['enable_qat_export']}\")\n",
    "if CONFIG[\"skip_data_generation\"]:\n",
    "    print(f\"  DATA GEN:        *** SKIPPED (skip_data_generation=True) ***\")\n",
    "if max_steps_override > 0:\n",
    "    print(f\"  Max steps:       {max_steps_override} (override)\")\n",
    "print()\n",
    "print(f\"  Tool Calling SFT: batch={CONFIG['tool_calling_batch']} x grad={CONFIG['tool_calling_grad_accum']} = {tc_eff}, seq={CONFIG['tool_calling_seq_len']}, steps={CONFIG['tool_calling_max_steps']}\")\n",
    "print(f\"  Agent SFT:        batch={CONFIG['agent_sft_batch']} x grad={CONFIG['agent_sft_grad_accum']} = {as_eff}, seq={CONFIG['agent_sft_seq_len']}, steps={CONFIG['agent_sft_max_steps']}\")\n",
    "if CONFIG[\"include_preference\"]:\n",
    "    print(f\"  IPO:              batch={CONFIG['ipo_batch']} x grad={CONFIG['ipo_grad_accum']} = {ip_eff}, seq={CONFIG['ipo_seq_len']}, steps={CONFIG['ipo_max_steps']}\")\n",
    "if CONFIG[\"include_grpo\"]:\n",
    "    print(f\"  GRPO:             batch={CONFIG['grpo_batch']} x grad={CONFIG['grpo_grad_accum']} = {gr_eff}, seq={CONFIG['grpo_seq_len']}, steps={CONFIG['grpo_max_steps']}\")\n",
    "print(\"=\" * 62)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6573f767ff170f",
   "metadata": {},
   "source": "### 0.4 Pipeline Dashboard"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d75422c4badeb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\nfrom IPython.display import display\n\nclass PipelineTracker:\n    \"\"\"Track pipeline progress with visual indicators.\"\"\"\n\n    PHASES = [\n        (\"tool_calling_data\", \"Tool Calling Data\"),\n        (\"agent_traj_data\", \"Agent Trajectory Data\"),\n        (\"proxy_log_extract\", \"Proxy Log Extraction\"),\n        (\"tool_calling_sft\", \"Tool Calling SFT\"),\n        (\"merge\", \"Merge Adapter\"),\n        (\"agent_sft\", \"Agent SFT\"),\n        (\"ipo\", \"IPO Preference\"),\n        (\"grpo\", \"GRPO RL\"),\n        (\"eval\", \"Evaluation\"),\n        (\"export\", \"Export\"),\n    ]\n\n    def __init__(self):\n        self._bars = {}\n        self._labels = {}\n        rows = []\n        for key, name in self.PHASES:\n            label = widgets.HTML(\n                value=f\"<span style='color:#888'>&#x25CB; {name}</span>\",\n                layout=widgets.Layout(width=\"240px\"),\n            )\n            bar = widgets.FloatProgress(\n                value=0, min=0, max=1.0,\n                bar_style=\"info\",\n                layout=widgets.Layout(width=\"300px\", height=\"18px\"),\n            )\n            self._bars[key] = bar\n            self._labels[key] = label\n            rows.append(widgets.HBox([label, bar]))\n        self._container = widgets.VBox(rows)\n        display(widgets.HTML(\"<b>Pipeline Progress</b>\"))\n        display(self._container)\n\n    def start(self, phase):\n        self._labels[phase].value = (\n            f\"<span style='color:#2196F3'>&#x25B6; {dict(self.PHASES)[phase]}</span>\"\n        )\n        self._bars[phase].value = 0.1\n        self._bars[phase].bar_style = \"info\"\n\n    def complete(self, phase):\n        self._labels[phase].value = (\n            f\"<span style='color:#4CAF50'>&#x2714; {dict(self.PHASES)[phase]}</span>\"\n        )\n        self._bars[phase].value = 1.0\n        self._bars[phase].bar_style = \"success\"\n\n    def skip(self, phase):\n        self._labels[phase].value = (\n            f\"<span style='color:#9E9E9E'>&#x2014; {dict(self.PHASES)[phase]} (skipped)</span>\"\n        )\n        self._bars[phase].value = 1.0\n        self._bars[phase].bar_style = \"\"\n\n    def fail(self, phase):\n        self._labels[phase].value = (\n            f\"<span style='color:#F44336'>&#x2718; {dict(self.PHASES)[phase]}</span>\"\n        )\n        self._bars[phase].bar_style = \"danger\"\n\ntracker = PipelineTracker()\n\n# Quality gate results \u2014 populated after each training phase\ngate_report = {\n    \"tool_calling\": {\"passed\": None, \"loss\": None, \"notes\": []},\n    \"agent_sft\": {\"passed\": None, \"loss\": None, \"notes\": []},\n    \"ipo\": {\"passed\": None, \"loss\": None, \"notes\": []},\n    \"grpo\": {\"passed\": None, \"loss\": None, \"notes\": []},\n}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a29921984a191c6",
   "metadata": {},
   "source": "### 0.5 Set Up Persistent Storage"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac3698fb40edf9a",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nDRIVE_SUBDIRS = [\n    \"data/coding_tui/tool_calling\",\n    \"data/coding_tui/agent_traj\",\n    \"data/coding_tui/proxy_logs\",\n    \"data/coding_tui/preference\",\n    \"data/coding_tui/grpo\",\n    \"data/coding_tui/eval\",\n    \"checkpoints/tool_calling_sft\",\n    \"checkpoints/gpt-oss-20b-coding-tui-merged\",\n    \"checkpoints/agent_sft\",\n    \"checkpoints/agent_sft_ipo\",\n    \"checkpoints/agent_sft_grpo\",\n    \"evals\",\n]\n\nif DRIVE_MODE == \"mounted\":\n    for subdir in DRIVE_SUBDIRS:\n        drive_path = os.path.join(DRIVE_BASE, subdir)\n        os.makedirs(drive_path, exist_ok=True)\n        local_path = subdir\n        if not os.path.exists(local_path):\n            os.makedirs(os.path.dirname(local_path) or \".\", exist_ok=True)\n            os.symlink(drive_path, local_path)\n            print(f\"  Linked: {local_path} -> {drive_path}\")\n        else:\n            print(f\"  Exists: {local_path}\")\n    print(f\"\\nDrive base: {DRIVE_BASE}\")\nelif DRIVE_MODE == \"service_account\":\n    for subdir in DRIVE_SUBDIRS:\n        os.makedirs(subdir, exist_ok=True)\n        drive_helper.ensure_dir(subdir)\n    print(\"Drive directories created (service account mode).\")\nelse:\n    for subdir in DRIVE_SUBDIRS:\n        os.makedirs(subdir, exist_ok=True)\n    print(\"Local directories created (no Drive backup).\")"
  },
  {
   "cell_type": "markdown",
   "id": "160ebeffc16f9fab",
   "metadata": {},
   "source": "### 0.6 Check GPU & Configure MoE Backend"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460744279495b571",
   "metadata": {},
   "outputs": [],
   "source": "import torch, os\n\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n\n    print(f\"GPU: {gpu_name}\")\n    print(f\"VRAM: {gpu_mem:.1f} GB\")\n\n    # Auto-detect GPU tier override\n    detected_tier = None\n    if \"H100\" in gpu_name or \"H200\" in gpu_name:\n        detected_tier = \"h100_80gb\"\n    elif \"A100\" in gpu_name:\n        detected_tier = \"a100_80gb\" if gpu_mem > 45 else \"a100_40gb\"\n\n    if detected_tier and detected_tier != CONFIG[\"gpu_tier\"]:\n        print(f\"\\n  Auto-override: {CONFIG['gpu_tier']} -> {detected_tier}\")\n        CONFIG[\"gpu_tier\"] = detected_tier\n        tier = GPU_CONFIGS[detected_tier]\n        for k, v in tier.items():\n            CONFIG[k] = v\n        print(f\"  Updated CONFIG with {detected_tier} presets.\")\n\n    # Set MoE backend\n    os.environ[\"UNSLOTH_MOE_BACKEND\"] = CONFIG.get(\"moe_backend\", \"triton\")\n    print(f\"\\n  MoE backend: {os.environ['UNSLOTH_MOE_BACKEND']}\")\n    print(f\"  Load mode: {CONFIG['load_mode']}\")\n    print(f\"  Fast inference: {CONFIG.get('fast_inference', False)}\")\n\n    # FP8 detection\n    if CONFIG[\"load_mode\"] == \"fp8\":\n        try:\n            import transformer_engine\n            print(\"  FP8: transformer-engine available\")\n        except ImportError:\n            print(\"  FP8: transformer-engine not found, falling back to 4bit\")\n            CONFIG[\"load_mode\"] = \"4bit\"\nelse:\n    print(\"No GPU detected! Training will fail.\")\n    print(\"Enable GPU: Runtime -> Change runtime type -> GPU\")\n\nprint(f\"\\nFinal config: scope={CONFIG['training_scope']}, tier={CONFIG['gpu_tier']}\")"
  },
  {
   "cell_type": "markdown",
   "id": "9479da8417afda5e",
   "metadata": {},
   "source": "## Step 1: Data Preparation"
  },
  {
   "cell_type": "markdown",
   "id": "33b4d7c5bfaf7744",
   "metadata": {},
   "source": "### 1.1 Download Tool Calling Datasets\n\nDownloads and formats three tool/function calling datasets into Harmony format:\n- **Glaive v2** (113K): high-quality synthetic function calling conversations\n- **xLAM-60K** (60K): diverse function calling from Salesforce\n- **Hermes v1**: NousResearch curated function calling data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66623c5c43db762b",
   "metadata": {},
   "outputs": [],
   "source": "import os, json, sys\n\nsys.path.insert(0, \"scripts\")\nfrom dataset_formatters.function_calling import (\n    format_glaive_function_calling,\n    format_hermes_function_calling,\n)\nfrom dataset_formatters.harmony import encode_harmony_messages\n\nTOOL_CALLING_DATASETS = [\n    (\"glaiveai/glaive-function-calling-v2\", \"glaive\", 113000),\n    (\"Salesforce/xlam-function-calling-60k\", \"xlam\", 60000),\n    (\"NousResearch/hermes-function-calling-v1\", \"hermes\", None),\n]\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# xLAM inline formatter\n# xLAM format: {\"query\": str, \"tools\": str (JSON array), \"answers\": str (JSON array)}\n# Output: Harmony tool call format\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndef format_xlam_function_calling(example):\n    \"\"\"Format Salesforce xLAM function calling data into Harmony format.\n\n    xLAM schema:\n        query   - natural language user request\n        tools   - JSON-encoded list of tool schemas (name/description/parameters)\n        answers - JSON-encoded list of call dicts [{name: ..., arguments: {...}}]\n\n    Returns Harmony-encoded text with developer context, user query, and\n    one tool_call per answer entry in the assistant turn.\n    \"\"\"\n    query = example.get(\"query\", \"\").strip()\n    tools_raw = example.get(\"tools\", \"[]\")\n    answers_raw = example.get(\"answers\", \"[]\")\n\n    if not query:\n        return {\"text\": \"\"}\n\n    try:\n        tools = json.loads(tools_raw) if isinstance(tools_raw, str) else tools_raw\n    except (json.JSONDecodeError, TypeError):\n        tools = []\n\n    try:\n        answers = json.loads(answers_raw) if isinstance(answers_raw, str) else answers_raw\n    except (json.JSONDecodeError, TypeError):\n        answers = []\n\n    if not answers:\n        return {\"text\": \"\"}\n\n    # Build tool schema description for developer context\n    tool_descriptions = []\n    for t in tools:\n        name = t.get(\"name\", \"unknown\")\n        desc = t.get(\"description\", \"\")\n        params = t.get(\"parameters\", {})\n        tool_descriptions.append(\n            f\"  {name}: {desc}\\n    Parameters: {json.dumps(params, separators=(',', ':'))}\"\n        )\n    tool_ctx = \"\\n\".join(tool_descriptions) if tool_descriptions else \"No tools defined.\"\n\n    dev_instructions = (\n        \"You are a helpful assistant with access to tools. \"\n        \"Call the appropriate tool with valid parameters based on the user's request.\\n\\n\"\n        f\"Available tools:\\n{tool_ctx}\"\n    )\n\n    # Build tool_calls list from answers\n    tool_calls = []\n    for i, ans in enumerate(answers):\n        tc_name = ans.get(\"name\", \"\")\n        tc_args = ans.get(\"arguments\", ans.get(\"parameters\", {}))\n        if not tc_name:\n            continue\n        tool_calls.append({\n            \"id\": f\"call_{i}\",\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": tc_name,\n                \"arguments\": json.dumps(tc_args) if isinstance(tc_args, dict) else str(tc_args),\n            },\n        })\n\n    if not tool_calls:\n        return {\"text\": \"\"}\n\n    messages = [\n        {\"role\": \"user\", \"content\": query},\n        {\"role\": \"assistant\", \"tool_calls\": tool_calls},\n    ]\n\n    return {\"text\": encode_harmony_messages(messages, developer_instructions=dev_instructions)}\n\n\nif CONFIG[\"skip_data_generation\"] or not CONFIG[\"include_tool_calling\"]:\n    print(\"Skipping tool calling data download.\")\n    tracker.skip(\"tool_calling_data\")\nelse:\n    tracker.start(\"tool_calling_data\")\n    from datasets import load_dataset, Dataset, concatenate_datasets\n\n    all_tool_calling = []\n    stats = {}\n\n    for ds_name, ds_key, max_samples in TOOL_CALLING_DATASETS:\n        print(f\"\\nDownloading {ds_name}...\")\n        try:\n            if ds_key == \"glaive\":\n                raw = load_dataset(ds_name, split=\"train\")\n                if CONFIG[\"training_scope\"] == \"quick_test\":\n                    raw = raw.select(range(min(500, len(raw))))\n                elif max_samples:\n                    raw = raw.select(range(min(max_samples, len(raw))))\n                formatted = raw.map(format_glaive_function_calling, remove_columns=raw.column_names)\n\n            elif ds_key == \"xlam\":\n                raw = load_dataset(ds_name, split=\"train\")\n                if CONFIG[\"training_scope\"] == \"quick_test\":\n                    raw = raw.select(range(min(200, len(raw))))\n                elif max_samples:\n                    raw = raw.select(range(min(max_samples, len(raw))))\n                formatted = raw.map(format_xlam_function_calling, remove_columns=raw.column_names)\n\n            elif ds_key == \"hermes\":\n                raw = load_dataset(ds_name, split=\"train\")\n                if CONFIG[\"training_scope\"] == \"quick_test\":\n                    raw = raw.select(range(min(300, len(raw))))\n                formatted = raw.map(format_hermes_function_calling, remove_columns=raw.column_names)\n\n            else:\n                continue\n\n            # Filter empty examples\n            formatted = formatted.filter(lambda x: bool(x.get(\"text\", \"\").strip()))\n            stats[ds_key] = len(formatted)\n            all_tool_calling.append(formatted)\n            print(f\"  Formatted: {len(formatted):,} examples\")\n\n        except Exception as e:\n            print(f\"  WARNING: failed to load {ds_name}: {e}\")\n            stats[ds_key] = 0\n\n    if all_tool_calling:\n        combined = concatenate_datasets(all_tool_calling)\n        combined = combined.shuffle(seed=42)\n        out_path = \"data/coding_tui/tool_calling/train\"\n        combined.save_to_disk(out_path)\n        print(f\"\\nTotal tool calling examples: {len(combined):,} -> {out_path}\")\n    else:\n        print(\"WARNING: no tool calling data collected.\")\n\n    drive_helper.backup(\"data/coding_tui/tool_calling\", \"data/coding_tui/tool_calling\")\n    if DRIVE_MODE != \"local\":\n        print(\"Backed up to Drive.\")\n\n    tracker.complete(\"tool_calling_data\")"
  },
  {
   "cell_type": "markdown",
   "id": "d50f4da79bd9ec1a",
   "metadata": {},
   "source": "### 1.2 Download Agent Trajectory Datasets\n\nDownloads and formats agent trajectory datasets into Harmony agent format:\n- **code-act**: multi-step code execution agent trajectories\n- **commitpack**: commit-based code change examples\n- **editpackft**: instruction-following code edits"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1633fd3a9ccab688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json\n\nsys.path.insert(0, \"scripts\")\nfrom dataset_formatters.harmony import format_harmony_agent, encode_harmony_messages\n\nCODING_AGENT_DEV_PROMPT = (\n    \"You are a coding agent. Use tools to read files, write code, run tests, and \"\n    \"complete programming tasks. Do not just analyze \u2014 always take action and produce \"\n    \"working code. After making changes, verify they work by running the relevant tests. \"\n    \"If a tool call fails, diagnose and retry with corrected parameters.\"\n)\n\n\ndef format_code_act_example(example):\n    \"\"\"Format code-act dataset into Harmony agent trajectory format.\"\"\"\n    conversations = example.get(\"conversations\", [])\n    if not conversations:\n        return {\"text\": \"\"}\n\n    messages = []\n    for turn in conversations:\n        role = turn.get(\"role\", turn.get(\"from\", \"\"))\n        content = turn.get(\"content\", turn.get(\"value\", \"\"))\n        if not content:\n            continue\n        if role in (\"human\", \"user\"):\n            messages.append({\"role\": \"user\", \"content\": content})\n        elif role in (\"gpt\", \"assistant\"):\n            messages.append({\"role\": \"assistant\", \"content\": content})\n        elif role in (\"tool\", \"function\", \"observation\"):\n            messages.append({\"role\": \"tool\", \"content\": content})\n\n    if len(messages) < 2:\n        return {\"text\": \"\"}\n\n    return {\"text\": encode_harmony_messages(\n        messages,\n        developer_instructions=CODING_AGENT_DEV_PROMPT,\n        reasoning_effort=\"high\",\n    )}\n\n\ndef format_commitpackft_example(example):\n    \"\"\"Format commitpackft (commit message + code diff) into Harmony format.\"\"\"\n    subject = example.get(\"subject\", \"\")\n    message = example.get(\"message\", subject)\n    old_code = example.get(\"old_contents\", \"\")\n    new_code = example.get(\"new_contents\", \"\")\n\n    if not old_code or not new_code or not message:\n        return {\"text\": \"\"}\n    if old_code.strip() == new_code.strip():\n        return {\"text\": \"\"}\n    if len(old_code) > 8000 or len(new_code) > 8000:\n        return {\"text\": \"\"}\n\n    user_content = (\n        f\"Apply the following change to the code:\\n\\n\"\n        f\"Commit message: {message}\\n\\n\"\n        f\"Current code:\\n```\\n{old_code}\\n```\"\n    )\n    assistant_content = f\"```\\n{new_code}\\n```\"\n\n    messages = [\n        {\"role\": \"user\", \"content\": user_content},\n        {\"role\": \"assistant\", \"content\": assistant_content},\n    ]\n\n    return {\"text\": encode_harmony_messages(\n        messages,\n        developer_instructions=CODING_AGENT_DEV_PROMPT,\n        reasoning_effort=\"medium\",\n    )}\n\n\ndef format_editpackft_example(example):\n    \"\"\"Format nuprl/EditPackFT (instruction-following code edits) into Harmony format.\"\"\"\n    instruction = example.get(\"instruction\", \"\")\n    old_code = example.get(\"old_contents\", example.get(\"input\", \"\"))\n    new_code = example.get(\"new_contents\", example.get(\"output\", \"\"))\n\n    if not instruction or not old_code or not new_code:\n        return {\"text\": \"\"}\n    if old_code.strip() == new_code.strip():\n        return {\"text\": \"\"}\n    if len(old_code) > 8000 or len(new_code) > 8000:\n        return {\"text\": \"\"}\n\n    user_content = (\n        f\"{instruction}\\n\\n\"\n        f\"Code:\\n```\\n{old_code}\\n```\"\n    )\n    assistant_content = f\"```\\n{new_code}\\n```\"\n\n    messages = [\n        {\"role\": \"user\", \"content\": user_content},\n        {\"role\": \"assistant\", \"content\": assistant_content},\n    ]\n\n    return {\"text\": encode_harmony_messages(\n        messages,\n        developer_instructions=CODING_AGENT_DEV_PROMPT,\n        reasoning_effort=\"medium\",\n    )}\n\n\nif CONFIG[\"skip_data_generation\"] or not CONFIG[\"include_agent_trajectories\"]:\n    print(\"Skipping agent trajectory data download.\")\n    tracker.skip(\"agent_traj_data\")\nelse:\n    tracker.start(\"agent_traj_data\")\n    from datasets import load_dataset, Dataset, concatenate_datasets\n\n    all_agent_traj = []\n\n    # 1. xingyaoww/code-act \u2014 splits are \"codeact\" and \"general\", NOT \"train\"\n    print(\"\\nDownloading xingyaoww/code-act...\")\n    try:\n        codeact_split = load_dataset(\"xingyaoww/code-act\", split=\"codeact\")\n        general_split = load_dataset(\"xingyaoww/code-act\", split=\"general\")\n        raw = concatenate_datasets([codeact_split, general_split])\n        print(f\"  Loaded: {len(codeact_split):,} codeact + {len(general_split):,} general = {len(raw):,} total\")\n        if CONFIG[\"training_scope\"] == \"quick_test\":\n            raw = raw.select(range(min(200, len(raw))))\n        formatted = raw.map(format_code_act_example, remove_columns=raw.column_names)\n        formatted = formatted.filter(lambda x: bool(x.get(\"text\", \"\").strip()))\n        all_agent_traj.append(formatted)\n        print(f\"  Formatted: {len(formatted):,} examples\")\n    except Exception as e:\n        print(f\"  WARNING: failed to load xingyaoww/code-act: {e}\")\n\n    # 2. bigcode/commitpackft \u2014 Parquet-based replacement for commitpack (script loading deprecated)\n    #    Load multiple languages for coding diversity\n    print(\"\\nDownloading bigcode/commitpackft...\")\n    try:\n        commitpack_parts = []\n        languages = [\"python\", \"javascript\", \"go\", \"rust\", \"java\", \"typescript\"]\n        per_lang = 10000 if CONFIG[\"training_scope\"] != \"quick_test\" else 50\n        for lang in languages:\n            try:\n                lang_ds = load_dataset(\"bigcode/commitpackft\", lang, split=\"train\")\n                lang_ds = lang_ds.select(range(min(per_lang, len(lang_ds))))\n                commitpack_parts.append(lang_ds)\n                print(f\"  {lang}: {len(lang_ds):,} examples\")\n            except Exception as lang_e:\n                print(f\"  {lang}: skipped ({lang_e})\")\n        if commitpack_parts:\n            raw = concatenate_datasets(commitpack_parts)\n            formatted = raw.map(format_commitpackft_example, remove_columns=raw.column_names)\n            formatted = formatted.filter(lambda x: bool(x.get(\"text\", \"\").strip()))\n            all_agent_traj.append(formatted)\n            print(f\"  Formatted: {len(formatted):,} examples total\")\n    except Exception as e:\n        print(f\"  WARNING: failed to load bigcode/commitpackft: {e}\")\n\n    # 3. nuprl/EditPackFT \u2014 correct path (not bigcode/editpackft which doesn't exist)\n    print(\"\\nDownloading nuprl/EditPackFT...\")\n    try:\n        raw = load_dataset(\"nuprl/EditPackFT\", split=\"train\")\n        print(f\"  Loaded: {len(raw):,} examples\")\n        if CONFIG[\"training_scope\"] == \"quick_test\":\n            raw = raw.select(range(min(300, len(raw))))\n        formatted = raw.map(format_editpackft_example, remove_columns=raw.column_names)\n        formatted = formatted.filter(lambda x: bool(x.get(\"text\", \"\").strip()))\n        all_agent_traj.append(formatted)\n        print(f\"  Formatted: {len(formatted):,} examples\")\n    except Exception as e:\n        print(f\"  WARNING: failed to load nuprl/EditPackFT: {e}\")\n\n    if all_agent_traj:\n        combined = concatenate_datasets(all_agent_traj)\n        combined = combined.shuffle(seed=42)\n        out_path = \"data/coding_tui/agent_traj/train\"\n        combined.save_to_disk(out_path)\n        print(f\"\\nTotal agent trajectory examples: {len(combined):,} -> {out_path}\")\n    else:\n        print(\"WARNING: no agent trajectory data collected.\")\n\n    drive_helper.backup(\"data/coding_tui/agent_traj\", \"data/coding_tui/agent_traj\")\n    if DRIVE_MODE != \"local\":\n        print(\"Backed up to Drive.\")\n\n    tracker.complete(\"agent_traj_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b115a2a45b5208d",
   "metadata": {},
   "source": "### 1.3 Download Preference Datasets\n\nDownloads and formats preference datasets for IPO training.\nGood responses (task completed, decisive action) are paired against bad ones (circular analysis, incomplete)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c0ba32cdc76d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, re\n\nsys.path.insert(0, \"scripts\")\nfrom dataset_formatters.harmony import format_harmony_preference, encode_harmony_messages\n\nCODING_PREF_DEV = \"You are a helpful coding assistant. Provide complete, working code solutions.\"\n\n\ndef format_hh_rlhf_example(example):\n    \"\"\"Format Anthropic HH-RLHF into Harmony preference pairs.\n\n    hh-rlhf format: {\"chosen\": str, \"rejected\": str}\n    Both are conversation strings with \\\\n\\\\nHuman: / \\\\n\\\\nAssistant: turns.\n\n    Returns only \"text\" key to avoid column name conflicts with remove_columns.\n    The chosen/rejected/prompt fields are stored as JSON in a \"metadata\" column.\n    \"\"\"\n    chosen_raw = example.get(\"chosen\") or \"\"\n    rejected_raw = example.get(\"rejected\") or \"\"\n\n    if not chosen_raw or not rejected_raw:\n        return {\"text\": \"\", \"pref_chosen\": \"\", \"pref_rejected\": \"\", \"pref_prompt\": \"\"}\n\n    def parse_conversation(raw):\n        messages = []\n        parts = re.split(r'\\n\\n(Human|Assistant):\\s*', raw)\n        current_role = None\n        for part in parts:\n            stripped = part.strip()\n            if stripped == \"Human\":\n                current_role = \"user\"\n            elif stripped == \"Assistant\":\n                current_role = \"assistant\"\n            elif stripped and current_role:\n                messages.append({\"role\": current_role, \"content\": stripped})\n                current_role = None\n        return messages\n\n    chosen_msgs = parse_conversation(chosen_raw)\n    rejected_msgs = parse_conversation(rejected_raw)\n\n    if not chosen_msgs or not rejected_msgs:\n        return {\"text\": \"\", \"pref_chosen\": \"\", \"pref_rejected\": \"\", \"pref_prompt\": \"\"}\n\n    prompt_msgs = chosen_msgs[:-1] if len(chosen_msgs) > 1 else []\n    chosen_content = chosen_msgs[-1].get(\"content\", \"\") if chosen_msgs else \"\"\n    rejected_content = rejected_msgs[-1].get(\"content\", \"\") if rejected_msgs else \"\"\n\n    if not chosen_content or not rejected_content or chosen_content == rejected_content:\n        return {\"text\": \"\", \"pref_chosen\": \"\", \"pref_rejected\": \"\", \"pref_prompt\": \"\"}\n\n    chosen_full = encode_harmony_messages(\n        prompt_msgs + [{\"role\": \"assistant\", \"content\": chosen_content}],\n        developer_instructions=CODING_PREF_DEV,\n    )\n    rejected_full = encode_harmony_messages(\n        prompt_msgs + [{\"role\": \"assistant\", \"content\": rejected_content}],\n        developer_instructions=CODING_PREF_DEV,\n    )\n\n    prompt_text = prompt_msgs[-1].get(\"content\", \"\") if prompt_msgs else \"\"\n\n    # Use prefixed column names to avoid conflicts with original \"chosen\"/\"rejected\" columns\n    return {\n        \"text\": chosen_full,\n        \"pref_prompt\": prompt_text,\n        \"pref_chosen\": chosen_full,\n        \"pref_rejected\": rejected_full,\n    }\n\n\ndef format_code_feedback_example(example):\n    \"\"\"Format CodeFeedback-Filtered-Instruction into Harmony preference pairs.\n\n    Schema: {\"query\": str, \"answer\": str, \"resource\": str, \"lang\": str}\n    \"\"\"\n    query = (example.get(\"query\") or example.get(\"instruction\") or \"\").strip()\n    answer = (example.get(\"answer\") or example.get(\"output\") or \"\").strip()\n\n    if not query or not answer or len(answer) < 100:\n        return {\"text\": \"\", \"pref_chosen\": \"\", \"pref_rejected\": \"\", \"pref_prompt\": \"\"}\n\n    # Synthetic rejected: truncate + non-committal ending (trains model to prefer completion)\n    cutoff = max(50, int(len(answer) * 0.3))\n    rejected = answer[:cutoff] + \"\\n\\n(I would need to analyze this further before proceeding.)\"\n\n    chosen_msgs = [\n        {\"role\": \"user\", \"content\": query},\n        {\"role\": \"assistant\", \"content\": answer},\n    ]\n    rejected_msgs = [\n        {\"role\": \"user\", \"content\": query},\n        {\"role\": \"assistant\", \"content\": rejected},\n    ]\n\n    chosen_full = encode_harmony_messages(chosen_msgs, developer_instructions=CODING_PREF_DEV)\n    rejected_full = encode_harmony_messages(rejected_msgs, developer_instructions=CODING_PREF_DEV)\n\n    return {\n        \"text\": chosen_full,\n        \"pref_prompt\": query,\n        \"pref_chosen\": chosen_full,\n        \"pref_rejected\": rejected_full,\n    }\n\n\nif CONFIG[\"skip_data_generation\"] or not CONFIG[\"include_preference\"]:\n    print(\"Skipping preference data download.\")\nelse:\n    from datasets import load_dataset, Dataset, concatenate_datasets\n\n    all_pref = []\n\n    # 1. Anthropic/hh-rlhf\n    print(\"\\nDownloading Anthropic/hh-rlhf...\")\n    try:\n        raw = load_dataset(\"Anthropic/hh-rlhf\", split=\"train\")\n        print(f\"  Loaded: {len(raw):,} examples\")\n        if CONFIG[\"training_scope\"] == \"quick_test\":\n            raw = raw.select(range(min(300, len(raw))))\n        # Use batched=False and don't remove original columns to avoid name conflicts\n        formatted = raw.map(format_hh_rlhf_example)\n        # Now drop original columns manually\n        cols_to_keep = [\"text\", \"pref_prompt\", \"pref_chosen\", \"pref_rejected\"]\n        cols_to_drop = [c for c in formatted.column_names if c not in cols_to_keep]\n        if cols_to_drop:\n            formatted = formatted.remove_columns(cols_to_drop)\n        formatted = formatted.filter(lambda x: bool(x.get(\"text\", \"\").strip()))\n        all_pref.append(formatted)\n        print(f\"  Formatted: {len(formatted):,} preference pairs\")\n    except Exception as e:\n        print(f\"  WARNING: failed to load Anthropic/hh-rlhf: {e}\")\n        import traceback; traceback.print_exc()\n\n    # 2. m-a-p/CodeFeedback-Filtered-Instruction\n    print(\"\\nDownloading m-a-p/CodeFeedback-Filtered-Instruction...\")\n    try:\n        raw = load_dataset(\"m-a-p/CodeFeedback-Filtered-Instruction\", split=\"train\")\n        print(f\"  Loaded: {len(raw):,} examples\")\n        if CONFIG[\"training_scope\"] == \"quick_test\":\n            raw = raw.select(range(min(200, len(raw))))\n        formatted = raw.map(format_code_feedback_example)\n        cols_to_keep = [\"text\", \"pref_prompt\", \"pref_chosen\", \"pref_rejected\"]\n        cols_to_drop = [c for c in formatted.column_names if c not in cols_to_keep]\n        if cols_to_drop:\n            formatted = formatted.remove_columns(cols_to_drop)\n        formatted = formatted.filter(lambda x: bool(x.get(\"text\", \"\").strip()))\n        all_pref.append(formatted)\n        print(f\"  Formatted: {len(formatted):,} preference pairs\")\n    except Exception as e:\n        print(f\"  WARNING: failed to load m-a-p/CodeFeedback-Filtered-Instruction: {e}\")\n        import traceback; traceback.print_exc()\n\n    if all_pref:\n        combined = concatenate_datasets(all_pref)\n        combined = combined.shuffle(seed=42)\n        out_path = \"data/coding_tui/preference/train\"\n        combined.save_to_disk(out_path)\n        print(f\"\\nTotal preference examples: {len(combined):,} -> {out_path}\")\n    else:\n        print(\"WARNING: no preference data collected.\")\n\n    drive_helper.backup(\"data/coding_tui/preference\", \"data/coding_tui/preference\")\n    if DRIVE_MODE != \"local\":\n        print(\"Backed up to Drive.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338ed52feaab5e9e",
   "metadata": {},
   "source": "### 1.4 Extract Training Data from Proxy Logs (Optional)\n\nScans the MacLean AI proxy log directory for real Codex CLI agent sessions.\nEach session is a JSON file written by `claude-proxy-v2` containing the full\nrequest/response. These are the highest-value training examples because they\nare real agent tasks, not synthetic data.\n\nAlso scans for `*_cot.txt` files (chain-of-thought logs from the streaming filter)\nand associates them with the corresponding requests for thinking data.\n\n**Set `proxy_log_dir` in Step 0.3 to enable.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037f5211b81de6f5",
   "metadata": {},
   "outputs": [],
   "source": "import os, sys, json, glob, re\nfrom pathlib import Path\n\nsys.path.insert(0, \"scripts\")\nfrom dataset_formatters.harmony import encode_harmony_messages\n\nCODING_AGENT_DEV_PROMPT = (\n    \"You are a coding agent. Use tools to read files, write code, run tests, and \"\n    \"complete programming tasks. Do not just analyze \u2014 always take action and produce \"\n    \"working code. After making changes, verify they work by running the relevant tests. \"\n    \"If a tool call fails, diagnose and retry with corrected parameters.\"\n)\n\n\ndef load_proxy_log(log_path):\n    \"\"\"Load and validate a single proxy log JSON file.\n\n    Returns the parsed dict if valid, None otherwise.\n    Expected fields: request_num, path, translated, original_request,\n                     response, input_tokens, output_tokens, latency_ms\n    \"\"\"\n    try:\n        with open(log_path) as f:\n            data = json.load(f)\n    except (json.JSONDecodeError, OSError):\n        return None\n\n    # Must have a response with output\n    if data.get(\"output_tokens\", 0) <= 0:\n        return None\n\n    resp = data.get(\"response\", {})\n    if not resp:\n        return None\n\n    return data\n\n\ndef extract_messages_from_request(req_data, cot_text=None):\n    \"\"\"Extract Harmony-ready messages from a proxy log entry.\n\n    Handles both:\n    - /v1/messages (Anthropic format, possibly translated)\n    - /v1/chat/completions (OpenAI format)\n\n    Args:\n        req_data: parsed proxy log dict\n        cot_text: optional chain-of-thought text from *_cot.txt sidecar\n\n    Returns list of message dicts suitable for encode_harmony_messages, or None.\n    \"\"\"\n    path = req_data.get(\"path\", \"\")\n    original_req = req_data.get(\"original_request\", {})\n    response = req_data.get(\"response\", {})\n\n    messages = []\n\n    if \"/messages\" in path:\n        # Anthropic Messages API format\n        sys_content = original_req.get(\"system\", \"\")\n        if isinstance(sys_content, list):\n            # system can be a list of content blocks\n            sys_text = \" \".join(\n                block.get(\"text\", \"\") for block in sys_content\n                if isinstance(block, dict) and block.get(\"type\") == \"text\"\n            )\n        else:\n            sys_text = str(sys_content) if sys_content else \"\"\n\n        raw_msgs = original_req.get(\"messages\", [])\n        for m in raw_msgs:\n            role = m.get(\"role\", \"\")\n            content = m.get(\"content\", \"\")\n            if isinstance(content, list):\n                # content can be a list of blocks (text, tool_use, tool_result)\n                text_parts = []\n                tool_calls_out = []\n                for block in content:\n                    btype = block.get(\"type\", \"\")\n                    if btype == \"text\":\n                        text_parts.append(block.get(\"text\", \"\"))\n                    elif btype == \"tool_use\":\n                        tool_calls_out.append({\n                            \"id\": block.get(\"id\", \"\"),\n                            \"type\": \"function\",\n                            \"function\": {\n                                \"name\": block.get(\"name\", \"\"),\n                                \"arguments\": json.dumps(block.get(\"input\", {})),\n                            },\n                        })\n                    elif btype == \"tool_result\":\n                        # tool results come back as user messages in Anthropic format\n                        result_content = block.get(\"content\", \"\")\n                        if isinstance(result_content, list):\n                            result_content = \" \".join(\n                                rb.get(\"text\", \"\") for rb in result_content\n                                if isinstance(rb, dict)\n                            )\n                        messages.append({\n                            \"role\": \"tool\",\n                            \"tool_call_id\": block.get(\"tool_use_id\", \"\"),\n                            \"content\": result_content,\n                        })\n\n                msg_entry = {\"role\": role}\n                if text_parts:\n                    msg_entry[\"content\"] = \"\\n\".join(text_parts)\n                if tool_calls_out:\n                    msg_entry[\"tool_calls\"] = tool_calls_out\n                if role == \"assistant\" and cot_text:\n                    msg_entry[\"thinking\"] = cot_text\n                    cot_text = None  # use only for the first assistant turn\n\n                if role not in (\"user\",) or text_parts:\n                    messages.append(msg_entry)\n            else:\n                entry = {\"role\": role, \"content\": str(content) if content else \"\"}\n                if role == \"assistant\" and cot_text:\n                    entry[\"thinking\"] = cot_text\n                    cot_text = None\n                messages.append(entry)\n\n        # Extract assistant response\n        resp_content = response.get(\"content\", [])\n        if isinstance(resp_content, list):\n            resp_text_parts = []\n            resp_tool_calls = []\n            for block in resp_content:\n                btype = block.get(\"type\", \"\")\n                if btype == \"text\":\n                    resp_text_parts.append(block.get(\"text\", \"\"))\n                elif btype == \"tool_use\":\n                    resp_tool_calls.append({\n                        \"id\": block.get(\"id\", \"\"),\n                        \"type\": \"function\",\n                        \"function\": {\n                            \"name\": block.get(\"name\", \"\"),\n                            \"arguments\": json.dumps(block.get(\"input\", {})),\n                        },\n                    })\n            resp_entry = {\"role\": \"assistant\"}\n            if resp_text_parts:\n                resp_entry[\"content\"] = \"\\n\".join(resp_text_parts)\n            if resp_tool_calls:\n                resp_entry[\"tool_calls\"] = resp_tool_calls\n            messages.append(resp_entry)\n        elif isinstance(resp_content, str) and resp_content:\n            messages.append({\"role\": \"assistant\", \"content\": resp_content})\n\n        dev_instructions = sys_text if sys_text else CODING_AGENT_DEV_PROMPT\n\n    elif \"/chat/completions\" in path:\n        # OpenAI Chat Completions format\n        raw_msgs = original_req.get(\"messages\", [])\n        dev_instructions = CODING_AGENT_DEV_PROMPT\n\n        for m in raw_msgs:\n            role = m.get(\"role\", \"\")\n            content = m.get(\"content\", \"\")\n            tool_calls = m.get(\"tool_calls\", [])\n            tool_call_id = m.get(\"tool_call_id\")\n\n            if role == \"system\":\n                dev_instructions = content\n                continue\n\n            entry = {\"role\": role}\n            if content:\n                entry[\"content\"] = content\n            if tool_calls:\n                entry[\"tool_calls\"] = tool_calls\n            if tool_call_id:\n                entry[\"tool_call_id\"] = tool_call_id\n            if role == \"assistant\" and cot_text:\n                entry[\"thinking\"] = cot_text\n                cot_text = None\n            messages.append(entry)\n\n        # Extract response from choices\n        choices = response.get(\"choices\", [])\n        if choices:\n            resp_msg = choices[0].get(\"message\", {})\n            resp_entry = {\"role\": \"assistant\"}\n            if resp_msg.get(\"content\"):\n                resp_entry[\"content\"] = resp_msg[\"content\"]\n            if resp_msg.get(\"tool_calls\"):\n                resp_entry[\"tool_calls\"] = resp_msg[\"tool_calls\"]\n            messages.append(resp_entry)\n    else:\n        return None, None\n\n    if len(messages) < 2:\n        return None, None\n\n    return messages, dev_instructions\n\n\ndef extract_proxy_log_trajectories(log_dir, max_samples=None, quick_test=False):\n    \"\"\"Scan proxy log directory and extract training examples.\n\n    Args:\n        log_dir: path to MacLean AI proxy log directory\n        max_samples: cap on number of examples to extract\n        quick_test: if True, stop after 20 examples\n\n    Returns list of Harmony-encoded text strings.\n    \"\"\"\n    log_dir = Path(log_dir)\n    if not log_dir.exists():\n        print(f\"  WARNING: proxy log dir not found: {log_dir}\")\n        return []\n\n    # Find all JSON log files (exclude *_cot.txt sidecars)\n    log_files = sorted(log_dir.glob(\"*.json\"))\n    if not log_files:\n        # Try subdirectories (MacLean AI organises logs by date)\n        log_files = sorted(log_dir.glob(\"**/*.json\"))\n\n    print(f\"  Found {len(log_files)} log files in {log_dir}\")\n\n    examples = []\n    skipped = 0\n\n    for log_path in log_files:\n        if quick_test and len(examples) >= 20:\n            break\n        if max_samples and len(examples) >= max_samples:\n            break\n\n        req_data = load_proxy_log(log_path)\n        if req_data is None:\n            skipped += 1\n            continue\n\n        # Check for CoT sidecar: same stem but _cot.txt suffix\n        cot_path = log_path.with_name(log_path.stem + \"_cot.txt\")\n        cot_text = None\n        if cot_path.exists():\n            try:\n                cot_text = cot_path.read_text(encoding=\"utf-8\").strip() or None\n            except OSError:\n                pass\n\n        messages, dev_instructions = extract_messages_from_request(req_data, cot_text)\n        if messages is None:\n            skipped += 1\n            continue\n\n        try:\n            text = encode_harmony_messages(\n                messages,\n                developer_instructions=dev_instructions,\n                reasoning_effort=\"high\",\n            )\n        except Exception as e:\n            skipped += 1\n            continue\n\n        if text and len(text.strip()) > 200:\n            examples.append({\"text\": text})\n\n    print(f\"  Extracted: {len(examples):,} valid examples ({skipped} skipped)\")\n    return examples\n\n\n# \u2500\u2500 Run extraction \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nlog_dir = CONFIG.get(\"proxy_log_dir\", \"\").strip()\n\nif not CONFIG[\"include_proxy_logs\"] or not log_dir:\n    print(\"Proxy log extraction disabled (include_proxy_logs=False or proxy_log_dir not set).\")\n    print(\"To enable: set proxy_log_dir to the MacLean AI log directory in Step 0.3.\")\n    tracker.skip(\"proxy_log_extract\")\nelif CONFIG[\"skip_data_generation\"]:\n    print(\"Skipping proxy log extraction (skip_data_generation=True).\")\n    tracker.skip(\"proxy_log_extract\")\nelse:\n    tracker.start(\"proxy_log_extract\")\n\n    quick = CONFIG[\"training_scope\"] == \"quick_test\"\n    proxy_examples = extract_proxy_log_trajectories(\n        log_dir,\n        max_samples=5000 if not quick else None,\n        quick_test=quick,\n    )\n\n    if proxy_examples:\n        from datasets import Dataset, load_from_disk, concatenate_datasets\n\n        proxy_ds = Dataset.from_list(proxy_examples)\n        proxy_ds = proxy_ds.shuffle(seed=42)\n\n        # Save standalone proxy dataset\n        proxy_out = \"data/coding_tui/proxy_logs/train\"\n        proxy_ds.save_to_disk(proxy_out)\n        print(f\"\\nProxy log dataset: {len(proxy_ds):,} examples -> {proxy_out}\")\n\n        # Merge into agent_sft data (proxy logs are highest value)\n        agent_path = \"data/coding_tui/agent_traj/train\"\n        if os.path.exists(agent_path):\n            base_ds = load_from_disk(agent_path)\n            merged = concatenate_datasets([base_ds, proxy_ds])\n            merged = merged.shuffle(seed=42)\n            merged.save_to_disk(agent_path)\n            print(f\"Merged into agent_traj/train: {len(merged):,} total\")\n        else:\n            proxy_ds.save_to_disk(agent_path)\n            print(f\"Saved as agent_traj/train: {len(proxy_ds):,} examples\")\n\n        drive_helper.backup(\"data/coding_tui/proxy_logs\", \"data/coding_tui/proxy_logs\")\n        if DRIVE_MODE != \"local\":\n            print(\"Backed up to Drive.\")\n\n        tracker.complete(\"proxy_log_extract\")\n    else:\n        print(\"No valid proxy log examples extracted.\")\n        tracker.fail(\"proxy_log_extract\")"
  },
  {
   "cell_type": "markdown",
   "id": "97c6483566d1285b",
   "metadata": {},
   "source": "### 1.5 Verify Data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc558ca9eef76a0",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\ndata_checks = [\n    (\"Tool Calling train\", \"data/coding_tui/tool_calling/train\"),\n    (\"Agent Trajectory train\", \"data/coding_tui/agent_traj/train\"),\n    (\"Proxy Log train\", \"data/coding_tui/proxy_logs/train\"),\n    (\"Preference train\", \"data/coding_tui/preference/train\"),\n]\n\nprint(\"Data Verification Summary\")\nprint(\"=\" * 60)\nprint(f\"  {'Dataset':<30} {'Examples':>12}\")\nprint(\"-\" * 60)\n\ntotal = 0\nfor name, path in data_checks:\n    if os.path.exists(path):\n        try:\n            from datasets import load_from_disk\n            ds = load_from_disk(path)\n            count = len(ds)\n            total += count\n            print(f\"  {name:<30} {count:>12,}\")\n        except Exception as e:\n            print(f\"  {name:<30} {'ERROR':>12}  ({e})\")\n    else:\n        print(f\"  {name:<30} {'not found':>12}\")\n\nprint(\"-\" * 60)\nprint(f\"  {'TOTAL':<30} {total:>12,}\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.6 Verify MoE Expert LoRA Targeting\n\nGPT-OSS 20B uses MoE (Mixture of Experts) with fused FFN layers.\nUnsloth Bug #3405: default target modules (`gate_proj`, `up_proj`, `down_proj`) silently miss expert layers.\nThis cell loads the model briefly to verify that LoRA adapters will be applied to expert FFN layers,\nnot just attention layers. If experts are NOT targeted, training only adapts ~31.8M params instead of ~200M+."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys, torch, os\n",
    "sys.path.insert(0, \"scripts\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  MoE Expert LoRA Diagnostic\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from pipeline_lib.unsloth_utils import (\n",
    "    load_unsloth_model, detect_moe_experts, get_moe_expert_regex,\n",
    "    apply_lora_config, verify_expert_lora, print_trainable_params,\n",
    ")\n",
    "\n",
    "# Quick load to inspect architecture (will be reloaded during training)\n",
    "print(\"\\nLoading model for architecture inspection...\")\n",
    "model, tokenizer = load_unsloth_model(\n",
    "    model_name=\"openai/gpt-oss-20b\",\n",
    "    max_seq_length=2048,  # Short \u2014 just inspecting, not training\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# Step 1: Detect MoE experts\n",
    "print(\"\\n--- Step 1: MoE Architecture Detection ---\")\n",
    "moe_info = detect_moe_experts(model)\n",
    "print(f\"  Is MoE: {moe_info['is_moe']}\")\n",
    "print(f\"  Expert layer types: {moe_info['expert_module_names']}\")\n",
    "print(f\"  Number of experts: {moe_info['num_experts']}\")\n",
    "print(f\"  Expert param count: {moe_info['expert_param_count']:,}\")\n",
    "if moe_info.get('sample_param_names'):\n",
    "    print(f\"  Sample param names:\")\n",
    "    for n in moe_info['sample_param_names']:\n",
    "        print(f\"    {n}\")\n",
    "\n",
    "# Step 2: Get expert regex\n",
    "print(\"\\n--- Step 2: Expert Regex Pattern ---\")\n",
    "regex = get_moe_expert_regex(model)\n",
    "print(f\"  Regex: {regex}\")\n",
    "\n",
    "# Step 3: Apply LoRA (tries Unsloth+regex first, PEFT direct fallback)\n",
    "print(\"\\n--- Step 3: Apply LoRA (Unsloth regex \u2192 PEFT direct fallback) ---\")\n",
    "test_config = {\n",
    "    \"r\": 8,  # Small rank \u2014 just testing, not training\n",
    "    \"lora_alpha\": 16,\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_up_proj\", \"down_proj\"],\n",
    "    \"lora_dropout\": 0,\n",
    "    \"bias\": \"none\",\n",
    "    \"use_gradient_checkpointing\": \"unsloth\",\n",
    "    \"use_rslora\": False,\n",
    "}\n",
    "model = apply_lora_config(model, test_config, auto_detect_moe=True)\n",
    "\n",
    "print(\"\\n--- Step 4: Verification ---\")\n",
    "result = verify_expert_lora(model)\n",
    "print_trainable_params(model)\n",
    "\n",
    "if result[\"has_expert_lora\"]:\n",
    "    print(\"\\n  PASS: Expert FFN layers have LoRA adapters.\")\n",
    "    print(f\"  {result['expert_lora_count']} expert LoRA parameters detected.\")\n",
    "    print(f\"  {result['expert_lora_params']:,} expert trainable params.\")\n",
    "    print(\"  Training will update expert routing knowledge (not just attention).\")\n",
    "else:\n",
    "    print(\"\\n  FAIL: Expert FFN layers do NOT have LoRA adapters!\")\n",
    "    print(\"  Dumping all trainable parameter names for debugging:\")\n",
    "    for name in result[\"all_lora_names\"][:20]:\n",
    "        print(f\"    {name}\")\n",
    "    if len(result[\"all_lora_names\"]) > 20:\n",
    "        print(f\"    ... ({len(result['all_lora_names']) - 20} more)\")\n",
    "\n",
    "# Clean up to free VRAM before real training\n",
    "del model, tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "import gc; gc.collect()\n",
    "print(\"\\nDiagnostic complete. Model unloaded to free VRAM.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff172aa0151dbae",
   "metadata": {},
   "source": "## Step 2: Tool Calling SFT (Phase 1)\n\nTrain a LoRA adapter (rank 64) focused on correct tool/function calling.\n\n**Goals:**\n- Learn parameter accuracy for tool calls\n- Learn when NOT to call tools\n- Learn valid tool schemas and JSON formatting\n- Reduce hallucinated MCP server calls\n\nLow rank (64) to avoid catastrophic forgetting of general coding ability."
  },
  {
   "cell_type": "markdown",
   "id": "c95bdc5ffa260f7b",
   "metadata": {},
   "source": "### 2.1 Train Tool Calling Adapter"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2148b159c8b1f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] in (\"skip_to_rl\",):\n    print(f\"Skipping \u2014 scope is {CONFIG['training_scope']}\")\n    tracker.skip(\"tool_calling_sft\")\nelif not CONFIG[\"include_tool_calling\"]:\n    print(\"Skipping \u2014 include_tool_calling=False\")\n    tracker.skip(\"tool_calling_sft\")\nelse:\n    tracker.start(\"tool_calling_sft\")\n\n    batch = CONFIG[\"tool_calling_batch\"]\n    grad_accum = CONFIG[\"tool_calling_grad_accum\"]\n    max_steps = CONFIG[\"tool_calling_max_steps\"]\n    seq_len = CONFIG[\"tool_calling_seq_len\"]\n    epochs = 1  # Single pass for tool calling (large dataset)\n\n    cmd = \"python scripts/13_train_lang_adapter.py\"\n    cmd += \" --train_data_path data/coding_tui/tool_calling/train\"\n    cmd += f\" --per_device_train_batch_size {batch}\"\n    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n    cmd += f\" --max_steps {max_steps}\"\n    cmd += f\" --num_train_epochs {epochs}\"\n    cmd += f\" --output_dir checkpoints/tool_calling_sft\"\n\n    print(\"Training tool calling SFT adapter...\")\n    print(f\"  Data:     data/coding_tui/tool_calling/train\")\n    print(f\"  Batch:    {batch} x {grad_accum} = {batch * grad_accum}\")\n    print(f\"  Epochs:   {epochs}\")\n    print(f\"  Max steps: {max_steps} (-1 = epoch-controlled)\")\n    print(f\"  Seq len:  {seq_len}\")\n    print(f\"  LoRA rank: 64 (low rank to preserve general ability)\")\n    print(f\"  MoE backend: {CONFIG['moe_backend']}\")\n    print(\"=\" * 60)\n\n    !{cmd}\n\n    drive_helper.backup(\"checkpoints/tool_calling_sft\", \"checkpoints/tool_calling_sft\")\n    if DRIVE_MODE != \"local\":\n        print(\"\\nCheckpoint backed up to Drive.\")\n\n    tracker.complete(\"tool_calling_sft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b11bbeb2214c94",
   "metadata": {},
   "source": "### 2.2 Merge Tool Calling Adapter into Base"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb17eece05e3e412",
   "metadata": {},
   "outputs": [],
   "source": "if CONFIG[\"training_scope\"] in (\"skip_to_rl\",):\n    print(f\"Skipping \u2014 scope is {CONFIG['training_scope']}\")\n    tracker.skip(\"merge\")\nelif not CONFIG[\"include_tool_calling\"]:\n    print(\"Skipping \u2014 include_tool_calling=False\")\n    tracker.skip(\"merge\")\nelse:\n    tracker.start(\"merge\")\n\n    print(\"Merging tool calling adapter into base model...\")\n    print(\"=\" * 60)\n\n    !python scripts/19_merge_adapter.py \\\n        --adapter_path checkpoints/tool_calling_sft/final \\\n        --output_dir checkpoints/gpt-oss-20b-coding-tui-merged\n\n    drive_helper.backup(\n        \"checkpoints/gpt-oss-20b-coding-tui-merged\",\n        \"checkpoints/gpt-oss-20b-coding-tui-merged\",\n    )\n    if DRIVE_MODE != \"local\":\n        print(\"\\nMerged model backed up to Drive.\")\n\n    tracker.complete(\"merge\")"
  },
  {
   "cell_type": "markdown",
   "id": "a6bae2458cdb0003",
   "metadata": {},
   "source": "### 2.3 Verify Merge"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72dc4ead2938c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, glob\n\nif CONFIG[\"training_scope\"] not in (\"skip_to_rl\",) and CONFIG[\"include_tool_calling\"]:\n    print(\"=\" * 60)\n    print(\"  QUALITY GATE: Tool Calling SFT\")\n    print(\"=\" * 60)\n    gate = gate_report[\"tool_calling\"]\n    gate[\"passed\"] = True\n\n    # 1. Check training loss\n    state_files = glob.glob(\"checkpoints/tool_calling_sft/**/trainer_state.json\", recursive=True)\n    if state_files:\n        with open(sorted(state_files)[-1]) as f:\n            state = json.load(f)\n        log_history = state.get(\"log_history\", [])\n        train_losses = [e[\"loss\"] for e in log_history if \"loss\" in e]\n        if train_losses:\n            gate[\"loss\"] = train_losses[-1]\n            avg_last = sum(train_losses[-10:]) / len(train_losses[-10:])\n            gate[\"avg_loss\"] = avg_last\n            gate[\"first_loss\"] = train_losses[0]\n            gate[\"total_steps\"] = len(train_losses)\n            improvement = (train_losses[0] - avg_last) / train_losses[0] * 100\n            gate[\"improvement\"] = improvement\n            print(f\"  Loss (first):     {train_losses[0]:.4f}\")\n            print(f\"  Loss (final):     {train_losses[-1]:.4f}\")\n            print(f\"  Loss (avg last):  {avg_last:.4f}\")\n            print(f\"  Improvement:      {improvement:.1f}%\")\n            print(f\"  Total steps:      {len(train_losses)}\")\n            if avg_last > 2.0:\n                gate[\"passed\"] = False\n                gate[\"notes\"].append(f\"Loss too high ({avg_last:.4f} > 2.0)\")\n                print(f\"  \u2717 Loss too high\")\n            elif avg_last > 1.5:\n                gate[\"notes\"].append(f\"Loss elevated ({avg_last:.4f} > 1.5)\")\n                print(f\"  \u26a0 Loss elevated\")\n            else:\n                print(f\"  \u2713 Loss acceptable\")\n    else:\n        gate[\"passed\"] = False\n        gate[\"notes\"].append(\"No trainer_state.json found\")\n        print(\"  \u2717 No trainer_state.json found\")\n\n    # 2. Check merged model\n    merged_path = \"checkpoints/gpt-oss-20b-coding-tui-merged\"\n    if os.path.exists(merged_path):\n        files = os.listdir(merged_path)\n        total_size = sum(\n            os.path.getsize(os.path.join(merged_path, f))\n            for f in files if os.path.isfile(os.path.join(merged_path, f))\n        )\n        size_gb = total_size / (1024**3)\n        print(f\"  Merged model:     {size_gb:.1f} GB ({len(files)} files)\")\n        if size_gb < 5.0:\n            gate[\"passed\"] = False\n            gate[\"notes\"].append(f\"Merged model too small ({size_gb:.1f} GB)\")\n        else:\n            print(f\"  \u2713 Merged model OK\")\n    else:\n        gate[\"notes\"].append(\"Merged model not found (will use base)\")\n        print(f\"  \u26a0 Merged model not found \u2014 agent SFT will use base model\")\n\n    # 3. Quick inference test\n    print(\"\\n  Running inference smoke test...\")\n    try:\n        from unsloth import FastLanguageModel\n        test_model_path = merged_path if os.path.exists(merged_path) else \"checkpoints/tool_calling_sft/final\"\n        model, tokenizer = FastLanguageModel.from_pretrained(\n            test_model_path, max_seq_length=4096, load_in_4bit=True,\n        )\n        FastLanguageModel.for_inference(model)\n\n        test_prompt = (\n            \"<|developer|>\\nYou are a coding agent with tools: run_command, read_file, apply_patch.\\n\"\n            \"<|user|>\\nRead the file src/main.rs\\n\"\n            \"<|assistant|>\\n\"\n        )\n        inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n        outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.1, do_sample=True)\n        response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=False)\n\n        gate[\"inference_response\"] = response[:500]\n        has_tool = any(tok in response for tok in [\"read_file\", \"run_command\", \"<|tool_call|>\", \"function\"])\n        print(f\"  Response: {response[:200]}\")\n        if has_tool:\n            print(f\"  \u2713 Produces tool calls\")\n        else:\n            gate[\"notes\"].append(\"No tool call in inference test\")\n            print(f\"  \u26a0 No tool call detected\")\n\n        del model, tokenizer\n        import gc; gc.collect()\n        import torch; torch.cuda.empty_cache()\n    except Exception as e:\n        gate[\"notes\"].append(f\"Inference test failed: {e}\")\n        print(f\"  \u26a0 Inference test skipped: {e}\")\n\n    status = \"\u2705 PASSED\" if gate[\"passed\"] else \"\u274c FAILED\"\n    print(f\"\\n  {status}\")\n    if gate[\"notes\"]:\n        for note in gate[\"notes\"]:\n            print(f\"    \u2192 {note}\")\n    print(\"=\" * 60)\n    print(\"  Continuing to next phase regardless...\")\nelse:\n    print(\"Tool calling SFT skipped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6c9cb29f5905e5",
   "metadata": {},
   "source": "## Step 3: Agent SFT (Phase 2)\n\nTrain a higher-rank LoRA (rank 128) on agent trajectories using the merged\ntool-calling model as the base.\n\n**Data includes:**\n- Multi-turn code agent sessions (code-act, commitpack, editpackft)\n- Real proxy log trajectories from live Codex CLI sessions (most valuable)\n\n**Goals:**\n- Learn complete read \u2192 plan \u2192 edit \u2192 verify cycles\n- Learn to actually write code after planning\n- Learn context tracking across long sessions"
  },
  {
   "cell_type": "markdown",
   "id": "84ca073beddf1cbd",
   "metadata": {},
   "source": "### 3.1 Train Agent SFT Adapter"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4b40b52dd555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] in (\"tool_calling_only\", \"skip_to_rl\"):\n    print(f\"Skipping \u2014 scope is {CONFIG['training_scope']}\")\n    tracker.skip(\"agent_sft\")\nelif not CONFIG[\"include_agent_trajectories\"]:\n    print(\"Skipping \u2014 include_agent_trajectories=False\")\n    tracker.skip(\"agent_sft\")\nelse:\n    tracker.start(\"agent_sft\")\n\n    # Use merged model as base if available, otherwise base model\n    import os\n    if os.path.exists(\"checkpoints/gpt-oss-20b-coding-tui-merged\"):\n        base_model = \"checkpoints/gpt-oss-20b-coding-tui-merged\"\n        print(\"Using merged tool-calling model as base.\")\n    else:\n        base_model = \"openai/gpt-oss-20b\"\n        print(\"Merged model not found, using original base model.\")\n\n    batch = CONFIG[\"agent_sft_batch\"]\n    grad_accum = CONFIG[\"agent_sft_grad_accum\"]\n    max_steps = CONFIG[\"agent_sft_max_steps\"]\n    seq_len = CONFIG[\"agent_sft_seq_len\"]\n    epochs = 3  # Agent behavior needs more passes\n\n    cmd = \"python scripts/14_train_core_agent.py\"\n    cmd += \" --train_data_path data/coding_tui/agent_traj/train\"\n    cmd += f\" --per_device_train_batch_size {batch}\"\n    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n    cmd += f\" --max_steps {max_steps}\"\n    cmd += f\" --num_train_epochs {epochs}\"\n    cmd += f\" --output_dir checkpoints/agent_sft\"\n\n    print(\"Training agent SFT adapter (rank 128)...\")\n    print(f\"  Base:     {base_model}\")\n    print(f\"  Data:     data/coding_tui/agent_traj/train\")\n    print(f\"  Batch:    {batch} x {grad_accum} = {batch * grad_accum}\")\n    print(f\"  Epochs:   {epochs}\")\n    print(f\"  Max steps: {max_steps} (-1 = epoch-controlled)\")\n    print(f\"  Seq len:  {seq_len}\")\n    print(f\"  LoRA rank: 128\")\n    print(f\"  MoE backend: {CONFIG['moe_backend']}\")\n    print(f\"  Auto packing: enabled\")\n    print(\"=\" * 60)\n\n    !{cmd}\n\n    drive_helper.backup(\"checkpoints/agent_sft\", \"checkpoints/agent_sft\")\n    if DRIVE_MODE != \"local\":\n        print(\"\\nCheckpoint backed up to Drive.\")\n\n    tracker.complete(\"agent_sft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9415b023dc2fc8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, glob\n\nif CONFIG[\"training_scope\"] not in (\"tool_calling_only\", \"skip_to_rl\") and CONFIG[\"include_agent_trajectories\"]:\n    print(\"=\" * 60)\n    print(\"  QUALITY GATE: Agent SFT\")\n    print(\"=\" * 60)\n    gate = gate_report[\"agent_sft\"]\n    gate[\"passed\"] = True\n\n    # 1. Check training loss\n    state_files = glob.glob(\"checkpoints/agent_sft/**/trainer_state.json\", recursive=True)\n    if state_files:\n        with open(sorted(state_files)[-1]) as f:\n            state = json.load(f)\n        log_history = state.get(\"log_history\", [])\n        train_losses = [e[\"loss\"] for e in log_history if \"loss\" in e]\n        if train_losses:\n            gate[\"loss\"] = train_losses[-1]\n            avg_last = sum(train_losses[-10:]) / len(train_losses[-10:])\n            gate[\"avg_loss\"] = avg_last\n            gate[\"first_loss\"] = train_losses[0]\n            gate[\"total_steps\"] = len(train_losses)\n            improvement = (train_losses[0] - avg_last) / train_losses[0] * 100\n            gate[\"improvement\"] = improvement\n            print(f\"  Loss (first):     {train_losses[0]:.4f}\")\n            print(f\"  Loss (final):     {train_losses[-1]:.4f}\")\n            print(f\"  Loss (avg last):  {avg_last:.4f}\")\n            print(f\"  Improvement:      {improvement:.1f}%\")\n            print(f\"  Total steps:      {len(train_losses)}\")\n            if avg_last > 1.8:\n                gate[\"passed\"] = False\n                gate[\"notes\"].append(f\"Loss too high ({avg_last:.4f} > 1.8)\")\n            elif improvement < 10:\n                gate[\"notes\"].append(f\"Low improvement ({improvement:.1f}% < 10%)\")\n                print(f\"  \u26a0 Low convergence\")\n            else:\n                print(f\"  \u2713 Good convergence\")\n    else:\n        gate[\"passed\"] = False\n        gate[\"notes\"].append(\"No trainer_state.json found\")\n\n    # 2. Check checkpoint and LoRA config\n    ckpt_path = \"checkpoints/agent_sft/final\"\n    if os.path.exists(ckpt_path):\n        adapter_config = os.path.join(ckpt_path, \"adapter_config.json\")\n        if os.path.exists(adapter_config):\n            with open(adapter_config) as f:\n                cfg = json.load(f)\n            rank = cfg.get(\"r\", 0)\n            gate[\"lora_rank\"] = rank\n            print(f\"  LoRA rank: {rank}, alpha: {cfg.get('lora_alpha', '?')}\")\n            if rank != 128:\n                gate[\"notes\"].append(f\"Unexpected rank {rank} (expected 128)\")\n        print(f\"  \u2713 Checkpoint exists ({len(os.listdir(ckpt_path))} files)\")\n    else:\n        gate[\"passed\"] = False\n        gate[\"notes\"].append(\"Checkpoint not found\")\n\n    # 3. Behavior test\n    if os.path.exists(ckpt_path):\n        print(\"\\n  Running agent behavior test...\")\n        try:\n            from unsloth import FastLanguageModel\n            model, tokenizer = FastLanguageModel.from_pretrained(\n                ckpt_path, max_seq_length=8192, load_in_4bit=True,\n            )\n            FastLanguageModel.for_inference(model)\n\n            test_prompt = (\n                \"<|developer|>\\nYou are a coding agent. Use tools to complete tasks. Always take action.\\n\"\n                \"<|user|>\\nFix the failing test in src/parser.rs\\n\"\n                \"<|assistant|>\\nLet me read the test file first.\\n\"\n                \"<|tool_call|>\\n{\\\"id\\\": \\\"1\\\", \\\"name\\\": \\\"read_file\\\", \\\"arguments\\\": {\\\"path\\\": \\\"src/parser.rs\\\"}}\\n\"\n                \"<|tool_result|>\\n<|tool_call_id|>1\\nfn parse(input: &str) -> Result<Ast, Error> {\\n    unimplemented!()\\n}\\n\\n#[test]\\nfn test_parse_basic() {\\n    assert!(parse(\\\"hello\\\").is_ok());\\n}\\n\"\n                \"<|assistant|>\\n\"\n            )\n            inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n            outputs = model.generate(**inputs, max_new_tokens=300, temperature=0.1, do_sample=True)\n            response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=False)\n\n            gate[\"inference_response\"] = response[:500]\n            print(f\"  Response: {response[:300]}\")\n\n            takes_action = any(tok in response for tok in [\"apply_patch\", \"write\", \"impl\", \"fn parse\", \"<|tool_call|>\"])\n            stalls = any(phrase in response.lower() for phrase in [\n                \"i would need to\", \"let me analyze\", \"i need more context\",\n                \"further investigation\", \"i cannot\"\n            ])\n            gate[\"takes_action\"] = takes_action\n            gate[\"stalls\"] = stalls\n\n            if takes_action:\n                print(f\"  \u2713 Takes action\")\n            else:\n                gate[\"notes\"].append(\"No decisive action in behavior test\")\n                print(f\"  \u26a0 No decisive action detected\")\n            if stalls:\n                gate[\"notes\"].append(\"Stalling/hedging language detected\")\n                print(f\"  \u26a0 Stalling language detected\")\n            else:\n                print(f\"  \u2713 No stalling\")\n\n            del model, tokenizer\n            import gc; gc.collect()\n            import torch; torch.cuda.empty_cache()\n        except Exception as e:\n            gate[\"notes\"].append(f\"Behavior test failed: {e}\")\n            print(f\"  \u26a0 Test skipped: {e}\")\n\n    status = \"\u2705 PASSED\" if gate[\"passed\"] else \"\u274c FAILED\"\n    print(f\"\\n  {status}\")\n    if gate[\"notes\"]:\n        for note in gate[\"notes\"]:\n            print(f\"    \u2192 {note}\")\n    print(\"=\" * 60)\n    print(\"  Continuing to next phase regardless...\")\nelse:\n    print(\"Agent SFT skipped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45e28d85aa60666",
   "metadata": {},
   "source": "## Step 4: IPO Preference Optimisation (Phase 3)\n\nTrain with IPO on preference pairs targeting the key failure modes:\n\n**Good (chosen):** Task completed, code written, tests pass, decisive action\n**Bad (rejected):** Circular analysis, \"I would need to look at this more\", no code written, wrong tool params\n\nVery low learning rate (5e-7), 1 epoch to avoid collapse."
  },
  {
   "cell_type": "markdown",
   "id": "eb4e2d31d414dbb2",
   "metadata": {},
   "source": "### 4.1 Train with IPO"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c51fc5d0b7bbe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n\nif CONFIG[\"training_scope\"] == \"tool_calling_only\":\n    print(\"Skipping \u2014 scope is tool_calling_only\")\n    tracker.skip(\"ipo\")\nelif not CONFIG[\"include_preference\"]:\n    print(\"Skipping \u2014 include_preference=False\")\n    tracker.skip(\"ipo\")\nelse:\n    tracker.start(\"ipo\")\n\n    batch = CONFIG[\"ipo_batch\"]\n    grad_accum = CONFIG[\"ipo_grad_accum\"]\n    max_steps = CONFIG[\"ipo_max_steps\"]\n\n    # Determine best checkpoint to train from\n    if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n        ipo_base = \"checkpoints/agent_sft/final\"\n        print(\"skip_to_rl: starting IPO from agent_sft checkpoint\")\n    elif os.path.exists(\"checkpoints/agent_sft/final\"):\n        ipo_base = \"checkpoints/agent_sft/final\"\n    else:\n        ipo_base = \"checkpoints/tool_calling_sft/final\"\n        print(\"agent_sft not found, falling back to tool_calling_sft\")\n\n    # Check data exists\n    pref_path = \"data/coding_tui/preference/train\"\n    if not os.path.exists(pref_path):\n        print(f\"WARNING: preference data not found at {pref_path}\")\n        print(\"Run Step 1.3 first.\")\n        tracker.fail(\"ipo\")\n    else:\n        cmd = \"python scripts/17_ipo_preference.py\"\n        cmd += f\" --checkpoint {ipo_base}\"\n        cmd += f\" --train_data_path {pref_path}\"\n        cmd += f\" --per_device_train_batch_size {batch}\"\n        cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n        cmd += f\" --max_steps {max_steps}\"\n        cmd += \" --output_dir checkpoints/agent_sft_ipo\"\n        cmd += \" --beta 0.1\"\n\n        print(\"Training with IPO (preference optimisation)...\")\n        print(f\"  Base checkpoint: {ipo_base}\")\n        print(f\"  Data:            {pref_path}\")\n        print(f\"  Batch:           {batch} x {grad_accum} = {batch * grad_accum}\")\n        print(f\"  Steps:           {max_steps}\")\n        print(f\"  Loss:            IPO (beta=0.1)\")\n        print(f\"  Load mode:       {CONFIG['load_mode']}\")\n        print(f\"  MoE backend:     {CONFIG['moe_backend']}\")\n        print(\"=\" * 60)\n\n        !{cmd}\n\n        drive_helper.backup(\"checkpoints/agent_sft_ipo\", \"checkpoints/agent_sft_ipo\")\n        if DRIVE_MODE != \"local\":\n            print(\"\\nCheckpoint backed up to Drive.\")\n\n        tracker.complete(\"ipo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfecb5aa3fbd0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, glob\n\nif CONFIG[\"training_scope\"] not in (\"tool_calling_only\",) and CONFIG[\"include_preference\"]:\n    print(\"=\" * 60)\n    print(\"  QUALITY GATE: IPO Preference\")\n    print(\"=\" * 60)\n    gate = gate_report[\"ipo\"]\n    gate[\"passed\"] = True\n\n    state_files = glob.glob(\"checkpoints/agent_sft_ipo/**/trainer_state.json\", recursive=True)\n    if state_files:\n        with open(sorted(state_files)[-1]) as f:\n            state = json.load(f)\n        log_history = state.get(\"log_history\", [])\n\n        losses = [e.get(\"loss\", e.get(\"train_loss\")) for e in log_history]\n        losses = [l for l in losses if l is not None]\n        if losses:\n            gate[\"loss\"] = losses[-1]\n            gate[\"first_loss\"] = losses[0]\n            print(f\"  Loss (first):     {losses[0]:.4f}\")\n            print(f\"  Loss (final):     {losses[-1]:.4f}\")\n\n        reward_accs = [e.get(\"rewards/accuracies\", e.get(\"eval_rewards/accuracies\")) for e in log_history]\n        reward_accs = [r for r in reward_accs if r is not None]\n        if reward_accs:\n            gate[\"reward_accuracy\"] = reward_accs[-1]\n            print(f\"  Reward accuracy:  {reward_accs[-1]:.3f}\")\n            if reward_accs[-1] < 0.55:\n                gate[\"notes\"].append(f\"Reward accuracy near chance ({reward_accs[-1]:.3f})\")\n                print(f\"  \u26a0 Barely above chance\")\n            else:\n                print(f\"  \u2713 Distinguishes chosen from rejected\")\n    else:\n        gate[\"notes\"].append(\"No trainer_state.json found\")\n\n    ckpt_path = \"checkpoints/agent_sft_ipo/final\"\n    if os.path.exists(ckpt_path):\n        print(f\"  \u2713 Checkpoint: {len(os.listdir(ckpt_path))} files\")\n    else:\n        gate[\"passed\"] = False\n        gate[\"notes\"].append(\"Checkpoint not found\")\n        print(f\"  \u2717 Checkpoint not found\")\n\n    status = \"\u2705 PASSED\" if gate[\"passed\"] else \"\u274c FAILED\"\n    print(f\"\\n  {status}\")\n    if gate[\"notes\"]:\n        for note in gate[\"notes\"]:\n            print(f\"    \u2192 {note}\")\n    print(\"=\" * 60)\n    print(\"  Continuing to next phase regardless...\")\nelse:\n    print(\"IPO skipped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5ecf10ad9111a3",
   "metadata": {},
   "source": "## Step 5: GRPO RL (Phase 4)\n\nExecution-grounded RL with Codex-style evaluation.\n\n**Reward function:**\n- `+1.0` for code that compiles / passes syntax check\n- `+2.0` for passing test cases\n- `+0.5` for clean linting (no obvious errors)\n- `-1.0` for circular/no-action responses (no code written)\n- `-0.5` for tool calls with malformed JSON parameters\n\n**Goals:**\n- Reinforce follow-through and complete code generation\n- Penalise looping analysis without action\n- Reinforce correct tool parameter formatting\n\n**Optimisations:**\n- FP8 RL with vLLM inference on H100 (1.6x throughput)\n- Chunked batching for longer context\n- Harmony format compliance reward"
  },
  {
   "cell_type": "markdown",
   "id": "14cbc298778aeb40",
   "metadata": {},
   "source": "### 5.1 Train with GRPO"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be37e6df60bcc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n\nif CONFIG[\"training_scope\"] == \"tool_calling_only\":\n    print(\"Skipping \u2014 scope is tool_calling_only\")\n    tracker.skip(\"grpo\")\nelif not CONFIG[\"include_grpo\"]:\n    print(\"Skipping \u2014 include_grpo=False\")\n    tracker.skip(\"grpo\")\nelse:\n    tracker.start(\"grpo\")\n\n    batch = CONFIG[\"grpo_batch\"]\n    grad_accum = CONFIG[\"grpo_grad_accum\"]\n    max_steps = CONFIG[\"grpo_max_steps\"]\n    max_seq = CONFIG[\"grpo_seq_len\"]\n    num_gen = CONFIG[\"grpo_num_gen\"]\n\n    # Determine best checkpoint\n    if os.path.exists(\"checkpoints/agent_sft_ipo/final\"):\n        grpo_base = \"checkpoints/agent_sft_ipo/final\"\n    elif os.path.exists(\"checkpoints/agent_sft/final\"):\n        grpo_base = \"checkpoints/agent_sft/final\"\n        print(\"IPO checkpoint not found, using agent_sft.\")\n    elif os.path.exists(\"checkpoints/tool_calling_sft/final\"):\n        grpo_base = \"checkpoints/tool_calling_sft/final\"\n        print(\"agent_sft not found, using tool_calling_sft.\")\n    else:\n        grpo_base = \"openai/gpt-oss-20b\"\n        print(\"No fine-tuned checkpoint found, using base model.\")\n\n    cmd = \"python scripts/18_grpo_rl.py\"\n    cmd += f\" --checkpoint {grpo_base}\"\n    cmd += f\" --per_device_train_batch_size {batch}\"\n    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n    cmd += f\" --max_steps {max_steps}\"\n    cmd += f\" --num_generations {num_gen}\"\n    cmd += \" --output_dir checkpoints/agent_sft_grpo\"\n    cmd += \" --reward_mode coding_tui\"\n    cmd += \" --developer_prompt 'You are a coding agent. Use tools to read files, write code, run tests, and complete programming tasks. Do not just analyze \u2014 always take action and produce working code.'\"\n\n    v4_features = [f\"Split LoRA ({CONFIG['moe_backend']})\"]\n    if CONFIG[\"load_mode\"] == \"fp8\":\n        v4_features.append(\"FP8 weights\")\n    if CONFIG.get(\"fast_inference\"):\n        v4_features.append(\"vLLM inference\")\n    v4_features += [\"Chunked batching (auto)\", \"Auto packing\"]\n\n    if CONFIG[\"gpu_tier\"] == \"a100_40gb\":\n        print(\"NOTE: 40GB GPU \u2014 GRPO sequence length capped at 16384\")\n\n    print(\"Training with GRPO (execution-grounded RL)...\")\n    print(f\"  Base:          {grpo_base}\")\n    print(f\"  Batch:         {batch} x {grad_accum} = {batch * grad_accum}\")\n    print(f\"  Steps:         {max_steps}\")\n    print(f\"  Seq length:    {max_seq}\")\n    print(f\"  Generations:   {num_gen} per prompt\")\n    print()\n    print(\"  Reward signals:\")\n    print(\"    +1.0 code compiles / passes syntax check\")\n    print(\"    +2.0 test cases pass\")\n    print(\"    +0.5 clean linting\")\n    print(\"    -1.0 circular/no-action response\")\n    print(\"    -0.5 malformed tool call JSON\")\n    print()\n    print(\"  Active features:\")\n    for feat in v4_features:\n        print(f\"    \u2713 {feat}\")\n    print(\"=\" * 60)\n\n    !{cmd}\n\n    drive_helper.backup(\"checkpoints/agent_sft_grpo\", \"checkpoints/agent_sft_grpo\")\n    if DRIVE_MODE != \"local\":\n        print(\"\\nCheckpoint backed up to Drive.\")\n\n    tracker.complete(\"grpo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ba749f7a50ffec",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nif CONFIG[\"training_scope\"] not in (\"tool_calling_only\",) and CONFIG[\"include_grpo\"]:\n    ckpt_path = \"checkpoints/agent_sft_grpo/final\"\n\n    print(\"GRPO Verification:\")\n    print(\"=\" * 60)\n\n    if os.path.exists(ckpt_path):\n        files = os.listdir(ckpt_path)\n        print(f\"  \u2713 GRPO checkpoint: {ckpt_path} ({len(files)} files)\")\n    else:\n        print(f\"  \u2717 GRPO checkpoint not found at {ckpt_path}\")\n\n    print(\"=\" * 60)\nelse:\n    print(\"GRPO skipped for this training scope.\")"
  },
  {
   "cell_type": "markdown",
   "id": "gate_report_header",
   "metadata": {},
   "source": [
    "## Training Quality Report\\n\\nSummary of all quality gates across training phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gate_report_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n\nprint(\"=\" * 70)\nprint(\"  TRAINING QUALITY REPORT\")\nprint(\"=\" * 70)\n\nphases = [\n    (\"Tool Calling SFT\", \"tool_calling\"),\n    (\"Agent SFT\", \"agent_sft\"),\n    (\"IPO Preference\", \"ipo\"),\n    (\"GRPO RL\", \"grpo\"),\n]\n\npassed_count = 0\nfailed_count = 0\nskipped_count = 0\n\nfor label, key in phases:\n    gate = gate_report[key]\n    if gate[\"passed\"] is None:\n        status = \"\u23ed  SKIPPED\"\n        skipped_count += 1\n    elif gate[\"passed\"]:\n        status = \"\u2705 PASSED\"\n        passed_count += 1\n    else:\n        status = \"\u274c FAILED\"\n        failed_count += 1\n\n    loss_str = f\"loss={gate['loss']:.4f}\" if gate.get(\"loss\") else \"no loss data\"\n    print(f\"\\n  {status}  {label} ({loss_str})\")\n\n    if gate.get(\"first_loss\") and gate.get(\"loss\"):\n        imp = (gate[\"first_loss\"] - gate[\"loss\"]) / gate[\"first_loss\"] * 100\n        print(f\"           Loss: {gate['first_loss']:.4f} \u2192 {gate['loss']:.4f} ({imp:+.1f}%)\")\n    if gate.get(\"total_steps\"):\n        print(f\"           Steps: {gate['total_steps']}\")\n    if gate.get(\"reward_accuracy\"):\n        print(f\"           Reward accuracy: {gate['reward_accuracy']:.3f}\")\n    if gate.get(\"takes_action\") is not None:\n        action = \"\u2713 yes\" if gate[\"takes_action\"] else \"\u2717 no\"\n        stall = \"\u2717 yes\" if gate.get(\"stalls\") else \"\u2713 no\"\n        print(f\"           Takes action: {action} | Stalls: {stall}\")\n    if gate.get(\"notes\"):\n        for note in gate[\"notes\"]:\n            print(f\"           \u26a0 {note}\")\n\nprint(f\"\\n{'=' * 70}\")\nprint(f\"  SUMMARY: {passed_count} passed, {failed_count} failed, {skipped_count} skipped\")\n\nif failed_count == 0 and passed_count > 0:\n    print(f\"  \u2192 All gates passed. Model ready for evaluation.\")\nelif failed_count > 0:\n    print(f\"  \u2192 {failed_count} gate(s) failed. Review notes above.\")\n    print(f\"    The final model was still trained \u2014 evaluate it to decide\")\n    print(f\"    whether to retrain specific phases or accept the results.\")\nprint(f\"{'=' * 70}\")\n\n# Save report to disk for reference\nreport_path = \"evals/gate_report.json\"\nos.makedirs(\"evals\", exist_ok=True)\nwith open(report_path, \"w\") as f:\n    # Clean up non-serializable values\n    clean_report = {}\n    for k, v in gate_report.items():\n        clean_report[k] = {kk: vv for kk, vv in v.items() if not callable(vv)}\n    json.dump(clean_report, f, indent=2, default=str)\nprint(f\"\\nReport saved to {report_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceb3aeb8e91d6e7",
   "metadata": {},
   "source": "## Step 6: Evaluation\n\nEvaluate on coding agent tasks targeting the four failure modes:\n1. **Tool call format accuracy** \u2014 JSON schema compliance, valid tool names\n2. **Task completion rate** \u2014 did it actually produce a code change?\n3. **Circular detection rate** \u2014 does it loop the same analysis?\n4. **Code correctness** \u2014 compiles, passes tests"
  },
  {
   "cell_type": "markdown",
   "id": "f4c6a7fde4469632",
   "metadata": {},
   "source": "### 6.1 Run Coding Agent Evaluation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5016ed481527e8",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nif CONFIG[\"training_scope\"] == \"tool_calling_only\":\n    # Still run reduced eval\n    pass\n\ntracker.start(\"eval\")\n\n# Determine best checkpoint\nCHECKPOINT_PRIORITY = [\n    \"checkpoints/agent_sft_grpo/final\",\n    \"checkpoints/agent_sft_ipo/final\",\n    \"checkpoints/agent_sft/final\",\n    \"checkpoints/tool_calling_sft/final\",\n]\n\neval_checkpoint = None\nfor path in CHECKPOINT_PRIORITY:\n    if os.path.exists(path):\n        eval_checkpoint = path\n        break\n\nif eval_checkpoint is None:\n    print(\"\u2717 No checkpoint found. Train the model first.\")\n    tracker.fail(\"eval\")\nelse:\n    num_samples = CONFIG[\"eval_num_samples\"]\n\n    print(f\"Evaluating checkpoint: {eval_checkpoint}\")\n    print(f\"Samples: {num_samples}\")\n    print(\"=\" * 60)\n\n    !python scripts/eval_rust_agent.py \\\n        --checkpoint {eval_checkpoint} \\\n        --num_samples {num_samples} \\\n        --eval_mode coding_tui \\\n        --output_dir evals/coding_tui_agent\n\n    drive_helper.backup(\"evals/coding_tui_agent\", \"evals/coding_tui_agent\")\n    if DRIVE_MODE != \"local\":\n        print(\"\\nResults backed up to Drive.\")\n\n    tracker.complete(\"eval\")"
  },
  {
   "cell_type": "markdown",
   "id": "cffb6581ea3dfaf5",
   "metadata": {},
   "source": "### 6.2 Check Promotion Gates"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300ff1c11b78de01",
   "metadata": {},
   "outputs": [],
   "source": "!python scripts/12_check_gates.py coding_tui_agent"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbffa5f78567ba1",
   "metadata": {},
   "outputs": [],
   "source": "import os, json\n\nmetrics_path = \"evals/coding_tui_agent/metrics.json\"\n\nif os.path.exists(metrics_path):\n    with open(metrics_path) as f:\n        metrics = json.load(f)\n\n    # Targets tuned for a coding TUI agent\n    targets = {\n        \"tool_call_format_accuracy\": (0.95, \"higher\"),   # Valid JSON + known tool names\n        \"task_completion_rate\": (0.70, \"higher\"),         # Actually wrote/modified code\n        \"circular_detection_rate\": (0.10, \"lower\"),       # Loop rate should be low\n        \"code_correctness_rate\": (0.65, \"higher\"),        # Compiles / passes tests\n        \"follow_through_rate\": (0.80, \"higher\"),          # Takes action after planning\n    }\n\n    print(\"=\" * 62)\n    print(\"EVALUATION RESULTS \u2014 Coding TUI Agent\")\n    print(\"=\" * 62)\n    print(f\"  {'Metric':<34} {'Value':>8} {'Target':>8} {'Status':>8}\")\n    print(\"-\" * 62)\n\n    all_pass = True\n    for key, (target, direction) in targets.items():\n        value = metrics.get(key)\n        if value is None:\n            print(f\"  {key:<34} {'N/A':>8} {target:>8} {'\u2014':>8}\")\n            continue\n\n        if direction == \"higher\":\n            passed = value >= target\n        else:\n            passed = value <= target\n\n        if not passed:\n            all_pass = False\n\n        status = \"\u2713 PASS\" if passed else \"\u2717 FAIL\"\n        fmt_val = f\"{value:.1%}\" if isinstance(value, float) and value <= 1 else f\"{value}\"\n        fmt_tgt = f\"{target:.0%}\" if isinstance(target, float) and target <= 1 else f\"{target}\"\n        print(f\"  {key:<34} {fmt_val:>8} {fmt_tgt:>8} {status:>8}\")\n\n    print(\"=\" * 62)\n    if all_pass:\n        print(\"  ALL GATES PASSED \u2713 \u2014 Model ready for export\")\n    else:\n        print(\"  SOME GATES FAILED \u2717 \u2014 Consider additional training\")\n    print(\"=\" * 62)\nelse:\n    print(f\"\u2717 Metrics file not found at {metrics_path}\")\n    print(\"Run evaluation (6.1) first.\")"
  },
  {
   "cell_type": "markdown",
   "id": "9694b8b5ebd0a6a1",
   "metadata": {},
   "source": "## Step 7: Test Model\n\nLoad the trained model and test it interactively against the specific coding TUI\nagent failure modes that this pipeline targets."
  },
  {
   "cell_type": "markdown",
   "id": "831aba486e8d6c31",
   "metadata": {},
   "source": "### 7.1 Load Model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d94f07ab6826e9b",
   "metadata": {},
   "outputs": [],
   "source": "from unsloth import FastLanguageModel\nfrom peft import PeftModel\nimport torch, os\n\nCHECKPOINT_PRIORITY = [\n    \"checkpoints/agent_sft_grpo/final\",\n    \"checkpoints/agent_sft_ipo/final\",\n    \"checkpoints/agent_sft/final\",\n    \"checkpoints/tool_calling_sft/final\",\n]\n\nMERGED_PATH = \"checkpoints/gpt-oss-20b-coding-tui-merged\"\n\nMODEL_PATH = None\nis_adapter = False\nfor path in CHECKPOINT_PRIORITY:\n    if os.path.exists(path) and os.path.exists(os.path.join(path, \"adapter_config.json\")):\n        MODEL_PATH = path\n        is_adapter = True\n        break\n\nif MODEL_PATH is None and os.path.exists(MERGED_PATH):\n    MODEL_PATH = MERGED_PATH\n    is_adapter = False\n\nif MODEL_PATH is None:\n    print(\"\u2717 No checkpoint found. Train the model first.\")\nelse:\n    print(f\"Loading model from: {MODEL_PATH}\")\n    print(f\"  Type: {'LoRA adapter' if is_adapter else 'merged model'}\")\n\n    model = None\n    base_name = \"openai/gpt-oss-20b\"\n\n    # Try 1: Pre-quantized BNB 4-bit (avoids GptOssExperts BNB traversal issue)\n    try:\n        print(\"  Loading pre-quantized BNB 4-bit model...\")\n        model, tokenizer = FastLanguageModel.from_pretrained(\n            \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\",\n            max_seq_length=8192,\n            dtype=None,\n            load_in_4bit=False,\n        )\n        print(\"  Mode: BNB 4-bit (pre-quantized)\")\n    except Exception as e:\n        print(f\"  Pre-quantized BNB failed: {e}\")\n\n    # Try 2: bfloat16 without quantization\n    if model is None:\n        print(\"  Loading in bfloat16 (no quantization)...\")\n        model, tokenizer = FastLanguageModel.from_pretrained(\n            base_name,\n            max_seq_length=8192,\n            dtype=torch.bfloat16,\n            load_in_4bit=False,\n        )\n        print(\"  Mode: bfloat16 (no quantization)\")\n\n    if is_adapter:\n        print(f\"  Applying LoRA adapter from {MODEL_PATH}...\")\n        model = PeftModel.from_pretrained(model, MODEL_PATH)\n\n    FastLanguageModel.for_inference(model)\n    print(\"\u2713 Model loaded!\")"
  },
  {
   "cell_type": "markdown",
   "id": "5586e6239f2a9833",
   "metadata": {},
   "source": "### 7.2 Test Against Failure Modes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e7ea5f0916ba33",
   "metadata": {},
   "outputs": [],
   "source": "import sys, torch\nsys.path.insert(0, \"scripts\")\nfrom dataset_formatters.harmony import encode_harmony_messages\n\nCODING_AGENT_DEV = (\n    \"You are a coding agent. Use tools to read files, write code, run tests, and \"\n    \"complete programming tasks. Do not just analyze \u2014 always take action and produce \"\n    \"working code. After making changes, verify they work by running the relevant tests. \"\n    \"If a tool call fails, diagnose and retry with corrected parameters.\"\n)\n\n# Test prompts designed to expose each failure mode\nTEST_PROMPTS = [\n    # Test 1: Tool calling accuracy \u2014 should call read_file with valid path, NOT a made-up tool\n    (\n        \"Failure Mode: Tool Calling\",\n        \"Read the file at src/main.rs and fix any compilation errors you find.\",\n        [\"read_file\", \"write_file\", \"run_command\"],\n    ),\n    # Test 2: Follow-through \u2014 should NOT just say \"I would need to look at...\"\n    (\n        \"Failure Mode: No Follow-Through\",\n        \"Write a Python function called `binary_search(arr, target)` that returns the index of target in sorted arr, or -1 if not found. Add it to utils.py and write a pytest test for it.\",\n        None,\n    ),\n    # Test 3: Circular reasoning \u2014 model should take action, not loop\n    (\n        \"Failure Mode: Circular Reasoning\",\n        \"Analyze the codebase and suggest improvements. Then implement the most impactful one.\",\n        None,\n    ),\n    # Test 4: Context tracking \u2014 should remember the task mid-session\n    (\n        \"Failure Mode: Context Loss\",\n        \"I need you to refactor the authentication module. Start by reading auth.py, then identify the issues, then fix them one by one.\",\n        None,\n    ),\n]\n\ndef generate_response(prompt, tools=None, max_tokens=512):\n    \"\"\"Generate a response using Harmony format.\"\"\"\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    if tools:\n        tool_ctx = \"\\n\".join(f\"  - {t}(path: str)\" for t in tools)\n        messages[0][\"content\"] = (\n            f\"Available tools:\\n{tool_ctx}\\n\\n\" + messages[0][\"content\"]\n        )\n    formatted = encode_harmony_messages(\n        messages,\n        developer_instructions=CODING_AGENT_DEV,\n        add_generation_prompt=True,\n    )\n    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_tokens,\n            temperature=0.3,\n            do_sample=True,\n            top_p=0.9,\n        )\n    return tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n\n\nfor label, prompt, tools in TEST_PROMPTS:\n    print(f\"\\n{'=' * 64}\")\n    print(f\"TEST: {label}\")\n    print(f\"{'=' * 64}\")\n    print(f\"Prompt: {prompt[:120]}...\")\n    print(\"-\" * 64)\n    response = generate_response(prompt, tools, max_tokens=384)\n    print(response)\n    print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e043b8c87b869b0",
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Custom Prompt \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nCUSTOM_PROMPT = (\n    \"Read requirements.txt and install any missing packages, \"\n    \"then run the test suite and fix any failing tests.\"\n)\n\nprint(f\"Custom prompt: {CUSTOM_PROMPT}\")\nprint(\"=\" * 64)\nprint(generate_response(CUSTOM_PROMPT, max_tokens=512))"
  },
  {
   "cell_type": "markdown",
   "id": "e57b5e0fcf8554b1",
   "metadata": {},
   "source": "## Step 8: Export\n\nMerge the final adapter and export to HuggingFace safetensors + GGUF formats.\n\nThe GGUF file can be loaded directly into MacLean AI via llama-server\nfor Codex CLI integration testing."
  },
  {
   "cell_type": "markdown",
   "id": "52400da4f021f454",
   "metadata": {},
   "source": "### 8.1 Export to GGUF"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47003e9d53b4dcf9",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\ntracker.start(\"export\")\n\nADAPTER_PRIORITY = [\n    \"checkpoints/agent_sft_grpo/final\",\n    \"checkpoints/agent_sft_ipo/final\",\n    \"checkpoints/agent_sft/final\",\n    \"checkpoints/tool_calling_sft/final\",\n]\n\nadapter_path = None\nfor path in ADAPTER_PRIORITY:\n    if os.path.exists(path):\n        adapter_path = path\n        break\n\nif adapter_path is None:\n    print(\"\u2717 No adapter checkpoint found.\")\n    tracker.fail(\"export\")\nelse:\n    export_dir = \"checkpoints/gpt-oss-20b-coding-tui-export\"\n    print(f\"Exporting adapter: {adapter_path}\")\n    print(f\"Output: {export_dir}\")\n    print(\"=\" * 60)\n\n    !python scripts/19_merge_adapter.py \\\n        --adapter_path {adapter_path} \\\n        --output_dir {export_dir} \\\n        --export_formats hf gguf_q4\n\n    drive_helper.backup(export_dir, \"checkpoints/gpt-oss-20b-coding-tui-export\")\n    if DRIVE_MODE != \"local\":\n        print(\"\\nExport backed up to Drive.\")\n\n    tracker.complete(\"export\")"
  },
  {
   "cell_type": "markdown",
   "id": "e3fd14940c046655",
   "metadata": {},
   "source": "### 8.2 QAT Export (Optional)\n\nQuantisation-Aware Training for MXFP4 deployment.\nRecovers 97-100% quality vs 59-89% with post-training quantisation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9591992819e91213",
   "metadata": {},
   "outputs": [],
   "source": "if not CONFIG.get(\"enable_qat_export\"):\n    print(\"QAT export disabled. Enable via widget toggle in Step 0.3.\")\n    print(\"\\nQAT recovers 97-100% quality when deploying to MXFP4,\")\n    print(\"vs 59-89% with standard post-training quantisation (PTQ).\")\nelse:\n    import os\n    export_dir = \"checkpoints/gpt-oss-20b-coding-tui-export\"\n    qat_dir = \"checkpoints/gpt-oss-20b-coding-tui-qat\"\n\n    if not os.path.exists(export_dir):\n        print(\"\u2717 Run standard export (8.1) first.\")\n    else:\n        print(\"Running QAT pass on merged model...\")\n        print(\"  This fine-tunes with MXFP4-aware quantisation at reduced LR (1e-5).\")\n        print(\"=\" * 60)\n\n        try:\n            import modelopt.torch.quantization as mtq\n            print(\"\u2713 nvidia-modelopt available\")\n            print(\"\\nQAT pipeline (manual steps):\")\n            print(f\"  1. Load merged BF16 model from {export_dir}\")\n            print(f\"  2. mtq.quantize(model, config=mtq.MXFP4_DEFAULT_CFG)\")\n            print(f\"  3. Fine-tune for ~100 steps at LR 1e-5\")\n            print(f\"  4. Export to {qat_dir}\")\n        except ImportError:\n            print(\"\u2717 nvidia-modelopt not installed.\")\n            print(\"  Install: pip install nvidia-modelopt\")"
  },
  {
   "cell_type": "markdown",
   "id": "d21eff13aa7e28d2",
   "metadata": {},
   "source": "### 8.3 Download GGUF"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee853f0eb5a9b139",
   "metadata": {},
   "outputs": [],
   "source": "IN_COLAB = \"COLAB_GPU\" in os.environ or os.path.exists(\"/content\")\n\nif IN_COLAB:\n    from google.colab import files\n    import glob, os\n\n    export_dir = \"checkpoints/gpt-oss-20b-coding-tui-export\"\n    gguf_files = glob.glob(os.path.join(export_dir, \"**/*.gguf\"), recursive=True)\n\n    if gguf_files:\n        gguf_path = gguf_files[0]\n        size_gb = os.path.getsize(gguf_path) / (1024**3)\n        print(f\"Downloading: {os.path.basename(gguf_path)} ({size_gb:.1f} GB)\")\n        files.download(gguf_path)\n    else:\n        print(\"\u2717 No GGUF file found. Run export (8.1) first.\")\nelse:\n    print(\"Download not available outside Colab.\")\n    print(\"GGUF file is at: checkpoints/gpt-oss-20b-coding-tui-export/\")"
  },
  {
   "cell_type": "markdown",
   "id": "bf24311a84a2f0d2",
   "metadata": {},
   "source": "### 8.4 Upload to HuggingFace Hub"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2fb0e228f0c57a",
   "metadata": {},
   "outputs": [],
   "source": "# --- Configuration ---\nHF_REPO_ID = \"\"  # e.g. \"your-username/gpt-oss-20b-coding-tui-agent\"\nHF_PRIVATE = True\n\nassert HF_REPO_ID, \"Set HF_REPO_ID above before running this cell.\"\n\nimport os, glob\nfrom huggingface_hub import HfApi\n\n# Authenticate: try Colab Secrets first, then interactive login\ntry:\n    from google.colab import userdata\n    hf_token = userdata.get(\"HF_TOKEN\")\n    print(\"Using HF_TOKEN from Colab Secrets.\")\nexcept Exception:\n    from huggingface_hub import login\n    login()\n    hf_token = None\n\napi = HfApi(token=hf_token)\napi.create_repo(repo_id=HF_REPO_ID, private=HF_PRIVATE, exist_ok=True)\nprint(f\"Repo ready: https://huggingface.co/{HF_REPO_ID}\")\n\n# --- Model card ---\nexport_dir = \"checkpoints/gpt-oss-20b-coding-tui-export\"\nhf_dir = os.path.join(export_dir, \"hf\")\n\nmodel_card = \"\"\"\\\n---\nbase_model: openai/gpt-oss-20b\ntags:\n  - coding-agent\n  - tool-calling\n  - codex-cli\n  - gpt-oss\n  - qlora\n  - unsloth\n  - grpo\n  - tui\nlicense: apache-2.0\npipeline_tag: text-generation\n---\n\n# GPT-OSS 20B Coding TUI Agent\n\nFine-tuned from [openai/gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b) for\nuse as a coding TUI agent (Codex CLI integration via MacLean AI + llama-server).\n\n## Problem Statement\n\nGPT-OSS 20B exhibits four failure modes when used as a coding agent:\n1. Tool calling errors (invalid params, non-existent MCP servers)\n2. No follow-through (analysis loops, never writes code)\n3. Circular reasoning (repeating the same analysis)\n4. Context loss (forgetting task state mid-session)\n\n## Training Pipeline\n\n1. **Tool Calling SFT** (rank 64) \\u2014 Glaive + xLAM + Hermes in Harmony format\n2. **Merge** \\u2014 tool-calling adapter merged into base\n3. **Agent SFT** (rank 128) \\u2014 code-act, commitpack, editpackft + proxy log trajectories\n4. **IPO** \\u2014 decisive action preferred over circular analysis\n5. **GRPO RL** \\u2014 execution-grounded: code compiles, tests pass, no loops\n\nTrained with [Unsloth](https://github.com/unslothai/unsloth) QLoRA.\n\n## Deployment\n\nDesigned for deployment via [llama-server](https://github.com/ggerganov/llama.cpp)\nwith the [claude-proxy-v2](https://github.com/rmarnold/claude-proxy-v2) translation layer\nfor Codex CLI (OpenAI Responses API).\n\n## GGUF\n\nA quantised GGUF file is included for use with llama.cpp.\n\"\"\".format()\n\nreadme_path = os.path.join(hf_dir, \"README.md\")\nos.makedirs(hf_dir, exist_ok=True)\nwith open(readme_path, \"w\") as f:\n    f.write(model_card)\nprint(f\"Wrote model card to {readme_path}\")\n\n# --- Upload HF safetensors model ---\nassert os.path.isdir(hf_dir), f\"HF export dir not found: {hf_dir}. Run export (8.1) first.\"\nprint(f\"Uploading HF model from {hf_dir} ...\")\napi.upload_folder(\n    folder_path=hf_dir,\n    repo_id=HF_REPO_ID,\n    commit_message=\"Upload GPT-OSS 20B Coding TUI Agent (tool-calling + agent-SFT + IPO + GRPO)\",\n    token=hf_token,\n)\nprint(\"HF model uploaded.\")\n\n# --- Upload GGUF file ---\ngguf_files = glob.glob(os.path.join(export_dir, \"**/*.gguf\"), recursive=True)\nif gguf_files:\n    gguf_path = gguf_files[0]\n    gguf_name = os.path.basename(gguf_path)\n    size_gb = os.path.getsize(gguf_path) / (1024**3)\n    print(f\"Uploading GGUF: {gguf_name} ({size_gb:.1f} GB) ...\")\n    api.upload_file(\n        path_or_fileobj=gguf_path,\n        path_in_repo=gguf_name,\n        repo_id=HF_REPO_ID,\n        commit_message=f\"Upload GGUF quantisation ({gguf_name})\",\n        token=hf_token,\n    )\n    print(\"GGUF uploaded.\")\nelse:\n    print(\"No GGUF file found \\u2014 skipping. Run export (8.1) to generate one.\")\n\nprint(f\"\\nDone! View your model at: https://huggingface.co/{HF_REPO_ID}\")"
  },
  {
   "cell_type": "markdown",
   "id": "fb181a0eed17d3a5",
   "metadata": {},
   "source": "---\n## Training Complete!\n\nYour GPT-OSS 20B Coding TUI Agent is trained and ready for Codex CLI integration.\n\n**Pipeline summary:**\n1. Tool Calling SFT: Glaive (113K) + xLAM (60K) + Hermes \u2014 correct tool schemas and parameter formatting\n2. Merge: tool-calling adapter fused into base weights\n3. Agent SFT: code-act + commitpack + editpackft + real proxy log trajectories (most valuable)\n4. IPO: decisive action preferred over circular analysis (hh-rlhf + code feedback)\n5. GRPO RL: execution-grounded \u2014 rewards for compiling code and passing tests, penalises loops\n\n**Outputs:**\n- Checkpoints: `checkpoints/agent_sft_{ipo,grpo}/final`\n- Evaluation: `evals/coding_tui_agent/metrics.json`\n- Exported model: `checkpoints/gpt-oss-20b-coding-tui-export/`\n- All backed up to Google Drive: `gpt-oss-20b-coding-tui/`\n\n**MacLean AI integration:**\n- Copy the exported GGUF to your MacLean AI model directory\n- Select it in the Model Browser\n- Enable Codex CLI support in Settings\n- The proxy translation layer (`--translate-anthropic`) handles the Anthropic\u2194OpenAI format conversion\n- Test with: `codex \"Read src/main.rs and fix any compilation errors\"`\n\n**Next steps:**\n- Review evaluation metrics in Step 6\n- Test against each failure mode in Step 7\n- If circular reasoning persists: increase GRPO steps or add more no-action penalisation\n- If tool calling is still poor: increase tool_calling_sft steps or add more xLAM data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c65eef8e83ef10",
   "metadata": {},
   "outputs": [],
   "source": "# Disconnect and release GPU runtime to stop billing\ntry:\n    from google.colab import runtime\n    runtime.unassign()\nexcept ImportError:\n    print(\"Not in Colab \u2014 no runtime to release.\")"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}