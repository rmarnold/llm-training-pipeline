{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac31bce43c5cfb0c",
   "metadata": {},
   "source": "# Train GPT-OSS 20B Coding TUI Agent\n\n**Combined pipeline** — Tool calling SFT + agent trajectory training + proxy log extraction + IPO preference + GRPO RL.\n\n**Target failure modes to fix:**\n- Tool calling errors (invalid params, non-existent MCP servers)\n- No follow-through (analysis loops, never writes code)\n- Circular reasoning (repeating the same analysis)\n- Context loss (forgetting task state mid-session)\n\n**Pipeline:**\n1. Tool calling SFT (rank 64 LoRA)\n2. Merge → Agent SFT from proxy log trajectories (rank 128 LoRA)\n3. IPO preference optimisation (decisive action > endless analysis)\n4. GRPO RL (execution-grounded: compiles / tests pass / no loops)\n5. Eval → Export\n\n**Base model:** [openai/gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b) (20.9B MoE, 3.6B active)\n\n**Data sources:**\n- [glaiveai/glaive-function-calling-v2](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2) (113K)\n- [Salesforce/xlam-function-calling-60k](https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k) (60K)\n- [NousResearch/hermes-function-calling-v1](https://huggingface.co/datasets/NousResearch/hermes-function-calling-v1)\n- [xingyaoww/code-act](https://huggingface.co/datasets/xingyaoww/code-act)\n- [bigcode/commitpack](https://huggingface.co/datasets/bigcode/commitpack) (50K subsample)\n- [bigcode/editpackft](https://huggingface.co/datasets/bigcode/editpackft) (50K subsample)\n- [Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf)\n- [m-a-p/CodeFeedback-Filtered-Instruction](https://huggingface.co/datasets/m-a-p/CodeFeedback-Filtered-Instruction)\n- MacLean AI proxy logs (real Codex CLI agent sessions — most valuable source)"
  },
  {
   "cell_type": "markdown",
   "id": "8c223b20d8c73506",
   "metadata": {},
   "source": "## Step 0: Environment Setup"
  },
  {
   "cell_type": "markdown",
   "id": "d5b7cc89e998088b",
   "metadata": {},
   "source": "### 0.1 Mount Google Drive & Clone Repository"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f8caf22f35ebd8",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nIN_COLAB = \"COLAB_GPU\" in os.environ or os.path.exists(\"/content\")\n\nDRIVE_BASE = \"\"\nDRIVE_MODE = \"local\"\n\nif IN_COLAB:\n    from google.colab import drive\n    drive.mount(\"/content/drive\")\n    DRIVE_BASE = \"/content/drive/MyDrive/gpt-oss-20b-coding-tui\"\n    DRIVE_MODE = \"mounted\"\n    os.makedirs(DRIVE_BASE, exist_ok=True)\n\n    if not os.path.exists(\"llm-training-pipeline\"):\n        !git clone https://github.com/rmarnold/llm-training-pipeline.git\n    os.chdir(\"llm-training-pipeline\")\n    !git pull --ff-only\n    print(f\"Working directory: {os.getcwd()}\")\nelse:\n    print(\"Running locally (not in Colab).\")\n    print(f\"Working directory: {os.getcwd()}\")"
  },
  {
   "cell_type": "markdown",
   "id": "32511fbe9341b45c",
   "metadata": {},
   "source": "### 0.2 Install Dependencies"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565fd802759877f7",
   "metadata": {},
   "outputs": [],
   "source": "import subprocess, sys, os\n\nIN_COLAB = \"COLAB_GPU\" in os.environ or os.path.exists(\"/content\")\n\nif IN_COLAB:\n    # Core + GPT-OSS deps\n    !pip install -q -e \".[gpt_oss]\"\n\n    # Unsloth (Colab optimised)\n    !pip install -q unsloth\n\n    # vLLM for fast inference in GRPO\n    !pip install -q vllm\n\n    # ipywidgets for config UI\n    !pip install -q ipywidgets\n\n    # Datasets for HuggingFace downloads\n    !pip install -q datasets huggingface_hub\n\n    # Code execution evaluation deps\n    !pip install -q pyflakes astunparse\n\n    print(\"\\nDependencies installed.\")\nelse:\n    print(\"Assuming local dependencies are already installed.\")\n    print(\"Run: pip install -e '.[gpt_oss]'\")"
  },
  {
   "cell_type": "markdown",
   "id": "1d256e0fbc370a68",
   "metadata": {},
   "source": "### 0.3 Configure Pipeline\n\nToggle the form view (click the \"...\" menu on this cell) to see the interactive configuration panel.\nAdjust settings, then **run this cell** to apply them."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e618b72f6afe400",
   "metadata": {},
   "outputs": [],
   "source": "#@title ### Pipeline Configuration { display-mode: \"form\" }\n\n#@markdown ---\n#@markdown #### Core Settings\n\ntraining_scope = \"quick_test\"  #@param [\"full\", \"quick_test\", \"tool_calling_only\", \"skip_to_rl\"] {type: \"string\"}\ngpu_tier = \"h100_80gb\"  #@param [\"a100_40gb\", \"a100_80gb\", \"h100_80gb\"] {type: \"string\"}\nmax_steps_override = 0  #@param {type: \"integer\"}\n\n#@markdown > *Max Steps Override: 0 = use GPU tier defaults. Set > 0 to cap all stages.*\n\n#@markdown ---\n#@markdown #### Data Sources\n\ninclude_proxy_logs = True  #@param {type: \"boolean\"}\nproxy_log_dir = \"\"  #@param {type: \"string\"}\n\n#@markdown > *proxy_log_dir: path to MacLean AI proxy log directory (contains per-request JSON files).*\n\ninclude_tool_calling = True  #@param {type: \"boolean\"}\ninclude_agent_trajectories = True  #@param {type: \"boolean\"}\n\n#@markdown ---\n#@markdown #### Pipeline Phases\n\ninclude_preference = True  #@param {type: \"boolean\"}\ninclude_grpo = True  #@param {type: \"boolean\"}\nskip_data_generation = False  #@param {type: \"boolean\"}\n\n#@markdown ---\n#@markdown #### Export\n\nenable_qat_export = False  #@param {type: \"boolean\"}\n\n#@markdown ---\n#@markdown #### Advanced\n\nuse_service_account = False  #@param {type: \"boolean\"}\ndrive_folder_id = \"\"  #@param {type: \"string\"}\n\n# ======================================================================\n# GPU tier presets (auto-selected based on gpu_tier above)\n# ======================================================================\nimport os, sys, json\n\nGPU_CONFIGS = {\n    \"a100_40gb\": {\n        \"tool_calling_batch\": 2, \"tool_calling_grad_accum\": 16, \"tool_calling_max_steps\": 3000, \"tool_calling_seq_len\": 4096,\n        \"agent_sft_batch\": 1, \"agent_sft_grad_accum\": 8, \"agent_sft_max_steps\": 2000, \"agent_sft_seq_len\": 8192,\n        \"ipo_batch\": 1, \"ipo_grad_accum\": 16, \"ipo_max_steps\": 1000, \"ipo_seq_len\": 4096,\n        \"grpo_batch\": 1, \"grpo_grad_accum\": 8, \"grpo_max_steps\": 3000, \"grpo_seq_len\": 16384, \"grpo_num_gen\": 4,\n        \"eval_num_samples\": 100,\n        \"load_mode\": \"4bit\", \"moe_backend\": \"triton\", \"fast_inference\": False,\n    },\n    \"a100_80gb\": {\n        \"tool_calling_batch\": 4, \"tool_calling_grad_accum\": 8, \"tool_calling_max_steps\": 3000, \"tool_calling_seq_len\": 8192,\n        \"agent_sft_batch\": 2, \"agent_sft_grad_accum\": 4, \"agent_sft_max_steps\": 2000, \"agent_sft_seq_len\": 16384,\n        \"ipo_batch\": 1, \"ipo_grad_accum\": 16, \"ipo_max_steps\": 1000, \"ipo_seq_len\": 8192,\n        \"grpo_batch\": 1, \"grpo_grad_accum\": 8, \"grpo_max_steps\": 5000, \"grpo_seq_len\": 32768, \"grpo_num_gen\": 4,\n        \"eval_num_samples\": 200,\n        \"load_mode\": \"4bit\", \"moe_backend\": \"triton\", \"fast_inference\": False,\n    },\n    \"h100_80gb\": {\n        \"tool_calling_batch\": 6, \"tool_calling_grad_accum\": 8, \"tool_calling_max_steps\": 3000, \"tool_calling_seq_len\": 8192,\n        \"agent_sft_batch\": 6, \"agent_sft_grad_accum\": 4, \"agent_sft_max_steps\": 2000, \"agent_sft_seq_len\": 16384,\n        \"ipo_batch\": 2, \"ipo_grad_accum\": 16, \"ipo_max_steps\": 1000, \"ipo_seq_len\": 8192,\n        \"grpo_batch\": 2, \"grpo_grad_accum\": 8, \"grpo_max_steps\": 5000, \"grpo_seq_len\": 65536, \"grpo_num_gen\": 4,\n        \"eval_num_samples\": 200,\n        \"load_mode\": \"fp8\", \"moe_backend\": \"triton\", \"fast_inference\": True,\n    },\n}\n\ntier = GPU_CONFIGS[gpu_tier]\n\n# ======================================================================\n# Build CONFIG dict from form values\n# ======================================================================\nCONFIG = {\n    \"training_scope\": training_scope,\n    \"gpu_tier\": gpu_tier,\n    **tier,\n    # Data sources\n    \"include_proxy_logs\": include_proxy_logs,\n    \"proxy_log_dir\": proxy_log_dir,\n    \"include_tool_calling\": include_tool_calling,\n    \"include_agent_trajectories\": include_agent_trajectories,\n    # Pipeline phases\n    \"include_preference\": include_preference,\n    \"include_grpo\": include_grpo,\n    \"enable_qat_export\": enable_qat_export,\n    \"skip_data_generation\": skip_data_generation,\n    # Advanced\n    \"use_service_account\": use_service_account,\n    \"drive_folder_id\": drive_folder_id,\n}\n\n# Apply max_steps override\nif max_steps_override > 0:\n    for key in list(CONFIG.keys()):\n        if key.endswith(\"_max_steps\"):\n            CONFIG[key] = max_steps_override\n\n# Quick test caps\nif CONFIG[\"training_scope\"] == \"quick_test\":\n    for key in list(CONFIG.keys()):\n        if key.endswith(\"_max_steps\"):\n            CONFIG[key] = min(CONFIG[key], 50)\n    CONFIG[\"eval_num_samples\"] = 10\n\n# Scope-based overrides\nif CONFIG[\"training_scope\"] == \"tool_calling_only\":\n    CONFIG[\"include_preference\"] = False\n    CONFIG[\"include_grpo\"] = False\n    CONFIG[\"include_agent_trajectories\"] = False\nelif CONFIG[\"training_scope\"] == \"skip_to_rl\":\n    CONFIG[\"include_tool_calling\"] = False\n    CONFIG[\"include_agent_trajectories\"] = False\n\n# ======================================================================\n# Set up DriveHelper\n# ======================================================================\nsys.path.insert(0, \"scripts\")\nfrom pipeline_lib.drive_utils import DriveHelper\n\nif \"DRIVE_BASE\" not in dir():\n    DRIVE_BASE = \"\"\nif \"DRIVE_MODE\" not in dir():\n    DRIVE_MODE = \"local\"\n\nif CONFIG[\"use_service_account\"] and CONFIG[\"drive_folder_id\"]:\n    sa_path = \"service_account.json\"\n    try:\n        from google.colab import userdata\n        sa_json = userdata.get(\"SERVICE_ACCOUNT_JSON\")\n        with open(sa_path, \"w\") as f:\n            f.write(sa_json)\n    except Exception:\n        pass\n\n    if os.path.exists(sa_path) and os.path.getsize(sa_path) > 10:\n        try:\n            drive_helper = DriveHelper(\n                mode=\"service_account\",\n                credentials_path=sa_path,\n                folder_id=CONFIG[\"drive_folder_id\"],\n            )\n            DRIVE_MODE = \"service_account\"\n        except Exception as e:\n            print(f\"Service account failed: {e}\")\n            drive_helper = DriveHelper(mode=\"local\")\n            DRIVE_MODE = \"local\"\n    else:\n        drive_helper = DriveHelper(mode=\"local\")\n        DRIVE_MODE = \"local\"\nelif DRIVE_BASE:\n    drive_helper = DriveHelper(mode=\"mounted\", drive_base=DRIVE_BASE)\n    DRIVE_MODE = \"mounted\"\nelse:\n    drive_helper = DriveHelper(mode=\"local\")\n    DRIVE_MODE = \"local\"\n\n# Save for persistence across restarts\nos.makedirs(\"data\", exist_ok=True)\nwith open(\"data/config_coding_tui.json\", \"w\") as f:\n    json.dump(CONFIG, f, indent=2)\n\n# ======================================================================\n# Print summary\n# ======================================================================\nprint(\"=\" * 58)\nprint(\"  PIPELINE CONFIGURATION (Coding TUI Agent)\")\nprint(\"=\" * 58)\nprint(f\"  Scope:           {CONFIG['training_scope'].upper()}\")\nprint(f\"  GPU tier:        {CONFIG['gpu_tier']}\")\nprint(f\"  MoE backend:     {CONFIG['moe_backend']}\")\nprint(f\"  Load mode:       {CONFIG['load_mode']}\")\nprint(f\"  Drive mode:      {DRIVE_MODE}\")\nprint()\nprint(f\"  Proxy logs:      {CONFIG['include_proxy_logs']}\")\nif CONFIG[\"include_proxy_logs\"] and CONFIG[\"proxy_log_dir\"]:\n    print(f\"    Log dir:       {CONFIG['proxy_log_dir']}\")\nprint(f\"  Tool calling:    {CONFIG['include_tool_calling']}\")\nprint(f\"  Agent traj:      {CONFIG['include_agent_trajectories']}\")\nprint(f\"  IPO preference:  {CONFIG['include_preference']}\")\nprint(f\"  GRPO:            {CONFIG['include_grpo']}\")\nprint(f\"  QAT export:      {CONFIG['enable_qat_export']}\")\nif max_steps_override > 0:\n    print(f\"  Max steps:       {max_steps_override} (override)\")\nprint()\nprint(f\"  Tool Calling SFT: batch={CONFIG['tool_calling_batch']} x grad={CONFIG['tool_calling_grad_accum']}, seq={CONFIG['tool_calling_seq_len']}, steps={CONFIG['tool_calling_max_steps']}\")\nprint(f\"  Agent SFT:        batch={CONFIG['agent_sft_batch']} x grad={CONFIG['agent_sft_grad_accum']}, seq={CONFIG['agent_sft_seq_len']}, steps={CONFIG['agent_sft_max_steps']}\")\nif CONFIG[\"include_preference\"]:\n    print(f\"  IPO:              batch={CONFIG['ipo_batch']} x grad={CONFIG['ipo_grad_accum']}, seq={CONFIG['ipo_seq_len']}, steps={CONFIG['ipo_max_steps']}\")\nif CONFIG[\"include_grpo\"]:\n    print(f\"  GRPO:             batch={CONFIG['grpo_batch']} x grad={CONFIG['grpo_grad_accum']}, seq={CONFIG['grpo_seq_len']}, steps={CONFIG['grpo_max_steps']}\")\nprint(\"=\" * 58)"
  },
  {
   "cell_type": "markdown",
   "id": "8c6573f767ff170f",
   "metadata": {},
   "source": "### 0.4 Pipeline Dashboard"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d75422c4badeb55",
   "metadata": {},
   "outputs": [],
   "source": "import ipywidgets as widgets\nfrom IPython.display import display\n\nclass PipelineTracker:\n    \"\"\"Track pipeline progress with visual indicators.\"\"\"\n\n    PHASES = [\n        (\"tool_calling_data\", \"Tool Calling Data\"),\n        (\"agent_traj_data\", \"Agent Trajectory Data\"),\n        (\"proxy_log_extract\", \"Proxy Log Extraction\"),\n        (\"tool_calling_sft\", \"Tool Calling SFT\"),\n        (\"merge\", \"Merge Adapter\"),\n        (\"agent_sft\", \"Agent SFT\"),\n        (\"ipo\", \"IPO Preference\"),\n        (\"grpo\", \"GRPO RL\"),\n        (\"eval\", \"Evaluation\"),\n        (\"export\", \"Export\"),\n    ]\n\n    def __init__(self):\n        self._bars = {}\n        self._labels = {}\n        rows = []\n        for key, name in self.PHASES:\n            label = widgets.HTML(\n                value=f\"<span style='color:#888'>&#x25CB; {name}</span>\",\n                layout=widgets.Layout(width=\"240px\"),\n            )\n            bar = widgets.FloatProgress(\n                value=0, min=0, max=1.0,\n                bar_style=\"info\",\n                layout=widgets.Layout(width=\"300px\", height=\"18px\"),\n            )\n            self._bars[key] = bar\n            self._labels[key] = label\n            rows.append(widgets.HBox([label, bar]))\n        self._container = widgets.VBox(rows)\n        display(widgets.HTML(\"<b>Pipeline Progress</b>\"))\n        display(self._container)\n\n    def start(self, phase):\n        self._labels[phase].value = (\n            f\"<span style='color:#2196F3'>&#x25B6; {dict(self.PHASES)[phase]}</span>\"\n        )\n        self._bars[phase].value = 0.1\n        self._bars[phase].bar_style = \"info\"\n\n    def complete(self, phase):\n        self._labels[phase].value = (\n            f\"<span style='color:#4CAF50'>&#x2714; {dict(self.PHASES)[phase]}</span>\"\n        )\n        self._bars[phase].value = 1.0\n        self._bars[phase].bar_style = \"success\"\n\n    def skip(self, phase):\n        self._labels[phase].value = (\n            f\"<span style='color:#9E9E9E'>&#x2014; {dict(self.PHASES)[phase]} (skipped)</span>\"\n        )\n        self._bars[phase].value = 1.0\n        self._bars[phase].bar_style = \"\"\n\n    def fail(self, phase):\n        self._labels[phase].value = (\n            f\"<span style='color:#F44336'>&#x2718; {dict(self.PHASES)[phase]}</span>\"\n        )\n        self._bars[phase].bar_style = \"danger\"\n\ntracker = PipelineTracker()"
  },
  {
   "cell_type": "markdown",
   "id": "3a29921984a191c6",
   "metadata": {},
   "source": "### 0.5 Set Up Persistent Storage"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac3698fb40edf9a",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nDRIVE_SUBDIRS = [\n    \"data/coding_tui/tool_calling\",\n    \"data/coding_tui/agent_traj\",\n    \"data/coding_tui/proxy_logs\",\n    \"data/coding_tui/preference\",\n    \"data/coding_tui/grpo\",\n    \"data/coding_tui/eval\",\n    \"checkpoints/tool_calling_sft\",\n    \"checkpoints/gpt-oss-20b-coding-tui-merged\",\n    \"checkpoints/agent_sft\",\n    \"checkpoints/agent_sft_ipo\",\n    \"checkpoints/agent_sft_grpo\",\n    \"evals\",\n]\n\nif DRIVE_MODE == \"mounted\":\n    for subdir in DRIVE_SUBDIRS:\n        drive_path = os.path.join(DRIVE_BASE, subdir)\n        os.makedirs(drive_path, exist_ok=True)\n        local_path = subdir\n        if not os.path.exists(local_path):\n            os.makedirs(os.path.dirname(local_path) or \".\", exist_ok=True)\n            os.symlink(drive_path, local_path)\n            print(f\"  Linked: {local_path} -> {drive_path}\")\n        else:\n            print(f\"  Exists: {local_path}\")\n    print(f\"\\nDrive base: {DRIVE_BASE}\")\nelif DRIVE_MODE == \"service_account\":\n    for subdir in DRIVE_SUBDIRS:\n        os.makedirs(subdir, exist_ok=True)\n        drive_helper.ensure_dir(subdir)\n    print(\"Drive directories created (service account mode).\")\nelse:\n    for subdir in DRIVE_SUBDIRS:\n        os.makedirs(subdir, exist_ok=True)\n    print(\"Local directories created (no Drive backup).\")"
  },
  {
   "cell_type": "markdown",
   "id": "160ebeffc16f9fab",
   "metadata": {},
   "source": "### 0.6 Check GPU & Configure MoE Backend"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460744279495b571",
   "metadata": {},
   "outputs": [],
   "source": "import torch, os\n\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n\n    print(f\"GPU: {gpu_name}\")\n    print(f\"VRAM: {gpu_mem:.1f} GB\")\n\n    # Auto-detect GPU tier override\n    detected_tier = None\n    if \"H100\" in gpu_name or \"H200\" in gpu_name:\n        detected_tier = \"h100_80gb\"\n    elif \"A100\" in gpu_name:\n        detected_tier = \"a100_80gb\" if gpu_mem > 45 else \"a100_40gb\"\n\n    if detected_tier and detected_tier != CONFIG[\"gpu_tier\"]:\n        print(f\"\\n  Auto-override: {CONFIG['gpu_tier']} -> {detected_tier}\")\n        CONFIG[\"gpu_tier\"] = detected_tier\n        tier = GPU_CONFIGS[detected_tier]\n        for k, v in tier.items():\n            CONFIG[k] = v\n        print(f\"  Updated CONFIG with {detected_tier} presets.\")\n\n    # Set MoE backend\n    os.environ[\"UNSLOTH_MOE_BACKEND\"] = CONFIG.get(\"moe_backend\", \"triton\")\n    print(f\"\\n  MoE backend: {os.environ['UNSLOTH_MOE_BACKEND']}\")\n    print(f\"  Load mode: {CONFIG['load_mode']}\")\n    print(f\"  Fast inference: {CONFIG.get('fast_inference', False)}\")\n\n    # FP8 detection\n    if CONFIG[\"load_mode\"] == \"fp8\":\n        try:\n            import transformer_engine\n            print(\"  FP8: transformer-engine available\")\n        except ImportError:\n            print(\"  FP8: transformer-engine not found, falling back to 4bit\")\n            CONFIG[\"load_mode\"] = \"4bit\"\nelse:\n    print(\"No GPU detected! Training will fail.\")\n    print(\"Enable GPU: Runtime -> Change runtime type -> GPU\")\n\nprint(f\"\\nFinal config: scope={CONFIG['training_scope']}, tier={CONFIG['gpu_tier']}\")"
  },
  {
   "cell_type": "markdown",
   "id": "9479da8417afda5e",
   "metadata": {},
   "source": "## Step 1: Data Preparation"
  },
  {
   "cell_type": "markdown",
   "id": "33b4d7c5bfaf7744",
   "metadata": {},
   "source": "### 1.1 Download Tool Calling Datasets\n\nDownloads and formats three tool/function calling datasets into Harmony format:\n- **Glaive v2** (113K): high-quality synthetic function calling conversations\n- **xLAM-60K** (60K): diverse function calling from Salesforce\n- **Hermes v1**: NousResearch curated function calling data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66623c5c43db762b",
   "metadata": {},
   "outputs": [],
   "source": "import os, json, sys\n\nsys.path.insert(0, \"scripts\")\nfrom dataset_formatters.function_calling import (\n    format_glaive_function_calling,\n    format_hermes_function_calling,\n)\nfrom dataset_formatters.harmony import encode_harmony_messages\n\nTOOL_CALLING_DATASETS = [\n    (\"glaiveai/glaive-function-calling-v2\", \"glaive\", 113000),\n    (\"Salesforce/xlam-function-calling-60k\", \"xlam\", 60000),\n    (\"NousResearch/hermes-function-calling-v1\", \"hermes\", None),\n]\n\n# ──────────────────────────────────────────────────────────────────────────────\n# xLAM inline formatter\n# xLAM format: {\"query\": str, \"tools\": str (JSON array), \"answers\": str (JSON array)}\n# Output: Harmony tool call format\n# ──────────────────────────────────────────────────────────────────────────────\ndef format_xlam_function_calling(example):\n    \"\"\"Format Salesforce xLAM function calling data into Harmony format.\n\n    xLAM schema:\n        query   - natural language user request\n        tools   - JSON-encoded list of tool schemas (name/description/parameters)\n        answers - JSON-encoded list of call dicts [{name: ..., arguments: {...}}]\n\n    Returns Harmony-encoded text with developer context, user query, and\n    one tool_call per answer entry in the assistant turn.\n    \"\"\"\n    query = example.get(\"query\", \"\").strip()\n    tools_raw = example.get(\"tools\", \"[]\")\n    answers_raw = example.get(\"answers\", \"[]\")\n\n    if not query:\n        return {\"text\": \"\"}\n\n    try:\n        tools = json.loads(tools_raw) if isinstance(tools_raw, str) else tools_raw\n    except (json.JSONDecodeError, TypeError):\n        tools = []\n\n    try:\n        answers = json.loads(answers_raw) if isinstance(answers_raw, str) else answers_raw\n    except (json.JSONDecodeError, TypeError):\n        answers = []\n\n    if not answers:\n        return {\"text\": \"\"}\n\n    # Build tool schema description for developer context\n    tool_descriptions = []\n    for t in tools:\n        name = t.get(\"name\", \"unknown\")\n        desc = t.get(\"description\", \"\")\n        params = t.get(\"parameters\", {})\n        tool_descriptions.append(\n            f\"  {name}: {desc}\\n    Parameters: {json.dumps(params, separators=(',', ':'))}\"\n        )\n    tool_ctx = \"\\n\".join(tool_descriptions) if tool_descriptions else \"No tools defined.\"\n\n    dev_instructions = (\n        \"You are a helpful assistant with access to tools. \"\n        \"Call the appropriate tool with valid parameters based on the user's request.\\n\\n\"\n        f\"Available tools:\\n{tool_ctx}\"\n    )\n\n    # Build tool_calls list from answers\n    tool_calls = []\n    for i, ans in enumerate(answers):\n        tc_name = ans.get(\"name\", \"\")\n        tc_args = ans.get(\"arguments\", ans.get(\"parameters\", {}))\n        if not tc_name:\n            continue\n        tool_calls.append({\n            \"id\": f\"call_{i}\",\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": tc_name,\n                \"arguments\": json.dumps(tc_args) if isinstance(tc_args, dict) else str(tc_args),\n            },\n        })\n\n    if not tool_calls:\n        return {\"text\": \"\"}\n\n    messages = [\n        {\"role\": \"user\", \"content\": query},\n        {\"role\": \"assistant\", \"tool_calls\": tool_calls},\n    ]\n\n    return {\"text\": encode_harmony_messages(messages, developer_instructions=dev_instructions)}\n\n\nif CONFIG[\"skip_data_generation\"] or not CONFIG[\"include_tool_calling\"]:\n    print(\"Skipping tool calling data download.\")\n    tracker.skip(\"tool_calling_data\")\nelse:\n    tracker.start(\"tool_calling_data\")\n    from datasets import load_dataset, Dataset, concatenate_datasets\n\n    all_tool_calling = []\n    stats = {}\n\n    for ds_name, ds_key, max_samples in TOOL_CALLING_DATASETS:\n        print(f\"\\nDownloading {ds_name}...\")\n        try:\n            if ds_key == \"glaive\":\n                raw = load_dataset(ds_name, split=\"train\")\n                if CONFIG[\"training_scope\"] == \"quick_test\":\n                    raw = raw.select(range(min(500, len(raw))))\n                elif max_samples:\n                    raw = raw.select(range(min(max_samples, len(raw))))\n                formatted = raw.map(format_glaive_function_calling, remove_columns=raw.column_names)\n\n            elif ds_key == \"xlam\":\n                raw = load_dataset(ds_name, split=\"train\")\n                if CONFIG[\"training_scope\"] == \"quick_test\":\n                    raw = raw.select(range(min(200, len(raw))))\n                elif max_samples:\n                    raw = raw.select(range(min(max_samples, len(raw))))\n                formatted = raw.map(format_xlam_function_calling, remove_columns=raw.column_names)\n\n            elif ds_key == \"hermes\":\n                raw = load_dataset(ds_name, split=\"train\")\n                if CONFIG[\"training_scope\"] == \"quick_test\":\n                    raw = raw.select(range(min(300, len(raw))))\n                formatted = raw.map(format_hermes_function_calling, remove_columns=raw.column_names)\n\n            else:\n                continue\n\n            # Filter empty examples\n            formatted = formatted.filter(lambda x: bool(x.get(\"text\", \"\").strip()))\n            stats[ds_key] = len(formatted)\n            all_tool_calling.append(formatted)\n            print(f\"  Formatted: {len(formatted):,} examples\")\n\n        except Exception as e:\n            print(f\"  WARNING: failed to load {ds_name}: {e}\")\n            stats[ds_key] = 0\n\n    if all_tool_calling:\n        combined = concatenate_datasets(all_tool_calling)\n        combined = combined.shuffle(seed=42)\n        out_path = \"data/coding_tui/tool_calling/train\"\n        combined.save_to_disk(out_path)\n        print(f\"\\nTotal tool calling examples: {len(combined):,} -> {out_path}\")\n    else:\n        print(\"WARNING: no tool calling data collected.\")\n\n    drive_helper.backup(\"data/coding_tui/tool_calling\", \"data/coding_tui/tool_calling\")\n    if DRIVE_MODE != \"local\":\n        print(\"Backed up to Drive.\")\n\n    tracker.complete(\"tool_calling_data\")"
  },
  {
   "cell_type": "markdown",
   "id": "d50f4da79bd9ec1a",
   "metadata": {},
   "source": "### 1.2 Download Agent Trajectory Datasets\n\nDownloads and formats agent trajectory datasets into Harmony agent format:\n- **code-act**: multi-step code execution agent trajectories\n- **commitpack**: commit-based code change examples\n- **editpackft**: instruction-following code edits"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1633fd3a9ccab688",
   "metadata": {},
   "outputs": [],
   "source": "import os, sys, json\n\nsys.path.insert(0, \"scripts\")\nfrom dataset_formatters.harmony import format_harmony_agent, encode_harmony_messages\n\nAGENT_TRAJECTORY_DATASETS = [\n    (\"xingyaoww/code-act\", \"code_act\", None),\n    (\"bigcode/commitpack\", \"commitpack\", 50000),\n    (\"bigcode/editpackft\", \"editpackft\", 50000),\n]\n\nCODING_AGENT_DEV_PROMPT = (\n    \"You are a coding agent. Use tools to read files, write code, run tests, and \"\n    \"complete programming tasks. Do not just analyze — always take action and produce \"\n    \"working code. After making changes, verify they work by running the relevant tests. \"\n    \"If a tool call fails, diagnose and retry with corrected parameters.\"\n)\n\n\ndef format_code_act_example(example):\n    \"\"\"Format code-act dataset into Harmony agent trajectory format.\n\n    code-act format: conversations list with role/content turns, where\n    tool calls appear as bash/python execution blocks in assistant content.\n    \"\"\"\n    conversations = example.get(\"conversations\", [])\n    if not conversations:\n        return {\"text\": \"\"}\n\n    messages = []\n    for turn in conversations:\n        role = turn.get(\"role\", turn.get(\"from\", \"\"))\n        content = turn.get(\"content\", turn.get(\"value\", \"\"))\n        if not content:\n            continue\n        if role in [\"human\", \"user\"]:\n            messages.append({\"role\": \"user\", \"content\": content})\n        elif role in [\"gpt\", \"assistant\"]:\n            messages.append({\"role\": \"assistant\", \"content\": content})\n        elif role in [\"tool\", \"function\", \"observation\"]:\n            messages.append({\"role\": \"tool\", \"content\": content})\n\n    if len(messages) < 2:\n        return {\"text\": \"\"}\n\n    return {\"text\": encode_harmony_messages(\n        messages,\n        developer_instructions=CODING_AGENT_DEV_PROMPT,\n        reasoning_effort=\"high\",\n    )}\n\n\ndef format_commitpack_example(example):\n    \"\"\"Format commitpack (code change from commit message) into Harmony format.\n\n    commitpack format: {subject, message, old_contents, new_contents, lang}\n    Task: given old code + commit message, produce the new code.\n    \"\"\"\n    subject = example.get(\"subject\", \"\")\n    message = example.get(\"message\", subject)\n    old_code = example.get(\"old_contents\", \"\")\n    new_code = example.get(\"new_contents\", \"\")\n    lang = example.get(\"lang\", \"\")\n\n    if not old_code or not new_code or not message:\n        return {\"text\": \"\"}\n\n    # Skip trivially identical or very large examples\n    if old_code.strip() == new_code.strip():\n        return {\"text\": \"\"}\n    if len(old_code) > 8000 or len(new_code) > 8000:\n        return {\"text\": \"\"}\n\n    fence = lang.lower() if lang else \"\"\n    user_content = (\n        f\"Apply the following change to the code:\\n\\n\"\n        f\"Commit message: {message}\\n\\n\"\n        f\"Current code:\\n```{fence}\\n{old_code}\\n```\"\n    )\n    assistant_content = f\"```{fence}\\n{new_code}\\n```\"\n\n    messages = [\n        {\"role\": \"user\", \"content\": user_content},\n        {\"role\": \"assistant\", \"content\": assistant_content},\n    ]\n\n    return {\"text\": encode_harmony_messages(\n        messages,\n        developer_instructions=CODING_AGENT_DEV_PROMPT,\n        reasoning_effort=\"medium\",\n    )}\n\n\ndef format_editpackft_example(example):\n    \"\"\"Format editpackft (instruction-following code edits) into Harmony format.\n\n    editpackft format: {instruction, old_code, new_code, lang}\n    Task: given code + instruction, apply the edit.\n    \"\"\"\n    instruction = example.get(\"instruction\", \"\")\n    old_code = example.get(\"old_code\", example.get(\"input\", \"\"))\n    new_code = example.get(\"new_code\", example.get(\"output\", \"\"))\n    lang = example.get(\"lang\", \"\")\n\n    if not instruction or not old_code or not new_code:\n        return {\"text\": \"\"}\n    if old_code.strip() == new_code.strip():\n        return {\"text\": \"\"}\n    if len(old_code) > 8000 or len(new_code) > 8000:\n        return {\"text\": \"\"}\n\n    fence = lang.lower() if lang else \"\"\n    user_content = (\n        f\"{instruction}\\n\\n\"\n        f\"Code:\\n```{fence}\\n{old_code}\\n```\"\n    )\n    assistant_content = f\"```{fence}\\n{new_code}\\n```\"\n\n    messages = [\n        {\"role\": \"user\", \"content\": user_content},\n        {\"role\": \"assistant\", \"content\": assistant_content},\n    ]\n\n    return {\"text\": encode_harmony_messages(\n        messages,\n        developer_instructions=CODING_AGENT_DEV_PROMPT,\n        reasoning_effort=\"medium\",\n    )}\n\n\nif CONFIG[\"skip_data_generation\"] or not CONFIG[\"include_agent_trajectories\"]:\n    print(\"Skipping agent trajectory data download.\")\n    tracker.skip(\"agent_traj_data\")\nelse:\n    tracker.start(\"agent_traj_data\")\n    from datasets import load_dataset, Dataset, concatenate_datasets\n\n    all_agent_traj = []\n\n    for ds_name, ds_key, max_samples in AGENT_TRAJECTORY_DATASETS:\n        print(f\"\\nDownloading {ds_name}...\")\n        try:\n            if ds_key == \"code_act\":\n                raw = load_dataset(ds_name, split=\"train\")\n                if CONFIG[\"training_scope\"] == \"quick_test\":\n                    raw = raw.select(range(min(200, len(raw))))\n                elif max_samples:\n                    raw = raw.select(range(min(max_samples, len(raw))))\n                formatted = raw.map(format_code_act_example, remove_columns=raw.column_names)\n\n            elif ds_key == \"commitpack\":\n                # commitpack has many language subsets; load the 'all' config or default\n                try:\n                    raw = load_dataset(ds_name, \"all\", split=\"train\")\n                except Exception:\n                    raw = load_dataset(ds_name, split=\"train\")\n                if CONFIG[\"training_scope\"] == \"quick_test\":\n                    raw = raw.select(range(min(300, len(raw))))\n                elif max_samples:\n                    raw = raw.select(range(min(max_samples, len(raw))))\n                formatted = raw.map(format_commitpack_example, remove_columns=raw.column_names)\n\n            elif ds_key == \"editpackft\":\n                raw = load_dataset(ds_name, split=\"train\")\n                if CONFIG[\"training_scope\"] == \"quick_test\":\n                    raw = raw.select(range(min(300, len(raw))))\n                elif max_samples:\n                    raw = raw.select(range(min(max_samples, len(raw))))\n                formatted = raw.map(format_editpackft_example, remove_columns=raw.column_names)\n\n            else:\n                continue\n\n            formatted = formatted.filter(lambda x: bool(x.get(\"text\", \"\").strip()))\n            all_agent_traj.append(formatted)\n            print(f\"  Formatted: {len(formatted):,} examples\")\n\n        except Exception as e:\n            print(f\"  WARNING: failed to load {ds_name}: {e}\")\n\n    if all_agent_traj:\n        combined = concatenate_datasets(all_agent_traj)\n        combined = combined.shuffle(seed=42)\n        out_path = \"data/coding_tui/agent_traj/train\"\n        combined.save_to_disk(out_path)\n        print(f\"\\nTotal agent trajectory examples: {len(combined):,} -> {out_path}\")\n    else:\n        print(\"WARNING: no agent trajectory data collected.\")\n\n    drive_helper.backup(\"data/coding_tui/agent_traj\", \"data/coding_tui/agent_traj\")\n    if DRIVE_MODE != \"local\":\n        print(\"Backed up to Drive.\")\n\n    tracker.complete(\"agent_traj_data\")"
  },
  {
   "cell_type": "markdown",
   "id": "1b115a2a45b5208d",
   "metadata": {},
   "source": "### 1.3 Download Preference Datasets\n\nDownloads and formats preference datasets for IPO training.\nGood responses (task completed, decisive action) are paired against bad ones (circular analysis, incomplete)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c0ba32cdc76d69",
   "metadata": {},
   "outputs": [],
   "source": "import os, sys, json\n\nsys.path.insert(0, \"scripts\")\nfrom dataset_formatters.harmony import format_harmony_preference, encode_harmony_messages\n\nPREFERENCE_DATASETS = [\n    (\"Anthropic/hh-rlhf\", \"hh_rlhf\", None),\n    (\"m-a-p/CodeFeedback-Filtered-Instruction\", \"code_feedback\", None),\n]\n\nCODING_PREF_DEV = \"You are a helpful coding assistant. Provide complete, working code solutions.\"\n\n\ndef format_hh_rlhf_example(example):\n    \"\"\"Format Anthropic HH-RLHF into Harmony preference pairs.\n\n    hh-rlhf format: {chosen: str, rejected: str}\n    Both are full conversation strings with \\\\nHuman: / \\\\nAssistant: turns.\n    \"\"\"\n    chosen_raw = example.get(\"chosen\", \"\")\n    rejected_raw = example.get(\"rejected\", \"\")\n\n    if not chosen_raw or not rejected_raw:\n        return {\"text\": \"\"}\n\n    def parse_conversation(raw):\n        \"\"\"Parse Human/Assistant turn format into message list.\"\"\"\n        messages = []\n        # Split on role markers\n        import re\n        parts = re.split(r'\\n(Human|Assistant):\\s*', raw)\n        current_role = None\n        for part in parts:\n            part = part.strip()\n            if part == \"Human\":\n                current_role = \"user\"\n            elif part == \"Assistant\":\n                current_role = \"assistant\"\n            elif part and current_role:\n                messages.append({\"role\": current_role, \"content\": part})\n                current_role = None\n        return messages\n\n    chosen_msgs = parse_conversation(chosen_raw)\n    rejected_msgs = parse_conversation(rejected_raw)\n\n    if not chosen_msgs or not rejected_msgs:\n        return {\"text\": \"\"}\n\n    # Extract the shared prompt (all turns up to last assistant turn)\n    prompt_msgs = chosen_msgs[:-1] if chosen_msgs else []\n    chosen_content = chosen_msgs[-1].get(\"content\", \"\") if chosen_msgs else \"\"\n    rejected_content = rejected_msgs[-1].get(\"content\", \"\") if rejected_msgs else \"\"\n\n    if not chosen_content or not rejected_content or chosen_content == rejected_content:\n        return {\"text\": \"\"}\n\n    # Encode chosen and rejected with full context\n    chosen_full = encode_harmony_messages(\n        prompt_msgs + [{\"role\": \"assistant\", \"content\": chosen_content}],\n        developer_instructions=CODING_PREF_DEV,\n    )\n    rejected_full = encode_harmony_messages(\n        prompt_msgs + [{\"role\": \"assistant\", \"content\": rejected_content}],\n        developer_instructions=CODING_PREF_DEV,\n    )\n\n    prompt_text = \"\"\n    if prompt_msgs:\n        prompt_text = prompt_msgs[-1].get(\"content\", \"\")\n\n    return {\n        \"text\": chosen_full,\n        \"prompt\": prompt_text,\n        \"chosen\": chosen_full,\n        \"rejected\": rejected_full,\n    }\n\n\ndef format_code_feedback_example(example):\n    \"\"\"Format CodeFeedback-Filtered-Instruction into Harmony preference pairs.\n\n    Format: {query: str, answer: str} — high quality coding Q&A.\n    We use these as positive examples; we generate synthetic rejected responses\n    by truncating or slightly degrading the chosen answer.\n    \"\"\"\n    query = example.get(\"query\", example.get(\"instruction\", \"\")).strip()\n    answer = example.get(\"answer\", example.get(\"output\", \"\")).strip()\n\n    if not query or not answer or len(answer) < 100:\n        return {\"text\": \"\"}\n\n    # Synthetic rejected: truncate answer at 30% and add a non-committal ending\n    cutoff = max(50, int(len(answer) * 0.3))\n    rejected = answer[:cutoff] + \"\\n\\n(I would need to analyze this further before proceeding.)\"\n\n    return format_harmony_preference({\n        \"prompt\": query,\n        \"chosen\": answer,\n        \"rejected\": rejected,\n    })\n\n\nif CONFIG[\"skip_data_generation\"] or not CONFIG[\"include_preference\"]:\n    print(\"Skipping preference data download.\")\nelse:\n    from datasets import load_dataset, Dataset, concatenate_datasets\n\n    all_pref = []\n\n    for ds_name, ds_key, max_samples in PREFERENCE_DATASETS:\n        print(f\"\\nDownloading {ds_name}...\")\n        try:\n            if ds_key == \"hh_rlhf\":\n                raw = load_dataset(ds_name, split=\"train\")\n                if CONFIG[\"training_scope\"] == \"quick_test\":\n                    raw = raw.select(range(min(300, len(raw))))\n                elif max_samples:\n                    raw = raw.select(range(min(max_samples, len(raw))))\n                formatted = raw.map(format_hh_rlhf_example, remove_columns=raw.column_names)\n\n            elif ds_key == \"code_feedback\":\n                raw = load_dataset(ds_name, split=\"train\")\n                if CONFIG[\"training_scope\"] == \"quick_test\":\n                    raw = raw.select(range(min(200, len(raw))))\n                elif max_samples:\n                    raw = raw.select(range(min(max_samples, len(raw))))\n                formatted = raw.map(format_code_feedback_example, remove_columns=raw.column_names)\n\n            else:\n                continue\n\n            formatted = formatted.filter(lambda x: bool(x.get(\"text\", \"\").strip()))\n            all_pref.append(formatted)\n            print(f\"  Formatted: {len(formatted):,} examples\")\n\n        except Exception as e:\n            print(f\"  WARNING: failed to load {ds_name}: {e}\")\n\n    if all_pref:\n        combined = concatenate_datasets(all_pref)\n        combined = combined.shuffle(seed=42)\n        out_path = \"data/coding_tui/preference/train\"\n        combined.save_to_disk(out_path)\n        print(f\"\\nTotal preference examples: {len(combined):,} -> {out_path}\")\n    else:\n        print(\"WARNING: no preference data collected.\")\n\n    drive_helper.backup(\"data/coding_tui/preference\", \"data/coding_tui/preference\")\n    if DRIVE_MODE != \"local\":\n        print(\"Backed up to Drive.\")"
  },
  {
   "cell_type": "markdown",
   "id": "338ed52feaab5e9e",
   "metadata": {},
   "source": "### 1.4 Extract Training Data from Proxy Logs (Optional)\n\nScans the MacLean AI proxy log directory for real Codex CLI agent sessions.\nEach session is a JSON file written by `claude-proxy-v2` containing the full\nrequest/response. These are the highest-value training examples because they\nare real agent tasks, not synthetic data.\n\nAlso scans for `*_cot.txt` files (chain-of-thought logs from the streaming filter)\nand associates them with the corresponding requests for thinking data.\n\n**Set `proxy_log_dir` in Step 0.3 to enable.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037f5211b81de6f5",
   "metadata": {},
   "outputs": [],
   "source": "import os, sys, json, glob, re\nfrom pathlib import Path\n\nsys.path.insert(0, \"scripts\")\nfrom dataset_formatters.harmony import encode_harmony_messages\n\nCODING_AGENT_DEV_PROMPT = (\n    \"You are a coding agent. Use tools to read files, write code, run tests, and \"\n    \"complete programming tasks. Do not just analyze — always take action and produce \"\n    \"working code. After making changes, verify they work by running the relevant tests. \"\n    \"If a tool call fails, diagnose and retry with corrected parameters.\"\n)\n\n\ndef load_proxy_log(log_path):\n    \"\"\"Load and validate a single proxy log JSON file.\n\n    Returns the parsed dict if valid, None otherwise.\n    Expected fields: request_num, path, translated, original_request,\n                     response, input_tokens, output_tokens, latency_ms\n    \"\"\"\n    try:\n        with open(log_path) as f:\n            data = json.load(f)\n    except (json.JSONDecodeError, OSError):\n        return None\n\n    # Must have a response with output\n    if data.get(\"output_tokens\", 0) <= 0:\n        return None\n\n    resp = data.get(\"response\", {})\n    if not resp:\n        return None\n\n    return data\n\n\ndef extract_messages_from_request(req_data, cot_text=None):\n    \"\"\"Extract Harmony-ready messages from a proxy log entry.\n\n    Handles both:\n    - /v1/messages (Anthropic format, possibly translated)\n    - /v1/chat/completions (OpenAI format)\n\n    Args:\n        req_data: parsed proxy log dict\n        cot_text: optional chain-of-thought text from *_cot.txt sidecar\n\n    Returns list of message dicts suitable for encode_harmony_messages, or None.\n    \"\"\"\n    path = req_data.get(\"path\", \"\")\n    original_req = req_data.get(\"original_request\", {})\n    response = req_data.get(\"response\", {})\n\n    messages = []\n\n    if \"/messages\" in path:\n        # Anthropic Messages API format\n        sys_content = original_req.get(\"system\", \"\")\n        if isinstance(sys_content, list):\n            # system can be a list of content blocks\n            sys_text = \" \".join(\n                block.get(\"text\", \"\") for block in sys_content\n                if isinstance(block, dict) and block.get(\"type\") == \"text\"\n            )\n        else:\n            sys_text = str(sys_content) if sys_content else \"\"\n\n        raw_msgs = original_req.get(\"messages\", [])\n        for m in raw_msgs:\n            role = m.get(\"role\", \"\")\n            content = m.get(\"content\", \"\")\n            if isinstance(content, list):\n                # content can be a list of blocks (text, tool_use, tool_result)\n                text_parts = []\n                tool_calls_out = []\n                for block in content:\n                    btype = block.get(\"type\", \"\")\n                    if btype == \"text\":\n                        text_parts.append(block.get(\"text\", \"\"))\n                    elif btype == \"tool_use\":\n                        tool_calls_out.append({\n                            \"id\": block.get(\"id\", \"\"),\n                            \"type\": \"function\",\n                            \"function\": {\n                                \"name\": block.get(\"name\", \"\"),\n                                \"arguments\": json.dumps(block.get(\"input\", {})),\n                            },\n                        })\n                    elif btype == \"tool_result\":\n                        # tool results come back as user messages in Anthropic format\n                        result_content = block.get(\"content\", \"\")\n                        if isinstance(result_content, list):\n                            result_content = \" \".join(\n                                rb.get(\"text\", \"\") for rb in result_content\n                                if isinstance(rb, dict)\n                            )\n                        messages.append({\n                            \"role\": \"tool\",\n                            \"tool_call_id\": block.get(\"tool_use_id\", \"\"),\n                            \"content\": result_content,\n                        })\n\n                msg_entry = {\"role\": role}\n                if text_parts:\n                    msg_entry[\"content\"] = \"\\n\".join(text_parts)\n                if tool_calls_out:\n                    msg_entry[\"tool_calls\"] = tool_calls_out\n                if role == \"assistant\" and cot_text:\n                    msg_entry[\"thinking\"] = cot_text\n                    cot_text = None  # use only for the first assistant turn\n\n                if role not in (\"user\",) or text_parts:\n                    messages.append(msg_entry)\n            else:\n                entry = {\"role\": role, \"content\": str(content) if content else \"\"}\n                if role == \"assistant\" and cot_text:\n                    entry[\"thinking\"] = cot_text\n                    cot_text = None\n                messages.append(entry)\n\n        # Extract assistant response\n        resp_content = response.get(\"content\", [])\n        if isinstance(resp_content, list):\n            resp_text_parts = []\n            resp_tool_calls = []\n            for block in resp_content:\n                btype = block.get(\"type\", \"\")\n                if btype == \"text\":\n                    resp_text_parts.append(block.get(\"text\", \"\"))\n                elif btype == \"tool_use\":\n                    resp_tool_calls.append({\n                        \"id\": block.get(\"id\", \"\"),\n                        \"type\": \"function\",\n                        \"function\": {\n                            \"name\": block.get(\"name\", \"\"),\n                            \"arguments\": json.dumps(block.get(\"input\", {})),\n                        },\n                    })\n            resp_entry = {\"role\": \"assistant\"}\n            if resp_text_parts:\n                resp_entry[\"content\"] = \"\\n\".join(resp_text_parts)\n            if resp_tool_calls:\n                resp_entry[\"tool_calls\"] = resp_tool_calls\n            messages.append(resp_entry)\n        elif isinstance(resp_content, str) and resp_content:\n            messages.append({\"role\": \"assistant\", \"content\": resp_content})\n\n        dev_instructions = sys_text if sys_text else CODING_AGENT_DEV_PROMPT\n\n    elif \"/chat/completions\" in path:\n        # OpenAI Chat Completions format\n        raw_msgs = original_req.get(\"messages\", [])\n        dev_instructions = CODING_AGENT_DEV_PROMPT\n\n        for m in raw_msgs:\n            role = m.get(\"role\", \"\")\n            content = m.get(\"content\", \"\")\n            tool_calls = m.get(\"tool_calls\", [])\n            tool_call_id = m.get(\"tool_call_id\")\n\n            if role == \"system\":\n                dev_instructions = content\n                continue\n\n            entry = {\"role\": role}\n            if content:\n                entry[\"content\"] = content\n            if tool_calls:\n                entry[\"tool_calls\"] = tool_calls\n            if tool_call_id:\n                entry[\"tool_call_id\"] = tool_call_id\n            if role == \"assistant\" and cot_text:\n                entry[\"thinking\"] = cot_text\n                cot_text = None\n            messages.append(entry)\n\n        # Extract response from choices\n        choices = response.get(\"choices\", [])\n        if choices:\n            resp_msg = choices[0].get(\"message\", {})\n            resp_entry = {\"role\": \"assistant\"}\n            if resp_msg.get(\"content\"):\n                resp_entry[\"content\"] = resp_msg[\"content\"]\n            if resp_msg.get(\"tool_calls\"):\n                resp_entry[\"tool_calls\"] = resp_msg[\"tool_calls\"]\n            messages.append(resp_entry)\n    else:\n        return None, None\n\n    if len(messages) < 2:\n        return None, None\n\n    return messages, dev_instructions\n\n\ndef extract_proxy_log_trajectories(log_dir, max_samples=None, quick_test=False):\n    \"\"\"Scan proxy log directory and extract training examples.\n\n    Args:\n        log_dir: path to MacLean AI proxy log directory\n        max_samples: cap on number of examples to extract\n        quick_test: if True, stop after 20 examples\n\n    Returns list of Harmony-encoded text strings.\n    \"\"\"\n    log_dir = Path(log_dir)\n    if not log_dir.exists():\n        print(f\"  WARNING: proxy log dir not found: {log_dir}\")\n        return []\n\n    # Find all JSON log files (exclude *_cot.txt sidecars)\n    log_files = sorted(log_dir.glob(\"*.json\"))\n    if not log_files:\n        # Try subdirectories (MacLean AI organises logs by date)\n        log_files = sorted(log_dir.glob(\"**/*.json\"))\n\n    print(f\"  Found {len(log_files)} log files in {log_dir}\")\n\n    examples = []\n    skipped = 0\n\n    for log_path in log_files:\n        if quick_test and len(examples) >= 20:\n            break\n        if max_samples and len(examples) >= max_samples:\n            break\n\n        req_data = load_proxy_log(log_path)\n        if req_data is None:\n            skipped += 1\n            continue\n\n        # Check for CoT sidecar: same stem but _cot.txt suffix\n        cot_path = log_path.with_name(log_path.stem + \"_cot.txt\")\n        cot_text = None\n        if cot_path.exists():\n            try:\n                cot_text = cot_path.read_text(encoding=\"utf-8\").strip() or None\n            except OSError:\n                pass\n\n        messages, dev_instructions = extract_messages_from_request(req_data, cot_text)\n        if messages is None:\n            skipped += 1\n            continue\n\n        try:\n            text = encode_harmony_messages(\n                messages,\n                developer_instructions=dev_instructions,\n                reasoning_effort=\"high\",\n            )\n        except Exception as e:\n            skipped += 1\n            continue\n\n        if text and len(text.strip()) > 200:\n            examples.append({\"text\": text})\n\n    print(f\"  Extracted: {len(examples):,} valid examples ({skipped} skipped)\")\n    return examples\n\n\n# ── Run extraction ─────────────────────────────────────────────────────────────\nlog_dir = CONFIG.get(\"proxy_log_dir\", \"\").strip()\n\nif not CONFIG[\"include_proxy_logs\"] or not log_dir:\n    print(\"Proxy log extraction disabled (include_proxy_logs=False or proxy_log_dir not set).\")\n    print(\"To enable: set proxy_log_dir to the MacLean AI log directory in Step 0.3.\")\n    tracker.skip(\"proxy_log_extract\")\nelif CONFIG[\"skip_data_generation\"]:\n    print(\"Skipping proxy log extraction (skip_data_generation=True).\")\n    tracker.skip(\"proxy_log_extract\")\nelse:\n    tracker.start(\"proxy_log_extract\")\n\n    quick = CONFIG[\"training_scope\"] == \"quick_test\"\n    proxy_examples = extract_proxy_log_trajectories(\n        log_dir,\n        max_samples=5000 if not quick else None,\n        quick_test=quick,\n    )\n\n    if proxy_examples:\n        from datasets import Dataset, load_from_disk, concatenate_datasets\n\n        proxy_ds = Dataset.from_list(proxy_examples)\n        proxy_ds = proxy_ds.shuffle(seed=42)\n\n        # Save standalone proxy dataset\n        proxy_out = \"data/coding_tui/proxy_logs/train\"\n        proxy_ds.save_to_disk(proxy_out)\n        print(f\"\\nProxy log dataset: {len(proxy_ds):,} examples -> {proxy_out}\")\n\n        # Merge into agent_sft data (proxy logs are highest value)\n        agent_path = \"data/coding_tui/agent_traj/train\"\n        if os.path.exists(agent_path):\n            base_ds = load_from_disk(agent_path)\n            merged = concatenate_datasets([base_ds, proxy_ds])\n            merged = merged.shuffle(seed=42)\n            merged.save_to_disk(agent_path)\n            print(f\"Merged into agent_traj/train: {len(merged):,} total\")\n        else:\n            proxy_ds.save_to_disk(agent_path)\n            print(f\"Saved as agent_traj/train: {len(proxy_ds):,} examples\")\n\n        drive_helper.backup(\"data/coding_tui/proxy_logs\", \"data/coding_tui/proxy_logs\")\n        if DRIVE_MODE != \"local\":\n            print(\"Backed up to Drive.\")\n\n        tracker.complete(\"proxy_log_extract\")\n    else:\n        print(\"No valid proxy log examples extracted.\")\n        tracker.fail(\"proxy_log_extract\")"
  },
  {
   "cell_type": "markdown",
   "id": "97c6483566d1285b",
   "metadata": {},
   "source": "### 1.5 Verify Data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc558ca9eef76a0",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\ndata_checks = [\n    (\"Tool Calling train\", \"data/coding_tui/tool_calling/train\"),\n    (\"Agent Trajectory train\", \"data/coding_tui/agent_traj/train\"),\n    (\"Proxy Log train\", \"data/coding_tui/proxy_logs/train\"),\n    (\"Preference train\", \"data/coding_tui/preference/train\"),\n]\n\nprint(\"Data Verification Summary\")\nprint(\"=\" * 60)\nprint(f\"  {'Dataset':<30} {'Examples':>12}\")\nprint(\"-\" * 60)\n\ntotal = 0\nfor name, path in data_checks:\n    if os.path.exists(path):\n        try:\n            from datasets import load_from_disk\n            ds = load_from_disk(path)\n            count = len(ds)\n            total += count\n            print(f\"  {name:<30} {count:>12,}\")\n        except Exception as e:\n            print(f\"  {name:<30} {'ERROR':>12}  ({e})\")\n    else:\n        print(f\"  {name:<30} {'not found':>12}\")\n\nprint(\"-\" * 60)\nprint(f\"  {'TOTAL':<30} {total:>12,}\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "id": "1ff172aa0151dbae",
   "metadata": {},
   "source": "## Step 2: Tool Calling SFT (Phase 1)\n\nTrain a LoRA adapter (rank 64) focused on correct tool/function calling.\n\n**Goals:**\n- Learn parameter accuracy for tool calls\n- Learn when NOT to call tools\n- Learn valid tool schemas and JSON formatting\n- Reduce hallucinated MCP server calls\n\nLow rank (64) to avoid catastrophic forgetting of general coding ability."
  },
  {
   "cell_type": "markdown",
   "id": "c95bdc5ffa260f7b",
   "metadata": {},
   "source": "### 2.1 Train Tool Calling Adapter"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2148b159c8b1f86b",
   "metadata": {},
   "outputs": [],
   "source": "if CONFIG[\"training_scope\"] in (\"skip_to_rl\",):\n    print(f\"Skipping — scope is {CONFIG['training_scope']}\")\n    tracker.skip(\"tool_calling_sft\")\nelif not CONFIG[\"include_tool_calling\"]:\n    print(\"Skipping — include_tool_calling=False\")\n    tracker.skip(\"tool_calling_sft\")\nelse:\n    tracker.start(\"tool_calling_sft\")\n\n    batch = CONFIG[\"tool_calling_batch\"]\n    grad_accum = CONFIG[\"tool_calling_grad_accum\"]\n    max_steps = CONFIG[\"tool_calling_max_steps\"]\n    seq_len = CONFIG[\"tool_calling_seq_len\"]\n\n    cmd = \"python scripts/13_train_lang_adapter.py\"\n    cmd += \" --train_data_path data/coding_tui/tool_calling/train\"\n    cmd += f\" --per_device_train_batch_size {batch}\"\n    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n    cmd += f\" --max_steps {max_steps}\"\n    cmd += f\" --output_dir checkpoints/tool_calling_sft\"\n    cmd += \" --lora_rank 64\"\n    cmd += \" --developer_prompt 'You are a coding agent with tool access. Call tools accurately with valid parameters. Never call tools that do not exist.'\"\n\n    print(\"Training tool calling SFT adapter...\")\n    print(f\"  Data:     data/coding_tui/tool_calling/train\")\n    print(f\"  Batch:    {batch} x {grad_accum} = {batch * grad_accum}\")\n    print(f\"  Steps:    {max_steps}\")\n    print(f\"  Seq len:  {seq_len}\")\n    print(f\"  LoRA rank: 64 (low rank to preserve general ability)\")\n    print(f\"  MoE backend: {CONFIG['moe_backend']}\")\n    print(\"=\" * 60)\n\n    !{cmd}\n\n    drive_helper.backup(\"checkpoints/tool_calling_sft\", \"checkpoints/tool_calling_sft\")\n    if DRIVE_MODE != \"local\":\n        print(\"\\nCheckpoint backed up to Drive.\")\n\n    tracker.complete(\"tool_calling_sft\")"
  },
  {
   "cell_type": "markdown",
   "id": "a4b11bbeb2214c94",
   "metadata": {},
   "source": "### 2.2 Merge Tool Calling Adapter into Base"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb17eece05e3e412",
   "metadata": {},
   "outputs": [],
   "source": "if CONFIG[\"training_scope\"] in (\"skip_to_rl\",):\n    print(f\"Skipping — scope is {CONFIG['training_scope']}\")\n    tracker.skip(\"merge\")\nelif not CONFIG[\"include_tool_calling\"]:\n    print(\"Skipping — include_tool_calling=False\")\n    tracker.skip(\"merge\")\nelse:\n    tracker.start(\"merge\")\n\n    print(\"Merging tool calling adapter into base model...\")\n    print(\"=\" * 60)\n\n    !python scripts/19_merge_adapter.py \\\n        --adapter_path checkpoints/tool_calling_sft/final \\\n        --output_dir checkpoints/gpt-oss-20b-coding-tui-merged\n\n    drive_helper.backup(\n        \"checkpoints/gpt-oss-20b-coding-tui-merged\",\n        \"checkpoints/gpt-oss-20b-coding-tui-merged\",\n    )\n    if DRIVE_MODE != \"local\":\n        print(\"\\nMerged model backed up to Drive.\")\n\n    tracker.complete(\"merge\")"
  },
  {
   "cell_type": "markdown",
   "id": "a6bae2458cdb0003",
   "metadata": {},
   "source": "### 2.3 Verify Merge"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72dc4ead2938c5e",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nif CONFIG[\"training_scope\"] not in (\"skip_to_rl\",) and CONFIG[\"include_tool_calling\"]:\n    merged_path = \"checkpoints/gpt-oss-20b-coding-tui-merged\"\n\n    print(\"Merge Verification:\")\n    print(\"=\" * 60)\n\n    if os.path.exists(merged_path):\n        files = os.listdir(merged_path)\n        total_size = sum(\n            os.path.getsize(os.path.join(merged_path, f))\n            for f in files if os.path.isfile(os.path.join(merged_path, f))\n        )\n        print(f\"  ✓ Merged model: {merged_path}\")\n        print(f\"    Files: {len(files)}\")\n        print(f\"    Total size: {total_size / (1024**3):.1f} GB\")\n    else:\n        print(f\"  ✗ Merged model not found at {merged_path}\")\n\n    print(\"=\" * 60)\nelse:\n    print(\"Merge skipped for this training scope.\")"
  },
  {
   "cell_type": "markdown",
   "id": "5e6c9cb29f5905e5",
   "metadata": {},
   "source": "## Step 3: Agent SFT (Phase 2)\n\nTrain a higher-rank LoRA (rank 128) on agent trajectories using the merged\ntool-calling model as the base.\n\n**Data includes:**\n- Multi-turn code agent sessions (code-act, commitpack, editpackft)\n- Real proxy log trajectories from live Codex CLI sessions (most valuable)\n\n**Goals:**\n- Learn complete read → plan → edit → verify cycles\n- Learn to actually write code after planning\n- Learn context tracking across long sessions"
  },
  {
   "cell_type": "markdown",
   "id": "84ca073beddf1cbd",
   "metadata": {},
   "source": "### 3.1 Train Agent SFT Adapter"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4b40b52dd555e",
   "metadata": {},
   "outputs": [],
   "source": "if CONFIG[\"training_scope\"] in (\"tool_calling_only\", \"skip_to_rl\"):\n    print(f\"Skipping — scope is {CONFIG['training_scope']}\")\n    tracker.skip(\"agent_sft\")\nelif not CONFIG[\"include_agent_trajectories\"]:\n    print(\"Skipping — include_agent_trajectories=False\")\n    tracker.skip(\"agent_sft\")\nelse:\n    tracker.start(\"agent_sft\")\n\n    # Use merged model as base if available, otherwise base model\n    import os\n    if os.path.exists(\"checkpoints/gpt-oss-20b-coding-tui-merged\"):\n        base_model = \"checkpoints/gpt-oss-20b-coding-tui-merged\"\n        print(\"Using merged tool-calling model as base.\")\n    else:\n        base_model = \"openai/gpt-oss-20b\"\n        print(\"Merged model not found, using original base model.\")\n\n    batch = CONFIG[\"agent_sft_batch\"]\n    grad_accum = CONFIG[\"agent_sft_grad_accum\"]\n    max_steps = CONFIG[\"agent_sft_max_steps\"]\n    seq_len = CONFIG[\"agent_sft_seq_len\"]\n\n    cmd = \"python scripts/14_train_core_agent.py\"\n    cmd += \" --train_data_path data/coding_tui/agent_traj/train\"\n    cmd += f\" --base_model_path {base_model}\"\n    cmd += f\" --per_device_train_batch_size {batch}\"\n    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n    cmd += f\" --max_steps {max_steps}\"\n    cmd += f\" --output_dir checkpoints/agent_sft\"\n    cmd += \" --lora_rank 128\"\n    cmd += \" --developer_prompt 'You are a coding agent. Use tools to read files, write code, run tests, and complete programming tasks. Do not just analyze — always take action and produce working code.'\"\n\n    print(\"Training agent SFT adapter (rank 128)...\")\n    print(f\"  Base:     {base_model}\")\n    print(f\"  Data:     data/coding_tui/agent_traj/train\")\n    print(f\"  Batch:    {batch} x {grad_accum} = {batch * grad_accum}\")\n    print(f\"  Steps:    {max_steps}\")\n    print(f\"  Seq len:  {seq_len}\")\n    print(f\"  LoRA rank: 128\")\n    print(f\"  MoE backend: {CONFIG['moe_backend']}\")\n    print(f\"  Auto packing: enabled\")\n    print(\"=\" * 60)\n\n    !{cmd}\n\n    drive_helper.backup(\"checkpoints/agent_sft\", \"checkpoints/agent_sft\")\n    if DRIVE_MODE != \"local\":\n        print(\"\\nCheckpoint backed up to Drive.\")\n\n    tracker.complete(\"agent_sft\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9415b023dc2fc8e0",
   "metadata": {},
   "outputs": [],
   "source": "import os, json\n\nif CONFIG[\"training_scope\"] not in (\"tool_calling_only\", \"skip_to_rl\") and CONFIG[\"include_agent_trajectories\"]:\n    ckpt_path = \"checkpoints/agent_sft/final\"\n\n    print(\"Agent SFT Verification:\")\n    print(\"=\" * 60)\n\n    if os.path.exists(ckpt_path):\n        files = os.listdir(ckpt_path)\n        print(f\"  ✓ Checkpoint: {ckpt_path} ({len(files)} files)\")\n\n        adapter_config = os.path.join(ckpt_path, \"adapter_config.json\")\n        if os.path.exists(adapter_config):\n            with open(adapter_config) as f:\n                cfg = json.load(f)\n            print(f\"    LoRA rank:       {cfg.get('r', '?')}\")\n            print(f\"    Alpha:           {cfg.get('lora_alpha', '?')}\")\n            print(f\"    Target modules:  {cfg.get('target_modules', '?')}\")\n    else:\n        print(f\"  ✗ Checkpoint not found at {ckpt_path}\")\n\n    print(\"=\" * 60)\nelse:\n    print(\"Agent SFT skipped for this training scope.\")"
  },
  {
   "cell_type": "markdown",
   "id": "b45e28d85aa60666",
   "metadata": {},
   "source": "## Step 4: IPO Preference Optimisation (Phase 3)\n\nTrain with IPO on preference pairs targeting the key failure modes:\n\n**Good (chosen):** Task completed, code written, tests pass, decisive action\n**Bad (rejected):** Circular analysis, \"I would need to look at this more\", no code written, wrong tool params\n\nVery low learning rate (5e-7), 1 epoch to avoid collapse."
  },
  {
   "cell_type": "markdown",
   "id": "eb4e2d31d414dbb2",
   "metadata": {},
   "source": "### 4.1 Train with IPO"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c51fc5d0b7bbe1d",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nif CONFIG[\"training_scope\"] == \"tool_calling_only\":\n    print(\"Skipping — scope is tool_calling_only\")\n    tracker.skip(\"ipo\")\nelif not CONFIG[\"include_preference\"]:\n    print(\"Skipping — include_preference=False\")\n    tracker.skip(\"ipo\")\nelse:\n    tracker.start(\"ipo\")\n\n    batch = CONFIG[\"ipo_batch\"]\n    grad_accum = CONFIG[\"ipo_grad_accum\"]\n    max_steps = CONFIG[\"ipo_max_steps\"]\n\n    # Determine best checkpoint to train from\n    if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n        ipo_base = \"checkpoints/agent_sft/final\"\n        print(\"skip_to_rl: starting IPO from agent_sft checkpoint\")\n    elif os.path.exists(\"checkpoints/agent_sft/final\"):\n        ipo_base = \"checkpoints/agent_sft/final\"\n    else:\n        ipo_base = \"checkpoints/tool_calling_sft/final\"\n        print(\"agent_sft not found, falling back to tool_calling_sft\")\n\n    # Check data exists\n    pref_path = \"data/coding_tui/preference/train\"\n    if not os.path.exists(pref_path):\n        print(f\"WARNING: preference data not found at {pref_path}\")\n        print(\"Run Step 1.3 first.\")\n        tracker.fail(\"ipo\")\n    else:\n        cmd = \"python scripts/17_ipo_preference.py\"\n        cmd += f\" --checkpoint {ipo_base}\"\n        cmd += f\" --train_data_path {pref_path}\"\n        cmd += f\" --per_device_train_batch_size {batch}\"\n        cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n        cmd += f\" --max_steps {max_steps}\"\n        cmd += \" --output_dir checkpoints/agent_sft_ipo\"\n        cmd += \" --beta 0.1\"\n\n        print(\"Training with IPO (preference optimisation)...\")\n        print(f\"  Base checkpoint: {ipo_base}\")\n        print(f\"  Data:            {pref_path}\")\n        print(f\"  Batch:           {batch} x {grad_accum} = {batch * grad_accum}\")\n        print(f\"  Steps:           {max_steps}\")\n        print(f\"  Loss:            IPO (beta=0.1)\")\n        print(f\"  Load mode:       {CONFIG['load_mode']}\")\n        print(f\"  MoE backend:     {CONFIG['moe_backend']}\")\n        print(\"=\" * 60)\n\n        !{cmd}\n\n        drive_helper.backup(\"checkpoints/agent_sft_ipo\", \"checkpoints/agent_sft_ipo\")\n        if DRIVE_MODE != \"local\":\n            print(\"\\nCheckpoint backed up to Drive.\")\n\n        tracker.complete(\"ipo\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfecb5aa3fbd0ec4",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nif CONFIG[\"training_scope\"] not in (\"tool_calling_only\",) and CONFIG[\"include_preference\"]:\n    ckpt_path = \"checkpoints/agent_sft_ipo/final\"\n\n    print(\"IPO Verification:\")\n    print(\"=\" * 60)\n\n    if os.path.exists(ckpt_path):\n        files = os.listdir(ckpt_path)\n        print(f\"  ✓ IPO checkpoint: {ckpt_path} ({len(files)} files)\")\n    else:\n        print(f\"  ✗ IPO checkpoint not found at {ckpt_path}\")\n\n    # Check TensorBoard logs for KL divergence\n    import glob\n    tb_files = glob.glob(\"checkpoints/agent_sft_ipo/**/events.out.tfevents*\", recursive=True)\n    if tb_files:\n        print(f\"  ✓ TensorBoard logs: {len(tb_files)} event files\")\n        print(\"    Monitor KL divergence: warn >0.3, abort >0.5\")\n    else:\n        print(\"  — No TensorBoard logs found\")\n\n    print(\"=\" * 60)\nelse:\n    print(\"IPO skipped for this training scope.\")"
  },
  {
   "cell_type": "markdown",
   "id": "ec5ecf10ad9111a3",
   "metadata": {},
   "source": "## Step 5: GRPO RL (Phase 4)\n\nExecution-grounded RL with Codex-style evaluation.\n\n**Reward function:**\n- `+1.0` for code that compiles / passes syntax check\n- `+2.0` for passing test cases\n- `+0.5` for clean linting (no obvious errors)\n- `-1.0` for circular/no-action responses (no code written)\n- `-0.5` for tool calls with malformed JSON parameters\n\n**Goals:**\n- Reinforce follow-through and complete code generation\n- Penalise looping analysis without action\n- Reinforce correct tool parameter formatting\n\n**Optimisations:**\n- FP8 RL with vLLM inference on H100 (1.6x throughput)\n- Chunked batching for longer context\n- Harmony format compliance reward"
  },
  {
   "cell_type": "markdown",
   "id": "14cbc298778aeb40",
   "metadata": {},
   "source": "### 5.1 Train with GRPO"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be37e6df60bcc5d",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nif CONFIG[\"training_scope\"] == \"tool_calling_only\":\n    print(\"Skipping — scope is tool_calling_only\")\n    tracker.skip(\"grpo\")\nelif not CONFIG[\"include_grpo\"]:\n    print(\"Skipping — include_grpo=False\")\n    tracker.skip(\"grpo\")\nelse:\n    tracker.start(\"grpo\")\n\n    batch = CONFIG[\"grpo_batch\"]\n    grad_accum = CONFIG[\"grpo_grad_accum\"]\n    max_steps = CONFIG[\"grpo_max_steps\"]\n    max_seq = CONFIG[\"grpo_seq_len\"]\n    num_gen = CONFIG[\"grpo_num_gen\"]\n\n    # Determine best checkpoint\n    if os.path.exists(\"checkpoints/agent_sft_ipo/final\"):\n        grpo_base = \"checkpoints/agent_sft_ipo/final\"\n    elif os.path.exists(\"checkpoints/agent_sft/final\"):\n        grpo_base = \"checkpoints/agent_sft/final\"\n        print(\"IPO checkpoint not found, using agent_sft.\")\n    elif os.path.exists(\"checkpoints/tool_calling_sft/final\"):\n        grpo_base = \"checkpoints/tool_calling_sft/final\"\n        print(\"agent_sft not found, using tool_calling_sft.\")\n    else:\n        grpo_base = \"openai/gpt-oss-20b\"\n        print(\"No fine-tuned checkpoint found, using base model.\")\n\n    cmd = \"python scripts/18_grpo_rl.py\"\n    cmd += f\" --checkpoint {grpo_base}\"\n    cmd += f\" --per_device_train_batch_size {batch}\"\n    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n    cmd += f\" --max_steps {max_steps}\"\n    cmd += f\" --num_generations {num_gen}\"\n    cmd += \" --output_dir checkpoints/agent_sft_grpo\"\n    cmd += \" --reward_mode coding_tui\"\n    cmd += \" --developer_prompt 'You are a coding agent. Use tools to read files, write code, run tests, and complete programming tasks. Do not just analyze — always take action and produce working code.'\"\n\n    v4_features = [f\"Split LoRA ({CONFIG['moe_backend']})\"]\n    if CONFIG[\"load_mode\"] == \"fp8\":\n        v4_features.append(\"FP8 weights\")\n    if CONFIG.get(\"fast_inference\"):\n        v4_features.append(\"vLLM inference\")\n    v4_features += [\"Chunked batching (auto)\", \"Auto packing\"]\n\n    if CONFIG[\"gpu_tier\"] == \"a100_40gb\":\n        print(\"NOTE: 40GB GPU — GRPO sequence length capped at 16384\")\n\n    print(\"Training with GRPO (execution-grounded RL)...\")\n    print(f\"  Base:          {grpo_base}\")\n    print(f\"  Batch:         {batch} x {grad_accum} = {batch * grad_accum}\")\n    print(f\"  Steps:         {max_steps}\")\n    print(f\"  Seq length:    {max_seq}\")\n    print(f\"  Generations:   {num_gen} per prompt\")\n    print()\n    print(\"  Reward signals:\")\n    print(\"    +1.0 code compiles / passes syntax check\")\n    print(\"    +2.0 test cases pass\")\n    print(\"    +0.5 clean linting\")\n    print(\"    -1.0 circular/no-action response\")\n    print(\"    -0.5 malformed tool call JSON\")\n    print()\n    print(\"  Active features:\")\n    for feat in v4_features:\n        print(f\"    ✓ {feat}\")\n    print(\"=\" * 60)\n\n    !{cmd}\n\n    drive_helper.backup(\"checkpoints/agent_sft_grpo\", \"checkpoints/agent_sft_grpo\")\n    if DRIVE_MODE != \"local\":\n        print(\"\\nCheckpoint backed up to Drive.\")\n\n    tracker.complete(\"grpo\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ba749f7a50ffec",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nif CONFIG[\"training_scope\"] not in (\"tool_calling_only\",) and CONFIG[\"include_grpo\"]:\n    ckpt_path = \"checkpoints/agent_sft_grpo/final\"\n\n    print(\"GRPO Verification:\")\n    print(\"=\" * 60)\n\n    if os.path.exists(ckpt_path):\n        files = os.listdir(ckpt_path)\n        print(f\"  ✓ GRPO checkpoint: {ckpt_path} ({len(files)} files)\")\n    else:\n        print(f\"  ✗ GRPO checkpoint not found at {ckpt_path}\")\n\n    print(\"=\" * 60)\nelse:\n    print(\"GRPO skipped for this training scope.\")"
  },
  {
   "cell_type": "markdown",
   "id": "5ceb3aeb8e91d6e7",
   "metadata": {},
   "source": "## Step 6: Evaluation\n\nEvaluate on coding agent tasks targeting the four failure modes:\n1. **Tool call format accuracy** — JSON schema compliance, valid tool names\n2. **Task completion rate** — did it actually produce a code change?\n3. **Circular detection rate** — does it loop the same analysis?\n4. **Code correctness** — compiles, passes tests"
  },
  {
   "cell_type": "markdown",
   "id": "f4c6a7fde4469632",
   "metadata": {},
   "source": "### 6.1 Run Coding Agent Evaluation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5016ed481527e8",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nif CONFIG[\"training_scope\"] == \"tool_calling_only\":\n    # Still run reduced eval\n    pass\n\ntracker.start(\"eval\")\n\n# Determine best checkpoint\nCHECKPOINT_PRIORITY = [\n    \"checkpoints/agent_sft_grpo/final\",\n    \"checkpoints/agent_sft_ipo/final\",\n    \"checkpoints/agent_sft/final\",\n    \"checkpoints/tool_calling_sft/final\",\n]\n\neval_checkpoint = None\nfor path in CHECKPOINT_PRIORITY:\n    if os.path.exists(path):\n        eval_checkpoint = path\n        break\n\nif eval_checkpoint is None:\n    print(\"✗ No checkpoint found. Train the model first.\")\n    tracker.fail(\"eval\")\nelse:\n    num_samples = CONFIG[\"eval_num_samples\"]\n\n    print(f\"Evaluating checkpoint: {eval_checkpoint}\")\n    print(f\"Samples: {num_samples}\")\n    print(\"=\" * 60)\n\n    !python scripts/eval_rust_agent.py \\\n        --checkpoint {eval_checkpoint} \\\n        --num_samples {num_samples} \\\n        --eval_mode coding_tui \\\n        --output_dir evals/coding_tui_agent\n\n    drive_helper.backup(\"evals/coding_tui_agent\", \"evals/coding_tui_agent\")\n    if DRIVE_MODE != \"local\":\n        print(\"\\nResults backed up to Drive.\")\n\n    tracker.complete(\"eval\")"
  },
  {
   "cell_type": "markdown",
   "id": "cffb6581ea3dfaf5",
   "metadata": {},
   "source": "### 6.2 Check Promotion Gates"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300ff1c11b78de01",
   "metadata": {},
   "outputs": [],
   "source": "!python scripts/12_check_gates.py coding_tui_agent"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbffa5f78567ba1",
   "metadata": {},
   "outputs": [],
   "source": "import os, json\n\nmetrics_path = \"evals/coding_tui_agent/metrics.json\"\n\nif os.path.exists(metrics_path):\n    with open(metrics_path) as f:\n        metrics = json.load(f)\n\n    # Targets tuned for a coding TUI agent\n    targets = {\n        \"tool_call_format_accuracy\": (0.95, \"higher\"),   # Valid JSON + known tool names\n        \"task_completion_rate\": (0.70, \"higher\"),         # Actually wrote/modified code\n        \"circular_detection_rate\": (0.10, \"lower\"),       # Loop rate should be low\n        \"code_correctness_rate\": (0.65, \"higher\"),        # Compiles / passes tests\n        \"follow_through_rate\": (0.80, \"higher\"),          # Takes action after planning\n    }\n\n    print(\"=\" * 62)\n    print(\"EVALUATION RESULTS — Coding TUI Agent\")\n    print(\"=\" * 62)\n    print(f\"  {'Metric':<34} {'Value':>8} {'Target':>8} {'Status':>8}\")\n    print(\"-\" * 62)\n\n    all_pass = True\n    for key, (target, direction) in targets.items():\n        value = metrics.get(key)\n        if value is None:\n            print(f\"  {key:<34} {'N/A':>8} {target:>8} {'—':>8}\")\n            continue\n\n        if direction == \"higher\":\n            passed = value >= target\n        else:\n            passed = value <= target\n\n        if not passed:\n            all_pass = False\n\n        status = \"✓ PASS\" if passed else \"✗ FAIL\"\n        fmt_val = f\"{value:.1%}\" if isinstance(value, float) and value <= 1 else f\"{value}\"\n        fmt_tgt = f\"{target:.0%}\" if isinstance(target, float) and target <= 1 else f\"{target}\"\n        print(f\"  {key:<34} {fmt_val:>8} {fmt_tgt:>8} {status:>8}\")\n\n    print(\"=\" * 62)\n    if all_pass:\n        print(\"  ALL GATES PASSED ✓ — Model ready for export\")\n    else:\n        print(\"  SOME GATES FAILED ✗ — Consider additional training\")\n    print(\"=\" * 62)\nelse:\n    print(f\"✗ Metrics file not found at {metrics_path}\")\n    print(\"Run evaluation (6.1) first.\")"
  },
  {
   "cell_type": "markdown",
   "id": "9694b8b5ebd0a6a1",
   "metadata": {},
   "source": "## Step 7: Test Model\n\nLoad the trained model and test it interactively against the specific coding TUI\nagent failure modes that this pipeline targets."
  },
  {
   "cell_type": "markdown",
   "id": "831aba486e8d6c31",
   "metadata": {},
   "source": "### 7.1 Load Model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d94f07ab6826e9b",
   "metadata": {},
   "outputs": [],
   "source": "from unsloth import FastLanguageModel\nfrom peft import PeftModel\nimport torch, os\n\nCHECKPOINT_PRIORITY = [\n    \"checkpoints/agent_sft_grpo/final\",\n    \"checkpoints/agent_sft_ipo/final\",\n    \"checkpoints/agent_sft/final\",\n    \"checkpoints/tool_calling_sft/final\",\n]\n\nMERGED_PATH = \"checkpoints/gpt-oss-20b-coding-tui-merged\"\n\nMODEL_PATH = None\nis_adapter = False\nfor path in CHECKPOINT_PRIORITY:\n    if os.path.exists(path) and os.path.exists(os.path.join(path, \"adapter_config.json\")):\n        MODEL_PATH = path\n        is_adapter = True\n        break\n\nif MODEL_PATH is None and os.path.exists(MERGED_PATH):\n    MODEL_PATH = MERGED_PATH\n    is_adapter = False\n\nif MODEL_PATH is None:\n    print(\"✗ No checkpoint found. Train the model first.\")\nelse:\n    print(f\"Loading model from: {MODEL_PATH}\")\n    print(f\"  Type: {'LoRA adapter' if is_adapter else 'merged model'}\")\n\n    model = None\n    base_name = \"openai/gpt-oss-20b\"\n\n    # Try 1: Pre-quantized BNB 4-bit (avoids GptOssExperts BNB traversal issue)\n    try:\n        print(\"  Loading pre-quantized BNB 4-bit model...\")\n        model, tokenizer = FastLanguageModel.from_pretrained(\n            \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\",\n            max_seq_length=8192,\n            dtype=None,\n            load_in_4bit=False,\n        )\n        print(\"  Mode: BNB 4-bit (pre-quantized)\")\n    except Exception as e:\n        print(f\"  Pre-quantized BNB failed: {e}\")\n\n    # Try 2: bfloat16 without quantization\n    if model is None:\n        print(\"  Loading in bfloat16 (no quantization)...\")\n        model, tokenizer = FastLanguageModel.from_pretrained(\n            base_name,\n            max_seq_length=8192,\n            dtype=torch.bfloat16,\n            load_in_4bit=False,\n        )\n        print(\"  Mode: bfloat16 (no quantization)\")\n\n    if is_adapter:\n        print(f\"  Applying LoRA adapter from {MODEL_PATH}...\")\n        model = PeftModel.from_pretrained(model, MODEL_PATH)\n\n    FastLanguageModel.for_inference(model)\n    print(\"✓ Model loaded!\")"
  },
  {
   "cell_type": "markdown",
   "id": "5586e6239f2a9833",
   "metadata": {},
   "source": "### 7.2 Test Against Failure Modes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e7ea5f0916ba33",
   "metadata": {},
   "outputs": [],
   "source": "import sys, torch\nsys.path.insert(0, \"scripts\")\nfrom dataset_formatters.harmony import encode_harmony_messages\n\nCODING_AGENT_DEV = (\n    \"You are a coding agent. Use tools to read files, write code, run tests, and \"\n    \"complete programming tasks. Do not just analyze — always take action and produce \"\n    \"working code. After making changes, verify they work by running the relevant tests. \"\n    \"If a tool call fails, diagnose and retry with corrected parameters.\"\n)\n\n# Test prompts designed to expose each failure mode\nTEST_PROMPTS = [\n    # Test 1: Tool calling accuracy — should call read_file with valid path, NOT a made-up tool\n    (\n        \"Failure Mode: Tool Calling\",\n        \"Read the file at src/main.rs and fix any compilation errors you find.\",\n        [\"read_file\", \"write_file\", \"run_command\"],\n    ),\n    # Test 2: Follow-through — should NOT just say \"I would need to look at...\"\n    (\n        \"Failure Mode: No Follow-Through\",\n        \"Write a Python function called `binary_search(arr, target)` that returns the index of target in sorted arr, or -1 if not found. Add it to utils.py and write a pytest test for it.\",\n        None,\n    ),\n    # Test 3: Circular reasoning — model should take action, not loop\n    (\n        \"Failure Mode: Circular Reasoning\",\n        \"Analyze the codebase and suggest improvements. Then implement the most impactful one.\",\n        None,\n    ),\n    # Test 4: Context tracking — should remember the task mid-session\n    (\n        \"Failure Mode: Context Loss\",\n        \"I need you to refactor the authentication module. Start by reading auth.py, then identify the issues, then fix them one by one.\",\n        None,\n    ),\n]\n\ndef generate_response(prompt, tools=None, max_tokens=512):\n    \"\"\"Generate a response using Harmony format.\"\"\"\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    if tools:\n        tool_ctx = \"\\n\".join(f\"  - {t}(path: str)\" for t in tools)\n        messages[0][\"content\"] = (\n            f\"Available tools:\\n{tool_ctx}\\n\\n\" + messages[0][\"content\"]\n        )\n    formatted = encode_harmony_messages(\n        messages,\n        developer_instructions=CODING_AGENT_DEV,\n        add_generation_prompt=True,\n    )\n    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_tokens,\n            temperature=0.3,\n            do_sample=True,\n            top_p=0.9,\n        )\n    return tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n\n\nfor label, prompt, tools in TEST_PROMPTS:\n    print(f\"\\n{'=' * 64}\")\n    print(f\"TEST: {label}\")\n    print(f\"{'=' * 64}\")\n    print(f\"Prompt: {prompt[:120]}...\")\n    print(\"-\" * 64)\n    response = generate_response(prompt, tools, max_tokens=384)\n    print(response)\n    print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e043b8c87b869b0",
   "metadata": {},
   "outputs": [],
   "source": "# ── Custom Prompt ─────────────────────────────────────────────────────────────\nCUSTOM_PROMPT = (\n    \"Read requirements.txt and install any missing packages, \"\n    \"then run the test suite and fix any failing tests.\"\n)\n\nprint(f\"Custom prompt: {CUSTOM_PROMPT}\")\nprint(\"=\" * 64)\nprint(generate_response(CUSTOM_PROMPT, max_tokens=512))"
  },
  {
   "cell_type": "markdown",
   "id": "e57b5e0fcf8554b1",
   "metadata": {},
   "source": "## Step 8: Export\n\nMerge the final adapter and export to HuggingFace safetensors + GGUF formats.\n\nThe GGUF file can be loaded directly into MacLean AI via llama-server\nfor Codex CLI integration testing."
  },
  {
   "cell_type": "markdown",
   "id": "52400da4f021f454",
   "metadata": {},
   "source": "### 8.1 Export to GGUF"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47003e9d53b4dcf9",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\ntracker.start(\"export\")\n\nADAPTER_PRIORITY = [\n    \"checkpoints/agent_sft_grpo/final\",\n    \"checkpoints/agent_sft_ipo/final\",\n    \"checkpoints/agent_sft/final\",\n    \"checkpoints/tool_calling_sft/final\",\n]\n\nadapter_path = None\nfor path in ADAPTER_PRIORITY:\n    if os.path.exists(path):\n        adapter_path = path\n        break\n\nif adapter_path is None:\n    print(\"✗ No adapter checkpoint found.\")\n    tracker.fail(\"export\")\nelse:\n    export_dir = \"checkpoints/gpt-oss-20b-coding-tui-export\"\n    print(f\"Exporting adapter: {adapter_path}\")\n    print(f\"Output: {export_dir}\")\n    print(\"=\" * 60)\n\n    !python scripts/19_merge_adapter.py \\\n        --adapter_path {adapter_path} \\\n        --output_dir {export_dir} \\\n        --export_formats hf gguf_q4\n\n    drive_helper.backup(export_dir, \"checkpoints/gpt-oss-20b-coding-tui-export\")\n    if DRIVE_MODE != \"local\":\n        print(\"\\nExport backed up to Drive.\")\n\n    tracker.complete(\"export\")"
  },
  {
   "cell_type": "markdown",
   "id": "e3fd14940c046655",
   "metadata": {},
   "source": "### 8.2 QAT Export (Optional)\n\nQuantisation-Aware Training for MXFP4 deployment.\nRecovers 97-100% quality vs 59-89% with post-training quantisation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9591992819e91213",
   "metadata": {},
   "outputs": [],
   "source": "if not CONFIG.get(\"enable_qat_export\"):\n    print(\"QAT export disabled. Enable via widget toggle in Step 0.3.\")\n    print(\"\\nQAT recovers 97-100% quality when deploying to MXFP4,\")\n    print(\"vs 59-89% with standard post-training quantisation (PTQ).\")\nelse:\n    import os\n    export_dir = \"checkpoints/gpt-oss-20b-coding-tui-export\"\n    qat_dir = \"checkpoints/gpt-oss-20b-coding-tui-qat\"\n\n    if not os.path.exists(export_dir):\n        print(\"✗ Run standard export (8.1) first.\")\n    else:\n        print(\"Running QAT pass on merged model...\")\n        print(\"  This fine-tunes with MXFP4-aware quantisation at reduced LR (1e-5).\")\n        print(\"=\" * 60)\n\n        try:\n            import modelopt.torch.quantization as mtq\n            print(\"✓ nvidia-modelopt available\")\n            print(\"\\nQAT pipeline (manual steps):\")\n            print(f\"  1. Load merged BF16 model from {export_dir}\")\n            print(f\"  2. mtq.quantize(model, config=mtq.MXFP4_DEFAULT_CFG)\")\n            print(f\"  3. Fine-tune for ~100 steps at LR 1e-5\")\n            print(f\"  4. Export to {qat_dir}\")\n        except ImportError:\n            print(\"✗ nvidia-modelopt not installed.\")\n            print(\"  Install: pip install nvidia-modelopt\")"
  },
  {
   "cell_type": "markdown",
   "id": "d21eff13aa7e28d2",
   "metadata": {},
   "source": "### 8.3 Download GGUF"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee853f0eb5a9b139",
   "metadata": {},
   "outputs": [],
   "source": "IN_COLAB = \"COLAB_GPU\" in os.environ or os.path.exists(\"/content\")\n\nif IN_COLAB:\n    from google.colab import files\n    import glob, os\n\n    export_dir = \"checkpoints/gpt-oss-20b-coding-tui-export\"\n    gguf_files = glob.glob(os.path.join(export_dir, \"**/*.gguf\"), recursive=True)\n\n    if gguf_files:\n        gguf_path = gguf_files[0]\n        size_gb = os.path.getsize(gguf_path) / (1024**3)\n        print(f\"Downloading: {os.path.basename(gguf_path)} ({size_gb:.1f} GB)\")\n        files.download(gguf_path)\n    else:\n        print(\"✗ No GGUF file found. Run export (8.1) first.\")\nelse:\n    print(\"Download not available outside Colab.\")\n    print(\"GGUF file is at: checkpoints/gpt-oss-20b-coding-tui-export/\")"
  },
  {
   "cell_type": "markdown",
   "id": "bf24311a84a2f0d2",
   "metadata": {},
   "source": "### 8.4 Upload to HuggingFace Hub"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2fb0e228f0c57a",
   "metadata": {},
   "outputs": [],
   "source": "# --- Configuration ---\nHF_REPO_ID = \"\"  # e.g. \"your-username/gpt-oss-20b-coding-tui-agent\"\nHF_PRIVATE = True\n\nassert HF_REPO_ID, \"Set HF_REPO_ID above before running this cell.\"\n\nimport os, glob\nfrom huggingface_hub import HfApi\n\n# Authenticate: try Colab Secrets first, then interactive login\ntry:\n    from google.colab import userdata\n    hf_token = userdata.get(\"HF_TOKEN\")\n    print(\"Using HF_TOKEN from Colab Secrets.\")\nexcept Exception:\n    from huggingface_hub import login\n    login()\n    hf_token = None\n\napi = HfApi(token=hf_token)\napi.create_repo(repo_id=HF_REPO_ID, private=HF_PRIVATE, exist_ok=True)\nprint(f\"Repo ready: https://huggingface.co/{HF_REPO_ID}\")\n\n# --- Model card ---\nexport_dir = \"checkpoints/gpt-oss-20b-coding-tui-export\"\nhf_dir = os.path.join(export_dir, \"hf\")\n\nmodel_card = \"\"\"\\\n---\nbase_model: openai/gpt-oss-20b\ntags:\n  - coding-agent\n  - tool-calling\n  - codex-cli\n  - gpt-oss\n  - qlora\n  - unsloth\n  - grpo\n  - tui\nlicense: apache-2.0\npipeline_tag: text-generation\n---\n\n# GPT-OSS 20B Coding TUI Agent\n\nFine-tuned from [openai/gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b) for\nuse as a coding TUI agent (Codex CLI integration via MacLean AI + llama-server).\n\n## Problem Statement\n\nGPT-OSS 20B exhibits four failure modes when used as a coding agent:\n1. Tool calling errors (invalid params, non-existent MCP servers)\n2. No follow-through (analysis loops, never writes code)\n3. Circular reasoning (repeating the same analysis)\n4. Context loss (forgetting task state mid-session)\n\n## Training Pipeline\n\n1. **Tool Calling SFT** (rank 64) \\u2014 Glaive + xLAM + Hermes in Harmony format\n2. **Merge** \\u2014 tool-calling adapter merged into base\n3. **Agent SFT** (rank 128) \\u2014 code-act, commitpack, editpackft + proxy log trajectories\n4. **IPO** \\u2014 decisive action preferred over circular analysis\n5. **GRPO RL** \\u2014 execution-grounded: code compiles, tests pass, no loops\n\nTrained with [Unsloth](https://github.com/unslothai/unsloth) QLoRA.\n\n## Deployment\n\nDesigned for deployment via [llama-server](https://github.com/ggerganov/llama.cpp)\nwith the [claude-proxy-v2](https://github.com/rmarnold/claude-proxy-v2) translation layer\nfor Codex CLI (OpenAI Responses API).\n\n## GGUF\n\nA quantised GGUF file is included for use with llama.cpp.\n\"\"\".format()\n\nreadme_path = os.path.join(hf_dir, \"README.md\")\nos.makedirs(hf_dir, exist_ok=True)\nwith open(readme_path, \"w\") as f:\n    f.write(model_card)\nprint(f\"Wrote model card to {readme_path}\")\n\n# --- Upload HF safetensors model ---\nassert os.path.isdir(hf_dir), f\"HF export dir not found: {hf_dir}. Run export (8.1) first.\"\nprint(f\"Uploading HF model from {hf_dir} ...\")\napi.upload_folder(\n    folder_path=hf_dir,\n    repo_id=HF_REPO_ID,\n    commit_message=\"Upload GPT-OSS 20B Coding TUI Agent (tool-calling + agent-SFT + IPO + GRPO)\",\n    token=hf_token,\n)\nprint(\"HF model uploaded.\")\n\n# --- Upload GGUF file ---\ngguf_files = glob.glob(os.path.join(export_dir, \"**/*.gguf\"), recursive=True)\nif gguf_files:\n    gguf_path = gguf_files[0]\n    gguf_name = os.path.basename(gguf_path)\n    size_gb = os.path.getsize(gguf_path) / (1024**3)\n    print(f\"Uploading GGUF: {gguf_name} ({size_gb:.1f} GB) ...\")\n    api.upload_file(\n        path_or_fileobj=gguf_path,\n        path_in_repo=gguf_name,\n        repo_id=HF_REPO_ID,\n        commit_message=f\"Upload GGUF quantisation ({gguf_name})\",\n        token=hf_token,\n    )\n    print(\"GGUF uploaded.\")\nelse:\n    print(\"No GGUF file found \\u2014 skipping. Run export (8.1) to generate one.\")\n\nprint(f\"\\nDone! View your model at: https://huggingface.co/{HF_REPO_ID}\")"
  },
  {
   "cell_type": "markdown",
   "id": "fb181a0eed17d3a5",
   "metadata": {},
   "source": "---\n## Training Complete!\n\nYour GPT-OSS 20B Coding TUI Agent is trained and ready for Codex CLI integration.\n\n**Pipeline summary:**\n1. Tool Calling SFT: Glaive (113K) + xLAM (60K) + Hermes — correct tool schemas and parameter formatting\n2. Merge: tool-calling adapter fused into base weights\n3. Agent SFT: code-act + commitpack + editpackft + real proxy log trajectories (most valuable)\n4. IPO: decisive action preferred over circular analysis (hh-rlhf + code feedback)\n5. GRPO RL: execution-grounded — rewards for compiling code and passing tests, penalises loops\n\n**Outputs:**\n- Checkpoints: `checkpoints/agent_sft_{ipo,grpo}/final`\n- Evaluation: `evals/coding_tui_agent/metrics.json`\n- Exported model: `checkpoints/gpt-oss-20b-coding-tui-export/`\n- All backed up to Google Drive: `gpt-oss-20b-coding-tui/`\n\n**MacLean AI integration:**\n- Copy the exported GGUF to your MacLean AI model directory\n- Select it in the Model Browser\n- Enable Codex CLI support in Settings\n- The proxy translation layer (`--translate-anthropic`) handles the Anthropic↔OpenAI format conversion\n- Test with: `codex \"Read src/main.rs and fix any compilation errors\"`\n\n**Next steps:**\n- Review evaluation metrics in Step 6\n- Test against each failure mode in Step 7\n- If circular reasoning persists: increase GRPO steps or add more no-action penalisation\n- If tool calling is still poor: increase tool_calling_sft steps or add more xLAM data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c65eef8e83ef10",
   "metadata": {},
   "outputs": [],
   "source": "# Disconnect and release GPU runtime to stop billing\ntry:\n    from google.colab import runtime\n    runtime.unassign()\nexcept ImportError:\n    print(\"Not in Colab — no runtime to release.\")"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}