{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GPT-OSS 20B → Rust Coding Agent (v2)\n",
    "\n",
    "End-to-end pipeline for training a Rust coding agent on OpenAI's GPT-OSS 20B (MoE, ~3.6B active params).\n",
    "\n",
    "**v2 Optimisations** (see `docs/V2_OPTIMIZATION_PLAN.md`):\n",
    "- **Split LoRA** — 7-12x faster MoE training via reordered LoRA computation\n",
    "- **FP8 RL** — 1.6x throughput, 60% less VRAM on H100 (auto-fallback to 4-bit on A100)\n",
    "- **GRPO long context** — Chunked batching enables 65K+ context (up from 32K)\n",
    "- **Flex Attention** — 8x longer sequences with attention sinks\n",
    "- **Auto packing** — 3x faster SFT with uncontaminated packing (zero-config)\n",
    "- **Expert monitoring** — Routing utilisation tracking across all phases\n",
    "- **QAT export** — 97-100% MXFP4 quality retention (vs 59-89% with PTQ)\n",
    "\n",
    "**4-Phase Pipeline:**\n",
    "1. **Lang Adapter** — Rust domain specialisation via QLoRA (script 13 + 19)\n",
    "2. **Core Agent SFT** — Agent trajectory training with tool use (script 14)\n",
    "3. **IPO Preference** — Identity Preference Optimisation on ranked pairs (script 17)\n",
    "4. **GRPO RL** — Group Relative Policy Optimisation with execution rewards (script 18)\n",
    "\n",
    "**Requirements:**\n",
    "- **GPU**: A100 40GB+ (H100 80GB recommended for FP8 + extended context)\n",
    "- **Storage**: Google Drive for persistent checkpoints\n",
    "- **Rust toolchain**: Installed automatically (rustup + cargo-mutants)"
   ],
   "id": "4426faf4d8d7ae35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Environment Setup"
   ],
   "id": "c68353567587459c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Mount Google Drive & Clone Repository\n",
    "\n",
    "**PyCharm / headless users:** If `drive.mount()` doesn't work (e.g. PyCharm Colab\n",
    "plugin can't relay the OAuth popup), set `use_service_account = True` and provide\n",
    "your service-account JSON key path in Step 0.3.\n"
   ],
   "id": "97b1ca5318fc7ab0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T06:23:20.573370Z",
     "start_time": "2026-02-13T06:23:19.467366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "use_service_account = True\n",
    "\n",
    "DRIVE_MOUNTED = False\n",
    "\n",
    "if IN_COLAB and not use_service_account:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        DRIVE_MOUNTED = True\n",
    "        print(\"Google Drive mounted\")\n",
    "    except Exception as e:\n",
    "        print(f\"drive.mount() failed: {e}\")\n",
    "        print(\"Falling back to local-only mode.\")\n",
    "        print(\"Tip: set use_service_account=True and provide a JSON key in Step 0.3.\")\n",
    "elif IN_COLAB and use_service_account:\n",
    "    print(\"Service-account mode selected — skipping drive.mount()\")\n",
    "    print(\"Configure credentials in Step 0.3.\")\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "\n",
    "REPO_URL = \"https://github.com/rmarnold/llm-training-pipeline.git\"\n",
    "BRANCH = \"main\"\n",
    "\n",
    "REPO_DIR = \"/content/llm-training-pipeline\"\n",
    "\n",
    "if IN_COLAB:\n",
    "    if os.path.exists(REPO_DIR):\n",
    "        %cd {REPO_DIR}\n",
    "        !git pull origin {BRANCH}\n",
    "    else:\n",
    "        !git clone -b {BRANCH} {REPO_URL} {REPO_DIR}\n",
    "        %cd {REPO_DIR}\n",
    "\n",
    "    PROJECT_ROOT = REPO_DIR\n",
    "else:\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"\\nProject root: {PROJECT_ROOT}\")\n"
   ],
   "id": "604e923187b87e5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service-account mode selected — skipping drive.mount()\n",
      "Configure credentials in Step 0.3.\n",
      "Cloning into '/content/llm-training-pipeline'...\n",
      "remote: Enumerating objects: 1077, done.\u001b[K\n",
      "remote: Counting objects: 100% (248/248), done.\u001b[K\n",
      "remote: Compressing objects: 100% (173/173), done.\u001b[K\n",
      "remote: Total 1077 (delta 139), reused 166 (delta 74), pack-reused 829 (from 1)\u001b[K\n",
      "Receiving objects: 100% (1077/1077), 1.78 MiB | 23.05 MiB/s, done.\n",
      "Resolving deltas: 100% (680/680), done.\n",
      "/content/llm-training-pipeline\n",
      "\n",
      "Project root: /content/llm-training-pipeline\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Install Dependencies\n",
    "\n",
    "Installs pipeline deps, latest Unsloth (with Split LoRA + FP8 RL), and the Rust toolchain.\n",
    "\n",
    "**Note:** flash-attn is intentionally NOT installed. FA3 is incompatible with GPT-OSS\n",
    "backward passes (incorrect training loss). Unsloth's Flex Attention replaces it\n",
    "automatically — no compilation step needed.\n"
   ],
   "id": "4f8d5ba0fcc56075"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T06:27:33.449268Z",
     "start_time": "2026-02-13T06:23:20.582734Z"
    }
   },
   "source": [
    "if IN_COLAB:\n",
    "    print(\"Installing Python dependencies...\")\n",
    "    print(\"=\" * 60)\n",
    "    !pip install -q -e \".[gpt_oss,rust_eval,colab]\"\n",
    "\n",
    "    # Fix pyarrow binary incompatibility with datasets 4.x on Colab\n",
    "    # (Colab's pre-installed pyarrow C extension doesn't match the new header)\n",
    "    !pip install -q --force-reinstall pyarrow\n",
    "\n",
    "    # v2: Force latest Unsloth with Split LoRA + FP8 RL + GRPO long context\n",
    "    # Flex Attention (bundled with Unsloth) replaces Flash Attention for GPT-OSS\n",
    "    print(\"\\nInstalling latest Unsloth (Split LoRA + FP8 RL + Flex Attention)...\")\n",
    "    !pip install -q --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo\n",
    "    !pip install -q \"unsloth[colab-new]\"\n",
    "\n",
    "    # v2: vLLM for FP8 RL inference (H100 only, optional)\n",
    "    !pip install -q vllm>=0.12.0 2>/dev/null || true\n",
    "\n",
    "    print(\"\\nInstalling Rust toolchain...\")\n",
    "    print(\"=\" * 60)\n",
    "    !curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
    "    os.environ[\"PATH\"] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "    !cargo install cargo-mutants\n",
    "\n",
    "    # Verification — use importlib.metadata to check versions without importing\n",
    "    # (importing unsloth triggers heavy CUDA init that can hang in a notebook cell)\n",
    "    from importlib.metadata import version, PackageNotFoundError\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Dependency Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for pkg in [\"unsloth\", \"trl\", \"peft\", \"datasets\", \"tiktoken\", \"vllm\"]:\n",
    "        try:\n",
    "            ver = version(pkg)\n",
    "            print(f\"\\u2713 {pkg}: {ver}\")\n",
    "        except PackageNotFoundError:\n",
    "            if pkg == \"vllm\":\n",
    "                print(f\"\\u2014 {pkg}: not installed (optional, H100 FP8 RL only)\")\n",
    "            else:\n",
    "                print(f\"\\u2717 {pkg}: not installed\")\n",
    "\n",
    "    import subprocess\n",
    "    for cmd, label in [(\"cargo --version\", \"cargo\"), (\"cargo mutants --version\", \"cargo-mutants\")]:\n",
    "        result = subprocess.run(cmd.split(), capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"\\u2713 {label}: {result.stdout.strip()}\")\n",
    "        else:\n",
    "            print(f\"\\u2717 {label}: not found\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"Running locally \\u2014 ensure deps are installed:\")\n",
    "    print(\"  pip install -e '.[gpt_oss,rust_eval]'\")\n",
    "    print(\"  pip install --upgrade unsloth unsloth_zoo\")\n"
   ],
   "id": "fb54d828abbdfd4b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing Python dependencies...\n",
      "============================================================\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.3/432.3 kB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m132.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/44/97/f984647661a9fd0a61276795c883e5de26d77596300ee289ca297e336285/trafilatura-1.11.0-py3-none-any.whl\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.5/376.5 kB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m131.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m915.7/915.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m174.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mmm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m184.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.9/837.9 kB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m164.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m121.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building editable for llm-training-pipeline (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for pyfastcopy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cuda-python 12.9.5 requires cuda-bindings~=12.9.5, but you have cuda-bindings 12.9.4 which is incompatible.\n",
      "torchaudio 2.9.0+cu128 requires torch==2.9.0, but you have torch 2.10.0 which is incompatible.\n",
      "fastai 2.8.6 requires torch<2.10,>=1.10, but you have torch 2.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "Installing latest Unsloth (Split LoRA + FP8 RL + Flex Attention)...\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.3/432.3 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.5/376.5 kB\u001b[0m \u001b[31m626.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: unsloth 2026.2.1 does not provide the extra 'triton'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "Installing Rust toolchain...\n",
      "============================================================\n",
      "\u001b[1minfo:\u001b[0m downloading installer\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mprofile set to 'default'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdefault host triple is x86_64-unknown-linux-gnu\n",
      "\u001b[0m\u001b[1minfo: \u001b[0msyncing channel updates for 'stable-x86_64-unknown-linux-gnu'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mlatest update on 2026-02-12, rust version 1.93.1 (01f6ddf75 2026-02-11)\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'cargo'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'clippy'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rust-docs'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rust-std'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rustc'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rustfmt'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'cargo'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'clippy'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rust-docs'\n",
      " 20.7 MiB /  20.7 MiB (100 %)  10.4 MiB/s in  1s         \n",
      "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rust-std'\n",
      " 28.1 MiB /  28.1 MiB (100 %)  14.6 MiB/s in  1s         \n",
      "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rustc'\n",
      " 74.4 MiB /  74.4 MiB (100 %)  16.1 MiB/s in  4s         \n",
      "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rustfmt'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdefault toolchain set to 'stable-x86_64-unknown-linux-gnu'\n",
      "\n",
      "  \u001b[0m\u001b[1m\u001b[0m\u001b[1m\u001b[32mstable-x86_64-unknown-linux-gnu installed\u001b[0m - rustc 1.93.1 (01f6ddf75 2026-02-11)\n",
      "\n",
      "\u001b[0m\u001b[1m\n",
      "Rust is installed now. Great!\n",
      "\u001b[0m\n",
      "To get started you may need to restart your current shell.\n",
      "This would reload your \u001b[0m\u001b[1mPATH\u001b[0m environment variable to include\n",
      "Cargo's bin directory ($HOME/.cargo/bin).\n",
      "\n",
      "To configure your current shell, you need to source\n",
      "the corresponding \u001b[0m\u001b[1menv\u001b[0m file under $HOME/.cargo.\n",
      "\n",
      "This is usually done by running one of the following (note the leading DOT):\n",
      ". \"$HOME/.cargo/env\"            # For sh/bash/zsh/ash/dash/pdksh\n",
      "source \"$HOME/.cargo/env.fish\"  # For fish\n",
      "source $\"($nu.home-path)/.cargo/env.nu\"  # For nushell\n",
      "\u001b[1m\u001b[92m    Updating\u001b[0m crates.io index\n",
      "\u001b[1m\u001b[92m Downloading\u001b[0m crates ...\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m cargo-mutants v26.2.0\n",
      "\u001b[1m\u001b[92m  Installing\u001b[0m cargo-mutants v26.2.0\n",
      "\u001b[1m\u001b[92m    Updating\u001b[0m crates.io index\n",
      "\u001b[1m\u001b[92m     Locking\u001b[0m 205 packages to latest compatible versions\n",
      "\u001b[1m\u001b[92m      Adding\u001b[0m cargo_metadata v0.19.2 \u001b[1m\u001b[33m(available: v0.23.1)\u001b[0m\n",
      "\u001b[1m\u001b[92m      Adding\u001b[0m console v0.15.11 \u001b[1m\u001b[33m(available: v0.16.2)\u001b[0m\n",
      "\u001b[1m\u001b[92m      Adding\u001b[0m ctrlc v3.5.1 \u001b[1m\u001b[33m(available: v3.5.2)\u001b[0m\n",
      "\u001b[1m\u001b[92m      Adding\u001b[0m itertools v0.13.0 \u001b[1m\u001b[33m(available: v0.14.0)\u001b[0m\n",
      "\u001b[1m\u001b[92m      Adding\u001b[0m jiff v0.1.29 \u001b[1m\u001b[33m(available: v0.2.20)\u001b[0m\n",
      "\u001b[1m\u001b[92m      Adding\u001b[0m mutants v0.0.3 \u001b[1m\u001b[33m(available: v0.0.4)\u001b[0m\n",
      "\u001b[1m\u001b[92m      Adding\u001b[0m nextest-metadata v0.12.3 \u001b[1m\u001b[33m(available: v0.13.3)\u001b[0m\n",
      "\u001b[1m\u001b[92m      Adding\u001b[0m nix v0.30.1 \u001b[1m\u001b[33m(available: v0.31.1)\u001b[0m\n",
      "\u001b[1m\u001b[92m      Adding\u001b[0m schemars v0.9.0 \u001b[1m\u001b[33m(available: v1.2.1)\u001b[0m\n",
      "\u001b[1m\u001b[92m      Adding\u001b[0m strum v0.26.3 \u001b[1m\u001b[33m(available: v0.27.2)\u001b[0m\n",
      "\u001b[1m\u001b[92m      Adding\u001b[0m target-lexicon v0.13.3 \u001b[1m\u001b[33m(available: v0.13.4)\u001b[0m\n",
      "\u001b[1m\u001b[92m      Adding\u001b[0m toml v0.8.23 \u001b[1m\u001b[33m(available: v1.0.1+spec-1.1.0)\u001b[0m\n",
      "\u001b[1m\u001b[92m      Adding\u001b[0m whoami v1.6.1 \u001b[1m\u001b[33m(available: v2.1.1)\u001b[0m\n",
      "\u001b[1m\u001b[92m Downloading\u001b[0m crates ...\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m atty v0.2.14\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m anstyle-query v1.1.5\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m bitflags v1.3.2\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m bitflags v2.10.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m anyhow v1.0.101\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m aho-corasick v1.1.4\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m terminal_size v0.4.3\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m fastrand v2.3.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m tempfile v3.25.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m target-spec v3.5.7\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m anstyle-parse v0.2.7\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m anstyle v1.0.13\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m test-log-macros v0.2.19\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m getrandom v0.4.1\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m toml_write v0.1.2\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m walkdir v2.5.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m thread_local v1.1.9\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m yansi v0.5.1\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m zmij v1.0.21\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m tracing-core v0.1.36\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m nutmeg v0.1.5\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m winnow v0.7.14\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m smol_str v0.3.5\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m smallvec v1.15.1\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m strum_macros v0.26.4\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m unicode-width v0.2.2\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m syn v2.0.115\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m tracing-subscriber v0.3.22\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m itertools v0.13.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m serde v1.0.228\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m libc v0.2.181\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m tracing v0.1.44\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m toml_datetime v0.6.11\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m whoami v1.6.1\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m schemars v0.9.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m serde_json v1.0.149\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m minimal-lexical v0.2.1\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m nom v7.1.3\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m regex v1.12.3\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m regex-syntax v0.8.9\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m nix v0.30.1\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m rustix v1.1.3\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m memchr v2.8.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m regex-automata v0.4.14\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m nextest-metadata v0.12.3\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m log v0.4.29\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m bstr v1.12.1\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m rustix v0.37.28\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m jiff v0.1.29\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m crossbeam-deque v0.8.6\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m serde_derive_internals v0.29.1\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m pin-project-lite v0.2.16\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m console v0.15.11\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m strsim v0.11.1\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m sharded-slab v0.1.7\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m same-file v1.0.6\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m reflink v0.1.3\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m ref-cast v1.0.25\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m proc-macro2 v1.0.106\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m nextest-workspace-hack v0.1.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m cfg-if v1.0.4\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m cargo_metadata v0.19.2\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m linux-raw-sys v0.3.8\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m semver v1.0.27\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m schemars_derive v0.9.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m ref-cast-impl v1.0.25\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m quote v1.0.44\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m nu-ansi-term v0.50.3\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m matchers v0.2.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m lazy_static v1.5.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m globset v0.4.18\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m fs2 v0.4.3\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m ctrlc v3.5.1\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m colorchoice v1.0.4\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m color-print v0.3.7\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m clap_builder v4.5.58\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m cfg-expr v0.20.6\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m similar v2.7.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m serde_spanned v0.6.9\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m serde_core v1.0.228\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m once_cell v1.21.3\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m is_terminal_polyfill v1.70.2\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m clap_complete v4.5.66\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m strum v0.26.3\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m jobserver v0.1.34\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m io-lifetimes v1.0.11\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m indexmap v2.13.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m ignore v0.4.25\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m flickzeug v0.4.5\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m errno v0.3.14\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m either v1.15.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m dyn-clone v1.0.20\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m color-print-proc-macro v0.3.7\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m clap_lex v1.0.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m num_cpus v1.17.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m guppy-workspace-hack v0.1.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m crossbeam-utils v0.8.21\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m clap v4.5.58\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m camino v1.2.2\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m toml_edit v0.22.27\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m serde_derive v1.0.228\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m rustversion v1.0.22\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m path-slash v0.2.1\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m itoa v1.0.17\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m equivalent v1.0.2\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m borsh v1.6.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m tracing-attributes v0.1.31\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m heck v0.5.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m hashbrown v0.16.1\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m unicode-ident v1.0.23\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m mutants v0.0.3\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m linux-raw-sys v0.11.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m tracing-log v0.2.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m toml v0.8.23\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m thiserror-impl v2.0.18\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m thiserror v2.0.18\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m env_logger v0.11.9\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m env_filter v1.0.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m utf8parse v0.2.2\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m terminal_size v0.2.6\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m indoc v2.0.7\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m cfg_aliases v0.2.1\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m target-lexicon v0.13.3\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m crossbeam-epoch v0.9.18\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m clap_derive v4.5.55\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m cargo-platform v0.1.9\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m test-log v0.2.19\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m anstream v0.6.21\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m quote v1.0.44\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m proc-macro2 v1.0.106\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m unicode-ident v1.0.23\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m serde_core v1.0.228\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m libc v0.2.181\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m memchr v2.8.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m serde v1.0.228\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m bitflags v2.10.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m utf8parse v0.2.2\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m zmij v1.0.21\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m rustix v1.1.3\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m once_cell v1.21.3\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m regex-syntax v0.8.9\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m log v0.4.29\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m cfg-if v1.0.4\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m linux-raw-sys v0.11.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m anstyle v1.0.13\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m serde_json v1.0.149\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m anstyle-query v1.1.5\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m colorchoice v1.0.4\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m is_terminal_polyfill v1.70.2\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m crossbeam-utils v0.8.21\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m target-lexicon v0.13.3\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m heck v0.5.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m itoa v1.0.17\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m smallvec v1.15.1\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m anstyle-parse v0.2.7\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m io-lifetimes v1.0.11\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m strsim v0.11.1\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m cfg_aliases v0.2.1\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m rustix v0.37.28\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m guppy-workspace-hack v0.1.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m tracing-core v0.1.36\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m anstream v0.6.21\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m camino v1.2.2\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m rustversion v1.0.22\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m thiserror v2.0.18\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m nix v0.30.1\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m nextest-workspace-hack v0.1.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m target-spec v3.5.7\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m getrandom v0.4.1\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m ref-cast v1.0.25\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m minimal-lexical v0.2.1\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m nu-ansi-term v0.50.3\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m pin-project-lite v0.2.16\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m linux-raw-sys v0.3.8\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m equivalent v1.0.2\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m clap_lex v1.0.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m bitflags v1.3.2\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m lazy_static v1.5.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m hashbrown v0.16.1\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m sharded-slab v0.1.7\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m env_filter v1.0.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m thread_local v1.1.9\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m anyhow v1.0.101\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m same-file v1.0.6\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m toml_write v0.1.2\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m winnow v0.7.14\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m aho-corasick v1.1.4\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m bstr v1.12.1\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m tracing-log v0.2.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m crossbeam-epoch v0.9.18\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m cfg-expr v0.20.6\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m env_logger v0.11.9\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m syn v2.0.115\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m nom v7.1.3\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m walkdir v2.5.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m yansi v0.5.1\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m fastrand v2.3.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m crossbeam-deque v0.8.6\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m either v1.15.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m unicode-width v0.2.2\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m dyn-clone v1.0.20\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m mutants v0.0.3\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m whoami v1.6.1\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m indexmap v2.13.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m indoc v2.0.7\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m path-slash v0.2.1\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m itertools v0.13.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m similar v2.7.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m atty v0.2.14\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m fs2 v0.4.3\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m console v0.15.11\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m num_cpus v1.17.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m reflink v0.1.3\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m jobserver v0.1.34\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m regex-automata v0.4.14\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m terminal_size v0.4.3\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m tempfile v3.25.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m clap_builder v4.5.58\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m smol_str v0.3.5\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m semver v1.0.27\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m ctrlc v3.5.1\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m terminal_size v0.2.6\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m nutmeg v0.1.5\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m serde_derive_internals v0.29.1\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m serde_derive v1.0.228\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m thiserror-impl v2.0.18\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m tracing-attributes v0.1.31\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m clap_derive v4.5.55\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m ref-cast-impl v1.0.25\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m test-log-macros v0.2.19\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m color-print-proc-macro v0.3.7\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m strum_macros v0.26.4\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m schemars_derive v0.9.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m color-print v0.3.7\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m tracing v0.1.44\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m flickzeug v0.4.5\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m strum v0.26.3\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m clap v4.5.58\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m clap_complete v4.5.66\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m matchers v0.2.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m globset v0.4.18\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m regex v1.12.3\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m tracing-subscriber v0.3.22\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m ignore v0.4.25\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m test-log v0.2.19\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m serde_spanned v0.6.9\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m toml_datetime v0.6.11\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m cargo-platform v0.1.9\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m schemars v0.9.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m toml_edit v0.22.27\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m jiff v0.1.29\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m cargo_metadata v0.19.2\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m nextest-metadata v0.12.3\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m toml v0.8.23\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m cargo-mutants v26.2.0\n",
      "\u001b[1m\u001b[92m    Finished\u001b[0m `release` profile [optimized] target(s) in 22.69s\n",
      "\u001b[1m\u001b[92m  Installing\u001b[0m /root/.cargo/bin/cargo-mutants\n",
      "\u001b[1m\u001b[92m   Installed\u001b[0m package `cargo-mutants v26.2.0` (executable `cargo-mutants`)\n",
      "\n",
      "============================================================\n",
      "Dependency Verification:\n",
      "============================================================\n",
      "✓ unsloth: 2026.2.1\n",
      "✓ trl: 0.24.0\n",
      "✓ peft: 0.18.1\n",
      "✓ datasets: 4.3.0\n",
      "✓ tiktoken: 0.12.0\n",
      "✓ vllm: 0.15.1\n",
      "✓ cargo: cargo 1.93.1 (083ac5135 2025-12-15)\n",
      "✓ cargo-mutants: cargo-mutants 26.2.0\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 0.3 Configure Pipeline\n\nEdit the variables below to configure the training run.\n\n**Training Scope** (`training_scope`):\n- `full` — All 4 phases end-to-end\n- `quick_test` — Short runs (100 steps each) to verify setup\n- `lang_adapter_only` — Only train lang_rust adapter + merge\n- `skip_to_rl` — Start from existing core_agent checkpoint (IPO + GRPO only)\n\n**Other settings:**\n- `gpu_tier` — Auto-detected below; override if needed\n- `max_steps_override` — Set >0 to cap all training stages (0 = use defaults)\n- `skip_data_generation` — Use pre-generated data from Drive\n- `include_grpo` — GRPO RL is slow; set `False` to skip\n- `enable_qat_export` — v2: QAT for MXFP4 export (97-100% quality vs 59-89% PTQ)\n\n**Service Account Setup** (for Drive backup):\n1. Set `use_service_account = True` in cell 0.1\n2. Run cell 0.3 — it will try, in order:\n   - **Existing file** at `/content/service_account.json` (instant on re-runs)\n   - **Colab Secrets** (if no file found — may timeout outside browser UI)\n   - **Paste prompt** — paste your JSON key content and press Enter twice\n3. Set `DRIVE_FOLDER_ID` in Colab Secrets, or set `drive_folder_id` below\n\n**PyCharm / headless users:** The paste prompt works in PyCharm and terminals.\nColab Secrets and file upload widgets require the Colab browser UI.",
   "id": "3d85f6b5a5ecf6cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T07:18:37.103762Z",
     "start_time": "2026-02-13T06:27:33.657647Z"
    }
   },
   "cell_type": "code",
   "source": "import json\ntraining_scope = \"quick_test\"  # \"full\", \"quick_test\", \"lang_adapter_only\", \"skip_to_rl\"\n\ngpu_tier = \"h100_80gb\"  # \"a100_40gb\", \"a100_80gb\", \"h100_80gb\"\n\nmax_steps_override = 0  # Set >0 to cap all stages (0 = use defaults)\n\nskip_data_generation = False  # True to use pre-generated data from Drive\n\ninclude_grpo = True  # False to skip GRPO RL (slow)\n\nenable_qat_export = True  # True for MXFP4 QAT export\n\n# ============================================================\n# SERVICE ACCOUNT CREDENTIALS\n# ============================================================\n# Priority order:\n#   1. Existing file at /content/service_account.json (instant, no timeout)\n#   2. Colab Secrets (only if no file found — may timeout outside browser UI)\n#   3. Paste JSON key via input() prompt\n#   4. Fall back to local mode (no Drive backup)\n\ndrive_folder_id = \"18UpFpUhiNrs2Etha0uFjSGWmj1Ee1SnX\"  # Override: Google Drive folder ID (if not using Colab Secrets)\n\n_SA_VM_PATH = \"/content/service_account.json\"\n_FOLDER_ID_PATH = \"/content/drive_folder_id.txt\"\nservice_account_key = \"\"\n\ndef _is_json(s):\n    \"\"\"Check if string looks like JSON (not a folder ID).\"\"\"\n    return s.strip().startswith(\"{\")\n\nif use_service_account and IN_COLAB:\n    # 1. Check for existing file first (avoids Colab Secrets timeout on re-runs)\n    if os.path.exists(_SA_VM_PATH):\n        service_account_key = _SA_VM_PATH\n        print(f\"Using existing key file: {_SA_VM_PATH}\")\n    else:\n        # 2. Try Colab Secrets (may timeout if not running in browser UI)\n        try:\n            from google.colab import userdata\n            _key_json = userdata.get(\"SERVICE_ACCOUNT_KEY\")\n            if _key_json:\n                with open(_SA_VM_PATH, \"w\") as _f:\n                    _f.write(_key_json)\n                service_account_key = _SA_VM_PATH\n                print(\"Service account key loaded from Colab Secrets.\")\n            else:\n                print(\"  Colab Secret 'SERVICE_ACCOUNT_KEY' is empty or not set.\")\n        except Exception as _e:\n            print(f\"  Colab Secrets lookup failed: {type(_e).__name__}: {_e}\")\n\n        # 3. Fall back to paste-based input (works in PyCharm, terminal, and Colab)\n        if not service_account_key:\n            try:\n                print(\"No service account key found.\")\n                _key_text = input(\"Paste service account JSON (entire content in one go): \")\n                _key_text = _key_text.strip()\n                if _key_text:\n                    json.loads(_key_text)  # Validate JSON\n                    with open(_SA_VM_PATH, \"w\") as _f:\n                        _f.write(_key_text)\n                    service_account_key = _SA_VM_PATH\n                    print(f\"Saved to {_SA_VM_PATH}\")\n            except json.JSONDecodeError:\n                print(\"  Invalid JSON \\u2014 key not saved.\")\n            except EOFError:\n                pass  # No stdin available\n\n    # Resolve drive_folder_id: hardcoded > saved file > Colab Secrets > input\n    if not drive_folder_id and os.path.exists(_FOLDER_ID_PATH):\n        with open(_FOLDER_ID_PATH) as _f:\n            drive_folder_id = _f.read().strip()\n        if drive_folder_id:\n            print(f\"Using saved folder ID from {_FOLDER_ID_PATH}\")\n\n    if not drive_folder_id:\n        try:\n            from google.colab import userdata\n            _fid = userdata.get(\"DRIVE_FOLDER_ID\") or \"\"\n            if _fid and not _is_json(_fid):\n                drive_folder_id = _fid\n                print(f\"Drive folder ID loaded from Colab Secrets.\")\n            elif _fid:\n                print(\"WARNING: DRIVE_FOLDER_ID Colab Secret contains JSON, not a folder ID. Ignoring.\")\n        except Exception:\n            pass\n\n    if not drive_folder_id and service_account_key:\n        _fid = input(\"Enter Google Drive folder ID (from URL): \").strip()\n        if _fid and not _is_json(_fid):\n            drive_folder_id = _fid\n            with open(_FOLDER_ID_PATH, \"w\") as _f:\n                _f.write(drive_folder_id)\n            print(f\"Saved folder ID to {_FOLDER_ID_PATH}\")\n        elif _fid:\n            print(\"ERROR: That looks like JSON, not a folder ID.\")\n            print(\"The folder ID is the part after /folders/ in the Google Drive URL.\")\n\n    if not service_account_key:\n        print(\"No service account key \\u2014 Drive backup disabled.\")\n        print(\"Tip: Re-run this cell and paste your JSON key when prompted.\")\n\nelif use_service_account:\n    # Running locally \\u2014 look for key file\n    for _path in [_SA_VM_PATH, \"service_account.json\"]:\n        if os.path.exists(_path):\n            service_account_key = _path\n            print(f\"Using key file: {_path}\")\n            break\n    if not service_account_key:\n        print(\"Running locally \\u2014 set service_account_key to your JSON key path.\")\n\n# ============================================================\n# DRIVE MODE\n# ============================================================\nfrom scripts.pipeline_lib.drive_utils import DriveHelper\n\nDRIVE_BASE = \"/content/drive/MyDrive/gpt-oss-20b-rust-agent-v2\"\n\nif DRIVE_MOUNTED:\n    DRIVE_MODE = \"mounted\"\nelif use_service_account and service_account_key and drive_folder_id:\n    DRIVE_MODE = \"service_account\"\nelse:\n    DRIVE_MODE = \"local\"\n\ndrive_helper = DriveHelper(\n    mode=DRIVE_MODE,\n    drive_base=DRIVE_BASE,\n    credentials_path=service_account_key or None,\n    folder_id=drive_folder_id or None,\n)\n\n# ============================================================\n# v2 GPU TIER CONFIGS (with H100 FP8 tier)\n# ============================================================\n\nGPU_CONFIGS = {\n    \"a100_40gb\": {\n        \"moe_backend\": \"unsloth_triton\",\n        \"load_mode\": \"4bit\",\n        \"fast_inference\": False,\n        \"lang_rust\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 8192, \"max_steps\": 3000},\n        \"core_agent\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 12288, \"max_steps\": 2000},\n        \"ipo\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 12288, \"max_steps\": 1000},\n        \"grpo\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 16384, \"max_steps\": 2000, \"num_gen\": 2},\n    },\n    \"a100_80gb\": {\n        \"moe_backend\": \"unsloth_triton\",\n        \"load_mode\": \"4bit\",\n        \"fast_inference\": False,\n        \"lang_rust\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 8192, \"max_steps\": 5000},\n        \"core_agent\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 16384, \"max_steps\": 3000},\n        \"ipo\": {\"batch\": 1, \"grad_accum\": 16, \"seq_len\": 16384, \"max_steps\": 2000},\n        \"grpo\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 32768, \"max_steps\": 5000, \"num_gen\": 4},\n    },\n    \"h100_80gb\": {\n        \"moe_backend\": \"grouped_mm\",\n        \"load_mode\": \"fp8\",\n        \"fast_inference\": True,\n        \"lang_rust\": {\"batch\": 2, \"grad_accum\": 4, \"seq_len\": 8192, \"max_steps\": 5000},\n        \"core_agent\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 16384, \"max_steps\": 3000},\n        \"ipo\": {\"batch\": 1, \"grad_accum\": 16, \"seq_len\": 16384, \"max_steps\": 2000},\n        \"grpo\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 65536, \"max_steps\": 7000, \"num_gen\": 4},\n    },\n}\n\n# Quick test overrides\nif training_scope == \"quick_test\":\n    max_steps_override = 100\n\ngpu_cfg = GPU_CONFIGS[gpu_tier]\n\n# Detect CPU count and RAM for parallel mutation jobs.\n# Each cargo-mutants worker spawns cargo build/test subprocesses that\n# typically peak at 1-3 GB RAM each.\nimport multiprocessing\nimport os as _os\ncpu_count = multiprocessing.cpu_count()\n\ntry:\n    _mem_bytes = _os.sysconf('SC_PAGE_SIZE') * _os.sysconf('SC_PHYS_PAGES')\n    total_ram_gb = _mem_bytes / (1024**3)\n    ram_based_jobs = max(1, int(total_ram_gb / 4))\nexcept (ValueError, OSError):\n    total_ram_gb = 0\n    ram_based_jobs = cpu_count\n\ncpu_based_jobs = max(1, cpu_count - 2)\nmutation_jobs = min(cpu_based_jobs, ram_based_jobs)\n\n# Repo workers: process multiple repos simultaneously\n# Each repo worker gets mutation_jobs // repo_workers internal jobs\nmutation_repo_workers = max(1, mutation_jobs // 4)\n\n# Build CONFIG dict\nCONFIG = {\n    \"training_scope\": training_scope,\n    \"gpu_tier\": gpu_tier,\n    \"include_grpo\": include_grpo,\n    \"skip_data_generation\": skip_data_generation,\n    \"enable_qat_export\": enable_qat_export,\n    # v2: MoE backend + load mode\n    \"moe_backend\": gpu_cfg[\"moe_backend\"],\n    \"load_mode\": gpu_cfg[\"load_mode\"],\n    \"fast_inference\": gpu_cfg[\"fast_inference\"],\n    # Lang adapter\n    \"lang_rust_batch\": gpu_cfg[\"lang_rust\"][\"batch\"],\n    \"lang_rust_grad_accum\": gpu_cfg[\"lang_rust\"][\"grad_accum\"],\n    \"lang_rust_seq_len\": gpu_cfg[\"lang_rust\"][\"seq_len\"],\n    \"lang_rust_max_steps\": max_steps_override or gpu_cfg[\"lang_rust\"][\"max_steps\"],\n    # Core agent\n    \"core_agent_batch\": gpu_cfg[\"core_agent\"][\"batch\"],\n    \"core_agent_grad_accum\": gpu_cfg[\"core_agent\"][\"grad_accum\"],\n    \"core_agent_seq_len\": gpu_cfg[\"core_agent\"][\"seq_len\"],\n    \"core_agent_max_steps\": max_steps_override or gpu_cfg[\"core_agent\"][\"max_steps\"],\n    # IPO\n    \"ipo_batch\": gpu_cfg[\"ipo\"][\"batch\"],\n    \"ipo_grad_accum\": gpu_cfg[\"ipo\"][\"grad_accum\"],\n    \"ipo_seq_len\": gpu_cfg[\"ipo\"][\"seq_len\"],\n    \"ipo_max_steps\": max_steps_override or gpu_cfg[\"ipo\"][\"max_steps\"],\n    # GRPO\n    \"grpo_batch\": gpu_cfg[\"grpo\"][\"batch\"],\n    \"grpo_grad_accum\": gpu_cfg[\"grpo\"][\"grad_accum\"],\n    \"grpo_seq_len\": gpu_cfg[\"grpo\"][\"seq_len\"],\n    \"grpo_max_steps\": max_steps_override or gpu_cfg[\"grpo\"][\"max_steps\"],\n    \"grpo_num_gen\": gpu_cfg[\"grpo\"][\"num_gen\"],\n    # Mutation generation \\u2014 balance CPU parallelism with RAM headroom\n    \"max_mutations_per_repo\": 50,\n    \"mutation_jobs\": mutation_jobs,\n    \"mutation_repo_workers\": mutation_repo_workers,\n    \"total_ram_gb\": total_ram_gb,\n    # Eval\n    \"eval_num_samples\": 10 if training_scope == \"quick_test\" else 50,\n}\n\nprint(\"=\" * 60)\nprint(\"PIPELINE CONFIGURATION (v2)\")\nprint(\"=\" * 60)\nprint(f\"\\nScope: {training_scope.upper()}\")\nprint(f\"GPU tier: {gpu_tier}\")\nprint(f\"CPUs: {cpu_count} | RAM: {total_ram_gb:.0f} GB\")\nprint(f\"Mutation parallelism: {CONFIG['mutation_jobs']} jobs, \"\n      f\"{CONFIG['mutation_repo_workers']} repo workers \"\n      f\"(cpu_limit: {cpu_based_jobs}, ram_limit(@4GB/worker): {ram_based_jobs})\")\nprint(f\"MoE backend: {CONFIG['moe_backend']}\")\nprint(f\"Load mode: {CONFIG['load_mode']}\")\nprint(f\"Fast inference (vLLM): {CONFIG['fast_inference']}\")\nprint(f\"Include GRPO: {include_grpo}\")\nprint(f\"QAT export: {enable_qat_export}\")\nprint(f\"Skip data gen: {skip_data_generation}\")\nprint(f\"Drive mode: {DRIVE_MODE}\")\nif max_steps_override:\n    print(f\"Max steps override: {max_steps_override}\")\nprint(f\"\\nLang Adapter:  batch={CONFIG['lang_rust_batch']} x grad_accum={CONFIG['lang_rust_grad_accum']}, seq={CONFIG['lang_rust_seq_len']}, steps={CONFIG['lang_rust_max_steps']}\")\nprint(f\"Core Agent:    batch={CONFIG['core_agent_batch']} x grad_accum={CONFIG['core_agent_grad_accum']}, seq={CONFIG['core_agent_seq_len']}, steps={CONFIG['core_agent_max_steps']}\")\nprint(f\"IPO:           batch={CONFIG['ipo_batch']} x grad_accum={CONFIG['ipo_grad_accum']}, seq={CONFIG['ipo_seq_len']}, steps={CONFIG['ipo_max_steps']}\")\nif include_grpo:\n    print(f\"GRPO:          batch={CONFIG['grpo_batch']} x grad_accum={CONFIG['grpo_grad_accum']}, seq={CONFIG['grpo_seq_len']}, steps={CONFIG['grpo_max_steps']}, gen={CONFIG['grpo_num_gen']}\")\nprint(\"=\" * 60)",
   "id": "93dd91bddbc4ed84",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Set Up Persistent Storage\n"
   ],
   "id": "aa77eea7bd7df89e"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "DRIVE_SUBDIRS = [\n    \"checkpoints/lang_rust\",\n    \"checkpoints/core_agent\",\n    \"checkpoints/core_agent_ipo\",\n    \"checkpoints/core_agent_grpo\",\n    \"checkpoints/gpt-oss-20b-rust-merged\",\n    \"data/rust/lang_rust\",\n    \"data/rust/core_agent\",\n    \"data/rust/mutations\",\n    \"data/rust/strandset\",\n    \"data/rust/ipo\",\n    \"data/rust/grpo\",\n    \"data/rust/eval\",\n    \"data/rust/repos\",\n    \"logs\",\n    \"evals/rust_agent\",\n]\n\nif DRIVE_MODE == \"mounted\":\n    # Mounted mode: create Drive dirs + symlink local → Drive (original behaviour)\n    print(f\"Setting up storage at: {DRIVE_BASE}\")\n    for subdir in DRIVE_SUBDIRS:\n        os.makedirs(os.path.join(DRIVE_BASE, subdir), exist_ok=True)\n\n    for dir_name in [\"checkpoints\", \"data\", \"logs\", \"evals\"]:\n        local_path = os.path.join(PROJECT_ROOT, dir_name)\n        drive_path = os.path.join(DRIVE_BASE, dir_name)\n\n        if os.path.exists(local_path) and not os.path.islink(local_path):\n            !cp -r {local_path}/* {drive_path}/ 2>/dev/null || true\n            !rm -rf {local_path}\n        elif os.path.islink(local_path):\n            os.unlink(local_path)\n\n        os.symlink(drive_path, local_path)\n        print(f\"  {dir_name} -> Drive (mounted)\")\n\nelif DRIVE_MODE == \"service_account\":\n    # Service-account mode: create local dirs, restore existing data from Drive\n    print(\"Setting up local storage + Drive API restore...\")\n    for subdir in DRIVE_SUBDIRS:\n        os.makedirs(os.path.join(PROJECT_ROOT, subdir), exist_ok=True)\n        drive_helper.ensure_dir(subdir)\n\n    for dir_name in [\"checkpoints\", \"data\", \"logs\", \"evals\"]:\n        local_path = os.path.join(PROJECT_ROOT, dir_name)\n        # Remove stale symlinks from previous mounted runs\n        if os.path.islink(local_path):\n            os.unlink(local_path)\n            os.makedirs(local_path, exist_ok=True)\n        print(f\"  {dir_name} -> local (backed up via Drive API)\")\n\n    print(\"\\nRestoring existing data from Drive...\")\n    for subdir in DRIVE_SUBDIRS:\n        local_target = os.path.join(PROJECT_ROOT, subdir)\n        drive_helper.restore(subdir, local_target)\n    print(\"Restore complete.\")\n\nelse:\n    # Local-only mode — no Drive\n    for d in [\"checkpoints\", \"data/rust\", \"logs\", \"evals/rust_agent\"]:\n        os.makedirs(d, exist_ok=True)\n    print(\"Local directories created (no Drive backup).\")\n\nprint(\"\\nStorage ready!\")",
   "id": "ef3dbe47b760cca0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5 Check GPU & Configure MoE Backend\n",
    "\n",
    "v2: Auto-detects H100 for FP8 RL and sets the optimal Split LoRA backend.\n"
   ],
   "id": "ec0e7828c971b71a"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    capability = torch.cuda.get_device_capability()\n",
    "    is_h100 = \"H100\" in gpu_name or \"H200\" in gpu_name or \"B200\" in gpu_name\n",
    "\n",
    "    CONFIG[\"use_fp8\"] = capability[0] >= 9 and is_h100\n",
    "\n",
    "    # v2: Auto-detect GPU tier (now includes H100)\n",
    "    if is_h100:\n",
    "        detected_tier = \"h100_80gb\"\n",
    "    elif gpu_memory >= 70:\n",
    "        detected_tier = \"a100_80gb\"\n",
    "    else:\n",
    "        detected_tier = \"a100_40gb\"\n",
    "\n",
    "    if detected_tier != CONFIG[\"gpu_tier\"]:\n",
    "        print(f\"NOTE: Auto-detected {detected_tier}, overriding configured {CONFIG['gpu_tier']}\")\n",
    "        CONFIG[\"gpu_tier\"] = detected_tier\n",
    "        # Re-derive tier-specific settings\n",
    "        gpu_cfg = GPU_CONFIGS[detected_tier]\n",
    "        CONFIG[\"moe_backend\"] = gpu_cfg[\"moe_backend\"]\n",
    "        CONFIG[\"load_mode\"] = gpu_cfg[\"load_mode\"]\n",
    "        CONFIG[\"fast_inference\"] = gpu_cfg[\"fast_inference\"]\n",
    "\n",
    "    # v2: Set Split LoRA MoE backend\n",
    "    os.environ[\"UNSLOTH_MOE_BACKEND\"] = CONFIG[\"moe_backend\"]\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"GPU: {gpu_name} ({gpu_memory:.0f} GB)\")\n",
    "    print(f\"Compute capability: {capability[0]}.{capability[1]}\")\n",
    "    print(f\"Tier: {CONFIG['gpu_tier']}\")\n",
    "    print(f\"\\nv2 Optimisations:\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(f\"  Load mode: {CONFIG['load_mode']}\")\n",
    "    print(f\"  FP8 available: {CONFIG['use_fp8']}\")\n",
    "    print(f\"  Fast inference (vLLM): {CONFIG['fast_inference']}\")\n",
    "\n",
    "    if gpu_memory < 40:\n",
    "        print(\"\\nWARNING: <40 GB VRAM. Long-context training (16K+) may OOM.\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"No GPU detected!\")\n",
    "    CONFIG[\"use_fp8\"] = False\n",
    "    os.environ[\"UNSLOTH_MOE_BACKEND\"] = \"native_torch\""
   ],
   "id": "f189d07bd7e89131",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Data Generation\n",
    "\n",
    "Generates mutation data from curated Rust repos and agent trajectories.\n",
    "Skip this step if you have pre-generated data on Drive (`skip_data_generation=True`)."
   ],
   "id": "7f93fc1acaa43c5f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Generate Mutation Data\n",
    "\n",
    "Runs `cargo-mutants` on curated Rust repos to produce bug-fix training pairs.\n"
   ],
   "id": "3db2daedc13bc633"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "if CONFIG[\"skip_data_generation\"]:\n    print(\"Skipping data generation (using pre-generated data from Drive)\")\nelif CONFIG[\"training_scope\"] in (\"skip_to_rl\",):\n    print(\"Skipping — not needed for this training scope\")\nelse:\n    max_muts = CONFIG[\"max_mutations_per_repo\"]\n    jobs = CONFIG[\"mutation_jobs\"]\n    repo_workers = CONFIG[\"mutation_repo_workers\"]\n\n    print(f\"Generating mutations (max {max_muts}/repo, {jobs} jobs, {repo_workers} repo workers)...\")\n    print(\"=\" * 60)\n\n    !python scripts/16_generate_mutations.py \\\n        --max_mutations_per_repo {max_muts} \\\n        --jobs {jobs} \\\n        --repo-workers {repo_workers}\n\n    drive_helper.backup(\"data/rust/mutations\", \"data/rust/mutations\")\n    if DRIVE_MODE != \"local\":\n        print(\"\\nBacked up mutations to Drive.\")",
   "id": "a06c4e99d8d41ffc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8jzfa7lhdyc",
   "source": "### 1.1b Download & Format Strandset-Rust-v1\n\nDownloads 191K verified Rust examples from [Strandset-Rust-v1](https://huggingface.co/datasets/Fortytwo-Network/Strandset-Rust-v1) (Apache 2.0),\nparses JSON fields, and formats them in Harmony for each training stage:\n- **lang_rust**: ~130K code completion examples\n- **core_agent**: ~60K debug/review examples\n- **ipo**: ~20K preference pairs (bug_detection: fixed=chosen, buggy=rejected)\n\nMerges Strandset data into existing lang_rust and core_agent datasets.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "76jbyamzf8c",
   "source": "if CONFIG[\"skip_data_generation\"]:\n    print(\"Skipping data generation (using pre-generated data from Drive)\")\nelif CONFIG[\"training_scope\"] in (\"skip_to_rl\",):\n    print(\"Skipping — not needed for this training scope\")\nelse:\n    max_samples = 500 if CONFIG[\"training_scope\"] == \"quick_test\" else 0  # 0 = all\n\n    print(\"Downloading & formatting Strandset-Rust-v1...\")\n    print(\"=\" * 60)\n\n    cmd = \"python scripts/20_prepare_strandset.py\"\n    if max_samples:\n        cmd += f\" --max_samples {max_samples}\"\n\n    !{cmd}\n\n    # Merge Strandset data into existing lang_rust and core_agent datasets\n    from datasets import load_from_disk, concatenate_datasets\n\n    for target in [\"lang_rust/train\", \"core_agent/train\", \"ipo/train\"]:\n        strandset_path = f\"data/rust/strandset/{target}\"\n        existing_path = f\"data/rust/{target}\"\n\n        if not os.path.exists(strandset_path):\n            continue\n\n        strandset_ds = load_from_disk(strandset_path)\n\n        if os.path.exists(existing_path):\n            existing_ds = load_from_disk(existing_path)\n            print(f\"\\n  Merging {target}: {len(existing_ds):,} existing + {len(strandset_ds):,} Strandset\")\n            merged = concatenate_datasets([existing_ds, strandset_ds])\n        else:\n            print(f\"\\n  Creating {target}: {len(strandset_ds):,} Strandset examples\")\n            merged = strandset_ds\n\n        os.makedirs(existing_path, exist_ok=True)\n        merged.save_to_disk(existing_path)\n        print(f\"  Saved merged dataset: {len(merged):,} total -> {existing_path}\")\n\n    drive_helper.backup(\"data/rust/strandset\", \"data/rust/strandset\")\n    if DRIVE_MODE != \"local\":\n        print(\"\\nBacked up Strandset data to Drive.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Generate Agent Trajectories\n",
    "\n",
    "Generates multi-turn agent trajectories from mutations + Strandset in Harmony format.\n"
   ],
   "id": "fc8a2b5d9669022f"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"skip_data_generation\"]:\n",
    "    print(\"Skipping data generation (using pre-generated data from Drive)\")\n",
    "elif CONFIG[\"training_scope\"] in (\"skip_to_rl\",):\n",
    "    print(\"Skipping — not needed for this training scope\")\n",
    "else:\n",
    "    max_samples = 500 if CONFIG[\"training_scope\"] == \"quick_test\" else 5000\n",
    "\n",
    "    print(f\"Generating trajectories (max {max_samples} per source)...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    cmd = f\"python scripts/15_generate_trajectories.py --max_samples {max_samples}\"\n",
    "\n",
    "    mutations_path = \"data/rust/mutations/mutations.jsonl\"\n",
    "    if os.path.exists(mutations_path):\n",
    "        cmd += f\" --mutations_path {mutations_path}\"\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"data/rust/core_agent\", \"data/rust/core_agent\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nBacked up trajectories to Drive.\")\n"
   ],
   "id": "4ce7bebf4b7f876c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Verify Data\n"
   ],
   "id": "6de8a13dcfeedf86"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "data_checks = [\n    (\"Mutations\", \"data/rust/mutations\"),\n    (\"Strandset formatted\", \"data/rust/strandset\"),\n    (\"Lang Rust train\", \"data/rust/lang_rust/train\"),\n    (\"Core Agent train\", \"data/rust/core_agent/train\"),\n    (\"IPO train\", \"data/rust/ipo/train\"),\n    (\"GRPO tasks\", \"data/rust/grpo\"),\n    (\"Eval tasks\", \"data/rust/eval\"),\n]\n\nprint(\"Data Verification:\")\nprint(\"=\" * 60)\nfor name, path in data_checks:\n    exists = os.path.exists(path)\n    if exists and os.path.isdir(path):\n        items = os.listdir(path)\n        print(f\"  \\u2713 {name}: {path} ({len(items)} items)\")\n    elif exists:\n        size_mb = os.path.getsize(path) / (1024 * 1024)\n        print(f\"  \\u2713 {name}: {path} ({size_mb:.1f} MB)\")\n    else:\n        needed = True\n        if CONFIG[\"training_scope\"] == \"skip_to_rl\" and name in (\"Mutations\", \"Lang Rust train\", \"Core Agent train\", \"Strandset formatted\"):\n            needed = False\n        if CONFIG[\"training_scope\"] == \"lang_adapter_only\" and name in (\"IPO train\", \"GRPO tasks\"):\n            needed = False\n        sym = \"\\u2717\" if needed else \"\\u2014\"\n        label = \"MISSING\" if needed else \"not needed\"\n        print(f\"  {sym} {name}: {label}\")\nprint(\"=\" * 60)",
   "id": "861c7d3cd76a69d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Lang Adapter Training\n",
    "\n",
    "Train a QLoRA adapter (rank 64) to specialise GPT-OSS 20B on Rust syntax, stdlib, and idioms.\n",
    "Then merge the adapter into the base weights for downstream training.\n",
    "\n",
    "**v2:** Split LoRA backend auto-enabled for 7-12x faster MoE training."
   ],
   "id": "31fd053ef297615e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Train lang_rust Adapter\n",
    "\n",
    "v2: Split LoRA enabled via UNSLOTH_MOE_BACKEND env var (set in 0.5).\n"
   ],
   "id": "fd35fdde7902ddda"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "    print(\"Skipping — scope is skip_to_rl\")\n",
    "else:\n",
    "    batch = CONFIG[\"lang_rust_batch\"]\n",
    "    grad_accum = CONFIG[\"lang_rust_grad_accum\"]\n",
    "    max_steps = CONFIG[\"lang_rust_max_steps\"]\n",
    "    seq_len = CONFIG[\"lang_rust_seq_len\"]\n",
    "\n",
    "    cmd = f\"python scripts/13_train_lang_adapter.py\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    print(f\"Training lang_rust adapter...\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Seq length: {seq_len} (from config)\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/lang_rust\", \"checkpoints/lang_rust\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")\n"
   ],
   "id": "4719fb4569fd4a48",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Merge lang_rust into Base\n"
   ],
   "id": "6d945162fc6c4887"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "    print(\"Skipping — scope is skip_to_rl\")\n",
    "else:\n",
    "    print(\"Merging lang_rust adapter into base model...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/19_merge_adapter.py \\\n",
    "        --adapter_path checkpoints/lang_rust/final \\\n",
    "        --output_dir checkpoints/gpt-oss-20b-rust-merged \\\n",
    "        --export_formats hf\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/gpt-oss-20b-rust-merged\", \"checkpoints/gpt-oss-20b-rust-merged\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nMerged model backed up to Drive.\")\n"
   ],
   "id": "14dd6ab10bc01ad0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Verify Merge\n"
   ],
   "id": "8a649a98ab1ceee0"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "    print(\"Skipping \\u2014 scope is skip_to_rl\")\n",
    "else:\n",
    "    merged_path = \"checkpoints/gpt-oss-20b-rust-merged\"\n",
    "    adapter_path = \"checkpoints/lang_rust/final\"\n",
    "\n",
    "    print(\"Merge Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(merged_path):\n",
    "        files = os.listdir(merged_path)\n",
    "        safetensors = [f for f in files if f.endswith(\".safetensors\")]\n",
    "        print(f\"  \\u2713 Merged model: {merged_path}\")\n",
    "        print(f\"    {len(safetensors)} safetensors shard(s), {len(files)} total files\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 Merged model not found at {merged_path}\")\n",
    "\n",
    "    if os.path.exists(adapter_path):\n",
    "        adapter_files = os.listdir(adapter_path)\n",
    "        print(f\"  \\u2713 Adapter: {adapter_path} ({len(adapter_files)} files)\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 Adapter not found at {adapter_path}\")\n",
    "\n",
    "    if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "        print(\"\\n\\u2713 lang_adapter_only scope complete. Stopping here.\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ],
   "id": "875f898a2de17db",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Core Agent SFT\n",
    "\n",
    "Train a higher-rank LoRA adapter (rank 128) on agent trajectories with tool use.\n",
    "Uses the merged lang_rust model as the base.\n",
    "\n",
    "**v2:** Auto uncontaminated packing (3x faster, zero-config). Flex Attention for long context."
   ],
   "id": "603346a356305aad"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Train core_agent Adapter\n",
    "\n",
    "v2: Auto packing (3x faster) + Split LoRA backend enabled.\n"
   ],
   "id": "2931e91685b890ab"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] in (\"lang_adapter_only\", \"skip_to_rl\"):\n",
    "    print(f\"Skipping — scope is {CONFIG['training_scope']}\")\n",
    "else:\n",
    "    batch = CONFIG[\"core_agent_batch\"]\n",
    "    grad_accum = CONFIG[\"core_agent_grad_accum\"]\n",
    "    max_steps = CONFIG[\"core_agent_max_steps\"]\n",
    "    seq_len = CONFIG[\"core_agent_seq_len\"]\n",
    "\n",
    "    cmd = f\"python scripts/14_train_core_agent.py\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    print(f\"Training core_agent adapter...\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Seq length: {seq_len} (from config)\")\n",
    "    print(f\"  LoRA rank: 128\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(f\"  Auto packing: enabled (uncontaminated)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/core_agent\", \"checkpoints/core_agent\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")\n"
   ],
   "id": "50b1d5d150eae5d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Verify core_agent\n"
   ],
   "id": "9f890c4676329aad"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] in (\"lang_adapter_only\", \"skip_to_rl\"):\n",
    "    print(f\"Skipping \\u2014 scope is {CONFIG['training_scope']}\")\n",
    "else:\n",
    "    ckpt_path = \"checkpoints/core_agent/final\"\n",
    "\n",
    "    print(\"Core Agent Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(ckpt_path):\n",
    "        files = os.listdir(ckpt_path)\n",
    "        print(f\"  \\u2713 Checkpoint: {ckpt_path} ({len(files)} files)\")\n",
    "\n",
    "        adapter_config = os.path.join(ckpt_path, \"adapter_config.json\")\n",
    "        if os.path.exists(adapter_config):\n",
    "            import json\n",
    "            with open(adapter_config) as f:\n",
    "                cfg = json.load(f)\n",
    "            print(f\"    LoRA rank: {cfg.get('r', '?')}\")\n",
    "            print(f\"    Alpha: {cfg.get('lora_alpha', '?')}\")\n",
    "            print(f\"    Target modules: {cfg.get('target_modules', '?')}\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 Checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ],
   "id": "7935e2cb9e427223",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Preference Optimisation (IPO)\n",
    "\n",
    "Train with Identity Preference Optimisation on ranked pairs.\n",
    "Very low learning rate (5e-7), 1 epoch only to avoid collapse.\n",
    "\n",
    "**v2:** FP8 weights on H100 (60% less VRAM). Expert utilisation monitoring."
   ],
   "id": "4dba79ff4175093b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Train with IPO\n",
    "\n",
    "v2: FP8 on H100, expert utilisation monitoring, Split LoRA.\n"
   ],
   "id": "54a5adb1ec756442"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping — scope is lang_adapter_only\")\n",
    "else:\n",
    "    batch = CONFIG[\"ipo_batch\"]\n",
    "    grad_accum = CONFIG[\"ipo_grad_accum\"]\n",
    "    max_steps = CONFIG[\"ipo_max_steps\"]\n",
    "\n",
    "    if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "        ipo_checkpoint = \"checkpoints/core_agent/final\"\n",
    "        print(\"Using existing core_agent checkpoint (skip_to_rl mode)\")\n",
    "    else:\n",
    "        ipo_checkpoint = \"checkpoints/core_agent/final\"\n",
    "\n",
    "    cmd = f\"python scripts/17_ipo_preference.py\"\n",
    "    cmd += f\" --checkpoint {ipo_checkpoint}\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    print(f\"Training with IPO...\")\n",
    "    print(f\"  Checkpoint: {ipo_checkpoint}\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Loss: IPO (beta=0.1)\")\n",
    "    print(f\"  Load mode: {CONFIG['load_mode']}\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/core_agent_ipo\", \"checkpoints/core_agent_ipo\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")\n"
   ],
   "id": "d1401e64737e0b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Verify IPO\n"
   ],
   "id": "442df61074a8cd86"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    ckpt_path = \"checkpoints/core_agent_ipo/final\"\n",
    "\n",
    "    print(\"IPO Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(ckpt_path):\n",
    "        files = os.listdir(ckpt_path)\n",
    "        print(f\"  \\u2713 IPO checkpoint: {ckpt_path} ({len(files)} files)\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 IPO checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "    # Check tensorboard logs for KL divergence\n",
    "    tb_dir = \"checkpoints/core_agent_ipo\"\n",
    "    tb_files = []\n",
    "    if os.path.exists(tb_dir):\n",
    "        for root, dirs, fnames in os.walk(tb_dir):\n",
    "            for fn in fnames:\n",
    "                if fn.startswith(\"events.out.tfevents\"):\n",
    "                    tb_files.append(os.path.join(root, fn))\n",
    "    if tb_files:\n",
    "        print(f\"  \\u2713 TensorBoard logs found ({len(tb_files)} event files)\")\n",
    "        print(f\"    Monitor KL divergence: warn >0.3, abort >0.5\")\n",
    "    else:\n",
    "        print(f\"  \\u2014 No TensorBoard logs found\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ],
   "id": "d83944ef11775585",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: GRPO RL\n",
    "\n",
    "Group Relative Policy Optimisation with execution-based rewards.\n",
    "Generates N completions per prompt, runs `cargo check/test/clippy`, computes group-relative advantages.\n",
    "\n",
    "**v2 Optimisations:**\n",
    "- FP8 RL with vLLM inference on H100 (1.6x throughput)\n",
    "- Chunked batching for 7x longer context\n",
    "- Extended curriculum: 65K context on H100 (up from 32K)\n",
    "- Harmony format compliance reward to prevent infinite reasoning loops\n",
    "\n",
    "**This step is optional** (`include_grpo=False` to skip)."
   ],
   "id": "a10e7f0a4032c87e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Train with GRPO\n",
    "\n",
    "v2: FP8 RL + vLLM (H100), chunked batching, extended curriculum.\n"
   ],
   "id": "640c22f7ecce99e7"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping — scope is lang_adapter_only\")\n",
    "elif not CONFIG[\"include_grpo\"]:\n",
    "    print(\"Skipping — GRPO disabled (include_grpo=False)\")\n",
    "else:\n",
    "    batch = CONFIG[\"grpo_batch\"]\n",
    "    grad_accum = CONFIG[\"grpo_grad_accum\"]\n",
    "    max_steps = CONFIG[\"grpo_max_steps\"]\n",
    "    max_seq = CONFIG[\"grpo_seq_len\"]\n",
    "\n",
    "    grpo_checkpoint = \"checkpoints/core_agent_ipo/final\"\n",
    "\n",
    "    cmd = f\"python scripts/18_grpo_rl.py\"\n",
    "    cmd += f\" --checkpoint {grpo_checkpoint}\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    # v2: Note which optimisations are active\n",
    "    v2_features = []\n",
    "    v2_features.append(f\"Split LoRA ({CONFIG['moe_backend']})\")\n",
    "    if CONFIG[\"load_mode\"] == \"fp8\":\n",
    "        v2_features.append(\"FP8 weights\")\n",
    "    if CONFIG[\"fast_inference\"]:\n",
    "        v2_features.append(\"vLLM inference\")\n",
    "    v2_features.append(\"Chunked batching (auto)\")\n",
    "    v2_features.append(\"Auto packing\")\n",
    "\n",
    "    if CONFIG[\"gpu_tier\"] == \"a100_40gb\":\n",
    "        print(\"NOTE: 40GB GPU — GRPO sequence length capped at 16384\")\n",
    "\n",
    "    print(f\"Training with GRPO (v2)...\")\n",
    "    print(f\"  Checkpoint: {grpo_checkpoint}\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Max seq length: {max_seq}\")\n",
    "    print(f\"  Generations per prompt: {CONFIG['grpo_num_gen']}\")\n",
    "    print(f\"\\n  v2 features active:\")\n",
    "    for feat in v2_features:\n",
    "        print(f\"    ✓ {feat}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/core_agent_grpo\", \"checkpoints/core_agent_grpo\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")\n"
   ],
   "id": "ced528c03911d4d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Verify GRPO\n"
   ],
   "id": "34e15c6770525e08"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "elif not CONFIG[\"include_grpo\"]:\n",
    "    print(\"Skipping \\u2014 GRPO disabled\")\n",
    "else:\n",
    "    ckpt_path = \"checkpoints/core_agent_grpo/final\"\n",
    "\n",
    "    print(\"GRPO Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(ckpt_path):\n",
    "        files = os.listdir(ckpt_path)\n",
    "        print(f\"  \\u2713 GRPO checkpoint: {ckpt_path} ({len(files)} files)\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 GRPO checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ],
   "id": "1f051b9601545463",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Evaluation\n",
    "\n",
    "Evaluate the best checkpoint on held-out Rust tasks using execution-based metrics\n",
    "(cargo check, cargo test, clippy)."
   ],
   "id": "c3395dfa3ecbc680"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Run Rust Evaluation\n"
   ],
   "id": "1fb7069b4d6fddef"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping — scope is lang_adapter_only\")\n",
    "else:\n",
    "    # Determine best checkpoint\n",
    "    if CONFIG[\"include_grpo\"] and os.path.exists(\"checkpoints/core_agent_grpo/final\"):\n",
    "        eval_checkpoint = \"checkpoints/core_agent_grpo/final\"\n",
    "    elif os.path.exists(\"checkpoints/core_agent_ipo/final\"):\n",
    "        eval_checkpoint = \"checkpoints/core_agent_ipo/final\"\n",
    "    elif os.path.exists(\"checkpoints/core_agent/final\"):\n",
    "        eval_checkpoint = \"checkpoints/core_agent/final\"\n",
    "    else:\n",
    "        eval_checkpoint = \"checkpoints/core_agent_ipo/final\"\n",
    "\n",
    "    num_samples = CONFIG[\"eval_num_samples\"]\n",
    "\n",
    "    print(f\"Evaluating checkpoint: {eval_checkpoint}\")\n",
    "    print(f\"Samples: {num_samples}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/eval_rust_agent.py \\\n",
    "        --checkpoint {eval_checkpoint} \\\n",
    "        --num_samples {num_samples}\n",
    "\n",
    "    drive_helper.backup(\"evals/rust_agent\", \"evals/rust_agent\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nResults backed up to Drive.\")\n"
   ],
   "id": "bad0cdf0e5a54cb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Check Promotion Gates\n"
   ],
   "id": "b0a563f4effe713f"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    print(\"Checking promotion gates...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/12_check_gates.py rust_agent"
   ],
   "id": "4197d776a316d2cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Display Results\n"
   ],
   "id": "d213ef309b2bacd7"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    import json\n",
    "\n",
    "    metrics_path = \"evals/rust_agent/metrics.json\"\n",
    "\n",
    "    if os.path.exists(metrics_path):\n",
    "        with open(metrics_path) as f:\n",
    "            metrics = json.load(f)\n",
    "\n",
    "        targets = {\n",
    "            \"cargo_check_pass_rate\": (0.85, \"higher\"),\n",
    "            \"cargo_test_pass_rate\": (0.70, \"higher\"),\n",
    "            \"clippy_clean_rate\": (0.80, \"higher\"),\n",
    "            \"iterations_to_green_median\": (3, \"lower\"),\n",
    "            \"diff_size_median\": (50, \"lower\"),\n",
    "            \"tool_call_format_accuracy\": (0.99, \"higher\"),\n",
    "            \"hallucinated_api_rate\": (0.05, \"lower\"),\n",
    "        }\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "        print(\"EVALUATION RESULTS\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"{'Metric':<32} {'Value':>8} {'Target':>8} {'Status':>8}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        for key, (target, direction) in targets.items():\n",
    "            value = metrics.get(key)\n",
    "            if value is None:\n",
    "                print(f\"{key:<32} {'N/A':>8} {target:>8} {'\\u2014':>8}\")\n",
    "                continue\n",
    "\n",
    "            if direction == \"higher\":\n",
    "                passed = value >= target\n",
    "            else:\n",
    "                passed = value <= target\n",
    "\n",
    "            status = \"\\u2713 PASS\" if passed else \"\\u2717 FAIL\"\n",
    "            fmt_val = f\"{value:.2%}\" if isinstance(value, float) and value <= 1 else f\"{value}\"\n",
    "            fmt_tgt = f\"{target:.0%}\" if isinstance(target, float) and target <= 1 else f\"{target}\"\n",
    "            print(f\"{key:<32} {fmt_val:>8} {fmt_tgt:>8} {status:>8}\")\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "    else:\n",
    "        print(f\"\\u2717 Metrics file not found at {metrics_path}\")\n",
    "        print(\"Run evaluation (6.1) first.\")"
   ],
   "id": "6ff37a75b3dd990f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Test Model\n",
    "\n",
    "Load the trained model and generate Rust code interactively.\n",
    "\n",
    "**v2:** FP8 loading on H100 for faster inference. `fast_inference=True` enables vLLM backend."
   ],
   "id": "856732da9a5a7910"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Load Model\n",
    "\n",
    "v2: FP8 loading on H100, vLLM-backed inference.\n"
   ],
   "id": "454dd084fa7f33d0"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "CHECKPOINT_PRIORITY = [\n",
    "    \"checkpoints/core_agent_grpo/final\",\n",
    "    \"checkpoints/core_agent_ipo/final\",\n",
    "    \"checkpoints/core_agent/final\",\n",
    "    \"checkpoints/gpt-oss-20b-rust-merged\",\n",
    "]\n",
    "\n",
    "MODEL_PATH = None\n",
    "for path in CHECKPOINT_PRIORITY:\n",
    "    if os.path.exists(path):\n",
    "        MODEL_PATH = path\n",
    "        break\n",
    "\n",
    "if MODEL_PATH is None:\n",
    "    print(\"\\u2717 No checkpoint found. Train the model first.\")\n",
    "else:\n",
    "    print(f\"Loading model from: {MODEL_PATH}\")\n",
    "\n",
    "    # v2: Use FP8 on H100, 4-bit otherwise\n",
    "    load_kwargs = {\n",
    "        \"max_seq_length\": 4096,\n",
    "        \"dtype\": torch.bfloat16,\n",
    "    }\n",
    "    if CONFIG.get(\"load_mode\") == \"fp8\" and CONFIG.get(\"use_fp8\"):\n",
    "        load_kwargs[\"load_in_fp8\"] = True\n",
    "        print(\"  Mode: FP8 (H100)\")\n",
    "    else:\n",
    "        load_kwargs[\"load_in_4bit\"] = True\n",
    "        print(\"  Mode: 4-bit QLoRA\")\n",
    "\n",
    "    if CONFIG.get(\"fast_inference\"):\n",
    "        load_kwargs[\"fast_inference\"] = True\n",
    "        print(\"  Inference: vLLM backend\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(MODEL_PATH, **load_kwargs)\n",
    "    FastLanguageModel.for_inference(model)\n",
    "\n",
    "    print(\"\\u2713 Model loaded!\")"
   ],
   "id": "58e85ca44306ad95",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Generate Rust Code\n",
    "\n",
    "Tests the model on 3 pre-defined Rust prompts using Harmony format.\n"
   ],
   "id": "2a8c82c63929e765"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"scripts\")\n",
    "from dataset_formatters.harmony import encode_harmony_messages\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    \"Write a Rust function `fn merge_sorted(a: &[i32], b: &[i32]) -> Vec<i32>` that merges two sorted slices into a single sorted vector.\",\n",
    "    \"This Rust code fails the borrow checker. Fix it:\\n```rust\\nfn main() {\\n    let mut v = vec![1, 2, 3];\\n    let first = &v[0];\\n    v.push(4);\\n    println!(\\\"{}\\\", first);\\n}\\n```\",\n",
    "    \"Write an async Rust function using tokio that fetches a URL with reqwest, retries up to 3 times on failure, and returns the response body as a String.\",\n",
    "]\n",
    "\n",
    "def generate_rust(prompt, max_tokens=1024):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted = encode_harmony_messages(\n",
    "        messages,\n",
    "        developer_instructions=\"You are a Rust programming expert. Write correct, idiomatic code.\",\n",
    "    )\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.3,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "for i, prompt in enumerate(TEST_PROMPTS, 1):\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Test {i}: {prompt[:80]}...\")\n",
    "    print(\"=\" * 60)\n",
    "    response = generate_rust(prompt)\n",
    "    print(response)\n",
    "    print()"
   ],
   "id": "fe79b1c5cd8fa9af",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Custom Prompt\n"
   ],
   "id": "f26961ac0a874651"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "CUSTOM_PROMPT = \"Write a Rust function that reads a CSV file and returns the sum of a specified column.\"\n",
    "\n",
    "print(f\"Prompt: {CUSTOM_PROMPT}\")\n",
    "print(\"=\" * 60)\n",
    "print(generate_rust(CUSTOM_PROMPT))"
   ],
   "id": "4b4b7f32818a3d05",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Export\n",
    "\n",
    "Merge the final adapter and export to HuggingFace + GGUF formats.\n",
    "\n",
    "**v2:** Optional QAT export for 97-100% MXFP4 quality retention (vs 59-89% with PTQ)."
   ],
   "id": "2cf19b6ed97d019e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Export to GGUF\n",
    "\n",
    "Merges the best adapter and exports as HF safetensors + GGUF Q4.\n"
   ],
   "id": "91788a1701ee9837"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ADAPTER_PRIORITY = [\n",
    "    \"checkpoints/core_agent_grpo/final\",\n",
    "    \"checkpoints/core_agent_ipo/final\",\n",
    "    \"checkpoints/core_agent/final\",\n",
    "    \"checkpoints/lang_rust/final\",\n",
    "]\n",
    "\n",
    "adapter_path = None\n",
    "for path in ADAPTER_PRIORITY:\n",
    "    if os.path.exists(path):\n",
    "        adapter_path = path\n",
    "        break\n",
    "\n",
    "if adapter_path is None:\n",
    "    print(\"✗ No adapter checkpoint found.\")\n",
    "else:\n",
    "    export_dir = \"checkpoints/gpt-oss-20b-rust-export-v2\"\n",
    "    print(f\"Exporting adapter: {adapter_path}\")\n",
    "    print(f\"Output: {export_dir}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/19_merge_adapter.py \\\n",
    "        --adapter_path {adapter_path} \\\n",
    "        --output_dir {export_dir} \\\n",
    "        --export_formats hf gguf_q4\n",
    "\n",
    "    drive_helper.backup(export_dir, \"checkpoints/gpt-oss-20b-rust-export-v2\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nExport backed up to Drive.\")\n"
   ],
   "id": "d1b8a168f48fae53",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 QAT Export (Optional)\n",
    "\n",
    "v2: Quantisation-Aware Training for MXFP4 deployment.\n",
    "Recovers 97-100% quality vs 59-89% with post-training quantisation.\n",
    "Requires: `pip install nvidia-modelopt`\n"
   ],
   "id": "c632af3d6ca89abd"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if not CONFIG.get(\"enable_qat_export\"):\n",
    "    print(\"QAT export disabled. Enable via enable_qat_export=True in Step 0.3.\")\n",
    "    print(\"\\nQAT recovers 97-100% quality when deploying to MXFP4,\")\n",
    "    print(\"vs 59-89% with standard post-training quantisation (PTQ).\")\n",
    "else:\n",
    "    export_dir = \"checkpoints/gpt-oss-20b-rust-export-v2\"\n",
    "    qat_dir = \"checkpoints/gpt-oss-20b-rust-qat\"\n",
    "\n",
    "    if not os.path.exists(export_dir):\n",
    "        print(\"\\u2717 Run standard export (8.1) first.\")\n",
    "    else:\n",
    "        print(\"Running QAT pass on merged model...\")\n",
    "        print(\"  This fine-tunes with MXFP4-aware quantisation at reduced LR (1e-5).\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        try:\n",
    "            import modelopt.torch.quantization as mtq\n",
    "            print(\"\\u2713 nvidia-modelopt available\")\n",
    "\n",
    "            # QAT would be run here via mtq.quantize()\n",
    "            # For now, document the expected command:\n",
    "            print(\"\\nQAT pipeline (manual steps):\")\n",
    "            print(f\"  1. Load merged BF16 model from {export_dir}\")\n",
    "            print(f\"  2. mtq.quantize(model, config=mtq.MXFP4_DEFAULT_CFG)\")\n",
    "            print(f\"  3. Fine-tune for ~100 steps at LR 1e-5\")\n",
    "            print(f\"  4. Export to {qat_dir}\")\n",
    "        except ImportError:\n",
    "            print(\"\\u2717 nvidia-modelopt not installed.\")\n",
    "            print(\"  Install: pip install nvidia-modelopt\")\n",
    "            print(\"  See: https://developer.nvidia.com/blog/fine-tuning-gpt-oss-for-accuracy-and-performance-with-quantization-aware-training/\")"
   ],
   "id": "684742559682938a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Download GGUF\n"
   ],
   "id": "88ce6eb89550c5d9"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    import glob\n",
    "\n",
    "    export_dir = \"checkpoints/gpt-oss-20b-rust-export-v2\"\n",
    "    gguf_files = glob.glob(os.path.join(export_dir, \"*.gguf\"))\n",
    "\n",
    "    if gguf_files:\n",
    "        gguf_path = gguf_files[0]\n",
    "        size_gb = os.path.getsize(gguf_path) / (1024**3)\n",
    "        print(f\"Downloading: {os.path.basename(gguf_path)} ({size_gb:.1f} GB)\")\n",
    "        files.download(gguf_path)\n",
    "    else:\n",
    "        print(\"\\u2717 No GGUF file found. Run export (8.1) first.\")\n",
    "else:\n",
    "    print(\"Download not available outside Colab.\")\n",
    "    print(\"GGUF file is at: checkpoints/gpt-oss-20b-rust-export-v2/\")"
   ],
   "id": "1240d8a590699baf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Complete!\n",
    "\n",
    "Your GPT-OSS 20B Rust coding agent (v2) is trained and ready to use.\n",
    "\n",
    "**v2 Optimisations Applied:**\n",
    "- Split LoRA: 7-12x faster MoE training\n",
    "- FP8 RL: 1.6x throughput on H100 (60% less VRAM)\n",
    "- Auto packing: 3x faster SFT\n",
    "- Chunked GRPO: 65K context on H100 (up from 32K)\n",
    "- QAT export: 97-100% MXFP4 quality (if enabled)\n",
    "\n",
    "**Outputs:**\n",
    "- Checkpoints: `checkpoints/core_agent_{ipo,grpo}/final`\n",
    "- Evaluation: `evals/rust_agent/metrics.json`\n",
    "- Exported model: `checkpoints/gpt-oss-20b-rust-export-v2/`\n",
    "- All backed up to Google Drive: `gpt-oss-20b-rust-agent-v2/`\n",
    "\n",
    "**Next steps:**\n",
    "- Review evaluation metrics in Step 6.3\n",
    "- Test interactively in Step 7\n",
    "- Deploy the GGUF file with llama.cpp or Ollama\n",
    "- For MXFP4 deployment, enable QAT export in Step 8.2\n",
    "\n",
    "**References:**\n",
    "- [V2 Optimization Plan](../docs/V2_OPTIMIZATION_PLAN.md)\n",
    "- [Unsloth Split LoRA](https://unsloth.ai/docs/new/faster-moe)\n",
    "- [Unsloth FP8 RL](https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/fp8-reinforcement-learning)\n",
    "- [NVIDIA QAT for GPT-OSS](https://developer.nvidia.com/blog/fine-tuning-gpt-oss-for-accuracy-and-performance-with-quantization-aware-training/)"
   ],
   "id": "f2381c249430c4d6"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}