{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GPT-OSS 20B → Rust Coding Agent (v2)\n",
    "\n",
    "End-to-end pipeline for training a Rust coding agent on OpenAI's GPT-OSS 20B (MoE, ~3.6B active params).\n",
    "\n",
    "**v2 Optimisations** (see `docs/V2_OPTIMIZATION_PLAN.md`):\n",
    "- **Split LoRA** — 7-12x faster MoE training via reordered LoRA computation\n",
    "- **FP8 RL** — 1.6x throughput, 60% less VRAM on H100 (auto-fallback to 4-bit on A100)\n",
    "- **GRPO long context** — Chunked batching enables 65K+ context (up from 32K)\n",
    "- **Flex Attention** — 8x longer sequences with attention sinks\n",
    "- **Auto packing** — 3x faster SFT with uncontaminated packing (zero-config)\n",
    "- **Expert monitoring** — Routing utilisation tracking across all phases\n",
    "- **QAT export** — 97-100% MXFP4 quality retention (vs 59-89% with PTQ)\n",
    "\n",
    "**4-Phase Pipeline:**\n",
    "1. **Lang Adapter** — Rust domain specialisation via QLoRA (script 13 + 19)\n",
    "2. **Core Agent SFT** — Agent trajectory training with tool use (script 14)\n",
    "3. **IPO Preference** — Identity Preference Optimisation on ranked pairs (script 17)\n",
    "4. **GRPO RL** — Group Relative Policy Optimisation with execution rewards (script 18)\n",
    "\n",
    "**Requirements:**\n",
    "- **GPU**: A100 40GB+ (H100 80GB recommended for FP8 + extended context)\n",
    "- **Storage**: Google Drive for persistent checkpoints\n",
    "- **Rust toolchain**: Installed automatically (rustup + cargo-mutants)"
   ],
   "id": "4426faf4d8d7ae35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Environment Setup"
   ],
   "id": "c68353567587459c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Mount Google Drive & Clone Repository\n",
    "\n",
    "**PyCharm / headless users:** If `drive.mount()` doesn't work (e.g. PyCharm Colab\n",
    "plugin can't relay the OAuth popup), set `use_service_account = True` and provide\n",
    "your service-account JSON key path in Step 0.3.\n"
   ],
   "id": "97b1ca5318fc7ab0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T07:14:32.313720Z",
     "start_time": "2026-02-12T07:14:31.494260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "use_service_account = True\n",
    "\n",
    "DRIVE_MOUNTED = False\n",
    "\n",
    "if IN_COLAB and not use_service_account:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        DRIVE_MOUNTED = True\n",
    "        print(\"Google Drive mounted\")\n",
    "    except Exception as e:\n",
    "        print(f\"drive.mount() failed: {e}\")\n",
    "        print(\"Falling back to local-only mode.\")\n",
    "        print(\"Tip: set use_service_account=True and provide a JSON key in Step 0.3.\")\n",
    "elif IN_COLAB and use_service_account:\n",
    "    print(\"Service-account mode selected — skipping drive.mount()\")\n",
    "    print(\"Configure credentials in Step 0.3.\")\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "\n",
    "REPO_URL = \"https://github.com/rmarnold/llm-training-pipeline.git\"\n",
    "BRANCH = \"main\"\n",
    "\n",
    "REPO_DIR = \"/content/llm-training-pipeline\"\n",
    "\n",
    "if IN_COLAB:\n",
    "    if os.path.exists(REPO_DIR):\n",
    "        %cd {REPO_DIR}\n",
    "        !git pull origin {BRANCH}\n",
    "    else:\n",
    "        !git clone -b {BRANCH} {REPO_URL} {REPO_DIR}\n",
    "        %cd {REPO_DIR}\n",
    "\n",
    "    PROJECT_ROOT = REPO_DIR\n",
    "else:\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"\\nProject root: {PROJECT_ROOT}\")\n"
   ],
   "id": "604e923187b87e5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service-account mode selected — skipping drive.mount()\n",
      "Configure credentials in Step 0.3.\n",
      "/content/llm-training-pipeline\n",
      "From https://github.com/rmarnold/llm-training-pipeline\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Already up to date.\n",
      "\n",
      "Project root: /content/llm-training-pipeline\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Install Dependencies\n",
    "\n",
    "Installs pipeline deps, latest Unsloth (with Split LoRA + FP8 RL), and the Rust toolchain.\n",
    "\n",
    "**Note:** flash-attn is intentionally NOT installed. FA3 is incompatible with GPT-OSS\n",
    "backward passes (incorrect training loss). Unsloth's Flex Attention replaces it\n",
    "automatically — no compilation step needed.\n"
   ],
   "id": "4f8d5ba0fcc56075"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T07:14:15.318325Z",
     "start_time": "2026-02-12T07:13:40.246621Z"
    }
   },
   "source": [
    "if IN_COLAB:\n",
    "    print(\"Installing Python dependencies...\")\n",
    "    print(\"=\" * 60)\n",
    "    !pip install -q -e \".[gpt_oss,rust_eval,colab]\"\n",
    "\n",
    "    # Fix pyarrow binary incompatibility with datasets 4.x on Colab\n",
    "    # (Colab's pre-installed pyarrow C extension doesn't match the new header)\n",
    "    !pip install -q --force-reinstall pyarrow\n",
    "\n",
    "    # v2: Force latest Unsloth with Split LoRA + FP8 RL + GRPO long context\n",
    "    # Flex Attention (bundled with Unsloth) replaces Flash Attention for GPT-OSS\n",
    "    print(\"\\nInstalling latest Unsloth (Split LoRA + FP8 RL + Flex Attention)...\")\n",
    "    !pip install -q --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo\n",
    "    !pip install -q \"unsloth[colab-new]\"\n",
    "\n",
    "    # v2: vLLM for FP8 RL inference (H100 only, optional)\n",
    "    !pip install -q vllm>=0.12.0 2>/dev/null || true\n",
    "\n",
    "    print(\"\\nInstalling Rust toolchain...\")\n",
    "    print(\"=\" * 60)\n",
    "    !curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
    "    os.environ[\"PATH\"] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "    !cargo install cargo-mutants\n",
    "\n",
    "    # Verification\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Dependency Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for pkg in [\"unsloth\", \"trl\", \"peft\", \"datasets\", \"tiktoken\", \"vllm\"]:\n",
    "        try:\n",
    "            mod = __import__(pkg)\n",
    "            ver = getattr(mod, \"__version__\", \"OK\")\n",
    "            print(f\"\\u2713 {pkg}: {ver}\")\n",
    "        except ImportError as e:\n",
    "            if pkg == \"vllm\":\n",
    "                print(f\"\\u2014 {pkg}: not installed (optional, H100 FP8 RL only)\")\n",
    "            else:\n",
    "                print(f\"\\u2717 {pkg}: {e}\")\n",
    "\n",
    "    import subprocess\n",
    "    for cmd, label in [(\"cargo --version\", \"cargo\"), (\"cargo mutants --version\", \"cargo-mutants\")]:\n",
    "        result = subprocess.run(cmd.split(), capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"\\u2713 {label}: {result.stdout.strip()}\")\n",
    "        else:\n",
    "            print(f\"\\u2717 {label}: not found\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"Running locally \\u2014 ensure deps are installed:\")\n",
    "    print(\"  pip install -e '.[gpt_oss,rust_eval]'\")\n",
    "    print(\"  pip install --upgrade unsloth unsloth_zoo\")\n"
   ],
   "id": "fb54d828abbdfd4b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing Python dependencies...\n",
      "============================================================\n",
      "  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build editable ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "  Building editable for llm-training-pipeline (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "\u001B[31mERROR: Operation cancelled by user\u001B[0m\u001B[31m\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33m    WARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\n",
      "Installing latest Unsloth (Split LoRA + FP8 RL + Flex Attention)...\n",
      "\u001B[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m69.7/69.7 kB\u001B[0m \u001B[31m68.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m432.3/432.3 kB\u001B[0m \u001B[31m467.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m376.5/376.5 kB\u001B[0m \u001B[31m569.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h\u001B[33m    WARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33m    WARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: unsloth 2026.2.1 does not provide the extra 'triton'\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33m    WARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33m    WARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0mTraceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
      "    return func(self, options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 455, in run\n",
      "    installed = install_given_reqs(\n",
      "                ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/req/__init__.py\", line 70, in install_given_reqs\n",
      "    requirement.install(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/req/req_install.py\", line 851, in install\n",
      "    install_wheel(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/install/wheel.py\", line 726, in install_wheel\n",
      "    _install_wheel(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/install/wheel.py\", line 585, in _install_wheel\n",
      "    record_installed(file.src_record_path, file.dest_path, file.changed)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/install/wheel.py\", line 468, in record_installed\n",
      "    newpath = _fs_to_record_path(destfile, lib_dir)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/install/wheel.py\", line 235, in _fs_to_record_path\n",
      "    path = os.path.relpath(path, lib_dir)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen posixpath>\", line 534, in relpath\n",
      "  File \"<frozen posixpath>\", line 71, in join\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
      "    return command.main(cmd_args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
      "    return self._main(args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
      "    return run(options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
      "    logger.critical(\"Operation cancelled by user\")\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1586, in critical\n",
      "    self._log(CRITICAL, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1684, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1700, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1762, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1028, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/rich/console.py\", line 1673, in print\n",
      "    with self:\n",
      "         ^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/rich/console.py\", line 865, in __exit__\n",
      "    self._exit_buffer()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/rich/console.py\", line 823, in _exit_buffer\n",
      "    self._check_buffer()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/rich/console.py\", line 2058, in _check_buffer\n",
      "    text = self._render_buffer(self._buffer[:])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/rich/console.py\", line 2074, in _render_buffer\n",
      "    not_terminal = not self.is_terminal\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/rich/console.py\", line 960, in is_terminal\n",
      "    return False if isatty is None else isatty()\n",
      "                                        ^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipython-input-279411094.py\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     12\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"\\nInstalling latest Unsloth (Split LoRA + FP8 RL + Flex Attention)...\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[0mget_ipython\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msystem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'pip install -q --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m     \u001B[0mget_ipython\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msystem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'pip install -q \"unsloth[colab-new]\"'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m     \u001B[0;31m# v2: vLLM for FP8 RL inference (H100 only, optional)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001B[0m in \u001B[0;36msystem\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    151\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mpip_warn\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 152\u001B[0;31m       \u001B[0m_pip\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprint_previous_import_warning\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    153\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    154\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_send_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexc_content\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001B[0m in \u001B[0;36mprint_previous_import_warning\u001B[0;34m(output)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mprint_previous_import_warning\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     55\u001B[0m   \u001B[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 56\u001B[0;31m   \u001B[0mpackages\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_previously_imported_packages\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     57\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0mpackages\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     58\u001B[0m     \u001B[0;31m# display a list of packages using the colab-display-data mimetype, which\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001B[0m in \u001B[0;36m_previously_imported_packages\u001B[0;34m(pip_output)\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0m_previously_imported_packages\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpip_output\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     49\u001B[0m   \u001B[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 50\u001B[0;31m   \u001B[0minstalled\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_extract_toplevel_packages\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpip_output\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     51\u001B[0m   \u001B[0;32mreturn\u001B[0m \u001B[0msorted\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minstalled\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mintersection\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msys\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodules\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001B[0m in \u001B[0;36m_extract_toplevel_packages\u001B[0;34m(pip_output)\u001B[0m\n\u001B[1;32m     37\u001B[0m   \u001B[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m   \u001B[0mtoplevel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcollections\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdefaultdict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 39\u001B[0;31m   \u001B[0;32mfor\u001B[0m \u001B[0mm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mps\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mimportlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmetadata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpackages_distributions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     40\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mp\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mps\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m       \u001B[0mtoplevel\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mp\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mm\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001B[0m in \u001B[0;36mpackages_distributions\u001B[0;34m()\u001B[0m\n\u001B[1;32m    945\u001B[0m     \u001B[0mpkg_to_dist\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcollections\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdefaultdict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    946\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mdist\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdistributions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 947\u001B[0;31m         \u001B[0;32mfor\u001B[0m \u001B[0mpkg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0m_top_level_declared\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdist\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_top_level_inferred\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdist\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    948\u001B[0m             \u001B[0mpkg_to_dist\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpkg\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdist\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmetadata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'Name'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    949\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpkg_to_dist\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001B[0m in \u001B[0;36m_top_level_inferred\u001B[0;34m(dist)\u001B[0m\n\u001B[1;32m    957\u001B[0m     opt_names = {\n\u001B[1;32m    958\u001B[0m         \u001B[0mf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparts\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparts\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0minspect\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetmodulename\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 959\u001B[0;31m         \u001B[0;32mfor\u001B[0m \u001B[0mf\u001B[0m \u001B[0;32min\u001B[0m \u001B[0malways_iterable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdist\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfiles\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    960\u001B[0m     }\n\u001B[1;32m    961\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/importlib_metadata/__init__.py\u001B[0m in \u001B[0;36mfiles\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    602\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlocate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexists\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpackage_paths\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    603\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 604\u001B[0;31m         return skip_missing_files(\n\u001B[0m\u001B[1;32m    605\u001B[0m             make_files(\n\u001B[1;32m    606\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_read_files_distinfo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/importlib_metadata/_functools.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(param, *args, **kwargs)\u001B[0m\n\u001B[1;32m    101\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparam\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    102\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mparam\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 103\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparam\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    104\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    105\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/importlib_metadata/__init__.py\u001B[0m in \u001B[0;36mskip_missing_files\u001B[0;34m(package_paths)\u001B[0m\n\u001B[1;32m    600\u001B[0m         \u001B[0;34m@\u001B[0m\u001B[0mpass_none\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    601\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mskip_missing_files\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpackage_paths\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 602\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlocate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexists\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpackage_paths\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    603\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    604\u001B[0m         return skip_missing_files(\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/importlib_metadata/__init__.py\u001B[0m in \u001B[0;36m<lambda>\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m    600\u001B[0m         \u001B[0;34m@\u001B[0m\u001B[0mpass_none\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    601\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mskip_missing_files\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpackage_paths\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 602\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlocate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexists\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpackage_paths\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    603\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    604\u001B[0m         return skip_missing_files(\n",
      "\u001B[0;32m/usr/lib/python3.12/pathlib.py\u001B[0m in \u001B[0;36mexists\u001B[0;34m(self, follow_symlinks)\u001B[0m\n\u001B[1;32m    858\u001B[0m         \"\"\"\n\u001B[1;32m    859\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 860\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfollow_symlinks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfollow_symlinks\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    861\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mOSError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    862\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0m_ignore_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.12/pathlib.py\u001B[0m in \u001B[0;36mstat\u001B[0;34m(self, follow_symlinks)\u001B[0m\n\u001B[1;32m    838\u001B[0m         \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0mdoes\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    839\u001B[0m         \"\"\"\n\u001B[0;32m--> 840\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfollow_symlinks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfollow_symlinks\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    841\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    842\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mlstat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Configure Pipeline\n",
    "\n",
    "Edit the variables below to configure the training run.\n",
    "\n",
    "**Training Scope** (`training_scope`):\n",
    "- `full` — All 4 phases end-to-end\n",
    "- `quick_test` — Short runs (100 steps each) to verify setup\n",
    "- `lang_adapter_only` — Only train lang_rust adapter + merge\n",
    "- `skip_to_rl` — Start from existing core_agent checkpoint (IPO + GRPO only)\n",
    "\n",
    "**Other settings:**\n",
    "- `gpu_tier` — Auto-detected below; override if needed\n",
    "- `max_steps_override` — Set >0 to cap all training stages (0 = use defaults)\n",
    "- `skip_data_generation` — Use pre-generated data from Drive\n",
    "- `include_grpo` — GRPO RL is slow; set `False` to skip\n",
    "- `enable_qat_export` — v2: QAT for MXFP4 export (97-100% quality vs 59-89% PTQ)\n",
    "\n",
    "**Service Account** (PyCharm / headless):\n",
    "- `service_account_key` — Path to service-account JSON key file (empty = browser auth)\n",
    "- `drive_folder_id` — Google Drive folder ID for `gpt-oss-20b-rust-agent-v2`\n"
   ],
   "id": "3d85f6b5a5ecf6cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "training_scope = \"quick_test\"  # \"full\", \"quick_test\", \"lang_adapter_only\", \"skip_to_rl\"\n",
    "\n",
    "gpu_tier = \"h100_80gb\"  # \"a100_40gb\", \"a100_80gb\", \"h100_80gb\"\n",
    "\n",
    "max_steps_override = 0  # Set >0 to cap all stages (0 = use defaults)\n",
    "\n",
    "skip_data_generation = False  # True to use pre-generated data from Drive\n",
    "\n",
    "include_grpo = True  # False to skip GRPO RL (slow)\n",
    "\n",
    "enable_qat_export = False  # True for MXFP4 QAT export\n",
    "\n",
    "\n",
    "service_account_key = \"/Users/robertarnold/Documents/gen-lang-client-0561194467-615037e1cde0.json\"  # Path to service-account JSON key (PyCharm/headless)\n",
    "\n",
    "drive_folder_id = \"\"  # Google Drive folder ID for gpt-oss-20b-rust-agent-v2\n",
    "\n",
    "# ============================================================\n",
    "# DRIVE MODE\n",
    "# ============================================================\n",
    "from scripts.pipeline_lib.drive_utils import DriveHelper\n",
    "\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/gpt-oss-20b-rust-agent-v2\"\n",
    "\n",
    "if DRIVE_MOUNTED:\n",
    "    DRIVE_MODE = \"mounted\"\n",
    "elif use_service_account and service_account_key and drive_folder_id:\n",
    "    DRIVE_MODE = \"service_account\"\n",
    "else:\n",
    "    DRIVE_MODE = \"local\"\n",
    "\n",
    "drive_helper = DriveHelper(\n",
    "    mode=DRIVE_MODE,\n",
    "    drive_base=DRIVE_BASE,\n",
    "    credentials_path=service_account_key or None,\n",
    "    folder_id=drive_folder_id or None,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# v2 GPU TIER CONFIGS (with H100 FP8 tier)\n",
    "# ============================================================\n",
    "\n",
    "GPU_CONFIGS = {\n",
    "    \"a100_40gb\": {\n",
    "        \"moe_backend\": \"unsloth_triton\",\n",
    "        \"load_mode\": \"4bit\",\n",
    "        \"fast_inference\": False,\n",
    "        \"lang_rust\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 8192, \"max_steps\": 3000},\n",
    "        \"core_agent\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 12288, \"max_steps\": 2000},\n",
    "        \"ipo\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 12288, \"max_steps\": 1000},\n",
    "        \"grpo\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 16384, \"max_steps\": 2000, \"num_gen\": 2},\n",
    "    },\n",
    "    \"a100_80gb\": {\n",
    "        \"moe_backend\": \"unsloth_triton\",\n",
    "        \"load_mode\": \"4bit\",\n",
    "        \"fast_inference\": False,\n",
    "        \"lang_rust\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 8192, \"max_steps\": 5000},\n",
    "        \"core_agent\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 16384, \"max_steps\": 3000},\n",
    "        \"ipo\": {\"batch\": 1, \"grad_accum\": 16, \"seq_len\": 16384, \"max_steps\": 2000},\n",
    "        \"grpo\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 32768, \"max_steps\": 5000, \"num_gen\": 4},\n",
    "    },\n",
    "    \"h100_80gb\": {\n",
    "        \"moe_backend\": \"grouped_mm\",\n",
    "        \"load_mode\": \"fp8\",\n",
    "        \"fast_inference\": True,\n",
    "        \"lang_rust\": {\"batch\": 2, \"grad_accum\": 4, \"seq_len\": 8192, \"max_steps\": 5000},\n",
    "        \"core_agent\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 16384, \"max_steps\": 3000},\n",
    "        \"ipo\": {\"batch\": 1, \"grad_accum\": 16, \"seq_len\": 16384, \"max_steps\": 2000},\n",
    "        \"grpo\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 65536, \"max_steps\": 7000, \"num_gen\": 4},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Quick test overrides\n",
    "if training_scope == \"quick_test\":\n",
    "    max_steps_override = 100\n",
    "\n",
    "gpu_cfg = GPU_CONFIGS[gpu_tier]\n",
    "\n",
    "# Detect CPU count and RAM for parallel mutation jobs.\n",
    "# Each cargo-mutants worker spawns cargo build/test subprocesses that can\n",
    "# each use 1-2 GB RAM.  Cap jobs to avoid OOM kills on Colab instances.\n",
    "import multiprocessing\n",
    "import os as _os\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "\n",
    "# RAM-aware job limit: allow ~20 GB per mutation worker for headroom\n",
    "try:\n",
    "    _mem_bytes = _os.sysconf('SC_PAGE_SIZE') * _os.sysconf('SC_PHYS_PAGES')\n",
    "    total_ram_gb = _mem_bytes / (1024**3)\n",
    "    ram_based_jobs = max(1, int(total_ram_gb / 20))\n",
    "except (ValueError, OSError):\n",
    "    total_ram_gb = 0\n",
    "    ram_based_jobs = cpu_count\n",
    "\n",
    "mutation_jobs = min(max(1, cpu_count - 2), ram_based_jobs)\n",
    "\n",
    "# Build CONFIG dict\n",
    "CONFIG = {\n",
    "    \"training_scope\": training_scope,\n",
    "    \"gpu_tier\": gpu_tier,\n",
    "    \"include_grpo\": include_grpo,\n",
    "    \"skip_data_generation\": skip_data_generation,\n",
    "    \"enable_qat_export\": enable_qat_export,\n",
    "    # v2: MoE backend + load mode\n",
    "    \"moe_backend\": gpu_cfg[\"moe_backend\"],\n",
    "    \"load_mode\": gpu_cfg[\"load_mode\"],\n",
    "    \"fast_inference\": gpu_cfg[\"fast_inference\"],\n",
    "    # Lang adapter\n",
    "    \"lang_rust_batch\": gpu_cfg[\"lang_rust\"][\"batch\"],\n",
    "    \"lang_rust_grad_accum\": gpu_cfg[\"lang_rust\"][\"grad_accum\"],\n",
    "    \"lang_rust_seq_len\": gpu_cfg[\"lang_rust\"][\"seq_len\"],\n",
    "    \"lang_rust_max_steps\": max_steps_override or gpu_cfg[\"lang_rust\"][\"max_steps\"],\n",
    "    # Core agent\n",
    "    \"core_agent_batch\": gpu_cfg[\"core_agent\"][\"batch\"],\n",
    "    \"core_agent_grad_accum\": gpu_cfg[\"core_agent\"][\"grad_accum\"],\n",
    "    \"core_agent_seq_len\": gpu_cfg[\"core_agent\"][\"seq_len\"],\n",
    "    \"core_agent_max_steps\": max_steps_override or gpu_cfg[\"core_agent\"][\"max_steps\"],\n",
    "    # IPO\n",
    "    \"ipo_batch\": gpu_cfg[\"ipo\"][\"batch\"],\n",
    "    \"ipo_grad_accum\": gpu_cfg[\"ipo\"][\"grad_accum\"],\n",
    "    \"ipo_seq_len\": gpu_cfg[\"ipo\"][\"seq_len\"],\n",
    "    \"ipo_max_steps\": max_steps_override or gpu_cfg[\"ipo\"][\"max_steps\"],\n",
    "    # GRPO\n",
    "    \"grpo_batch\": gpu_cfg[\"grpo\"][\"batch\"],\n",
    "    \"grpo_grad_accum\": gpu_cfg[\"grpo\"][\"grad_accum\"],\n",
    "    \"grpo_seq_len\": gpu_cfg[\"grpo\"][\"seq_len\"],\n",
    "    \"grpo_max_steps\": max_steps_override or gpu_cfg[\"grpo\"][\"max_steps\"],\n",
    "    \"grpo_num_gen\": gpu_cfg[\"grpo\"][\"num_gen\"],\n",
    "    # Mutation generation — balance CPU parallelism with RAM headroom\n",
    "    \"max_mutations_per_repo\": 50 if training_scope == \"quick_test\" else 100,\n",
    "    \"mutation_jobs\": mutation_jobs,\n",
    "    # Eval\n",
    "    \"eval_num_samples\": 10 if training_scope == \"quick_test\" else 50,\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PIPELINE CONFIGURATION (v2)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nScope: {training_scope.upper()}\")\n",
    "print(f\"GPU tier: {gpu_tier}\")\n",
    "ram_str = f\" | RAM: {total_ram_gb:.0f} GB\" if total_ram_gb else \"\"\n",
    "print(f\"CPUs: {cpu_count}{ram_str} (mutation jobs: {CONFIG['mutation_jobs']})\")\n",
    "print(f\"MoE backend: {CONFIG['moe_backend']}\")\n",
    "print(f\"Load mode: {CONFIG['load_mode']}\")\n",
    "print(f\"Fast inference (vLLM): {CONFIG['fast_inference']}\")\n",
    "print(f\"Include GRPO: {include_grpo}\")\n",
    "print(f\"QAT export: {enable_qat_export}\")\n",
    "print(f\"Skip data gen: {skip_data_generation}\")\n",
    "print(f\"Drive mode: {DRIVE_MODE}\")\n",
    "if max_steps_override:\n",
    "    print(f\"Max steps override: {max_steps_override}\")\n",
    "print(f\"\\nLang Adapter:  batch={CONFIG['lang_rust_batch']} x grad_accum={CONFIG['lang_rust_grad_accum']}, seq={CONFIG['lang_rust_seq_len']}, steps={CONFIG['lang_rust_max_steps']}\")\n",
    "print(f\"Core Agent:    batch={CONFIG['core_agent_batch']} x grad_accum={CONFIG['core_agent_grad_accum']}, seq={CONFIG['core_agent_seq_len']}, steps={CONFIG['core_agent_max_steps']}\")\n",
    "print(f\"IPO:           batch={CONFIG['ipo_batch']} x grad_accum={CONFIG['ipo_grad_accum']}, seq={CONFIG['ipo_seq_len']}, steps={CONFIG['ipo_max_steps']}\")\n",
    "if include_grpo:\n",
    "    print(f\"GRPO:          batch={CONFIG['grpo_batch']} x grad_accum={CONFIG['grpo_grad_accum']}, seq={CONFIG['grpo_seq_len']}, steps={CONFIG['grpo_max_steps']}, gen={CONFIG['grpo_num_gen']}\")\n",
    "print(\"=\" * 60)\n"
   ],
   "id": "93dd91bddbc4ed84"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Set Up Persistent Storage\n"
   ],
   "id": "aa77eea7bd7df89e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVE_SUBDIRS = [\n",
    "    \"checkpoints/lang_rust\",\n",
    "    \"checkpoints/core_agent\",\n",
    "    \"checkpoints/core_agent_ipo\",\n",
    "    \"checkpoints/core_agent_grpo\",\n",
    "    \"checkpoints/gpt-oss-20b-rust-merged\",\n",
    "    \"data/rust/lang_rust\",\n",
    "    \"data/rust/core_agent\",\n",
    "    \"data/rust/mutations\",\n",
    "    \"data/rust/ipo\",\n",
    "    \"data/rust/grpo\",\n",
    "    \"data/rust/eval\",\n",
    "    \"data/rust/repos\",\n",
    "    \"logs\",\n",
    "    \"evals/rust_agent\",\n",
    "]\n",
    "\n",
    "if DRIVE_MODE == \"mounted\":\n",
    "    # Mounted mode: create Drive dirs + symlink local → Drive (original behaviour)\n",
    "    print(f\"Setting up storage at: {DRIVE_BASE}\")\n",
    "    for subdir in DRIVE_SUBDIRS:\n",
    "        os.makedirs(os.path.join(DRIVE_BASE, subdir), exist_ok=True)\n",
    "\n",
    "    for dir_name in [\"checkpoints\", \"data\", \"logs\", \"evals\"]:\n",
    "        local_path = os.path.join(PROJECT_ROOT, dir_name)\n",
    "        drive_path = os.path.join(DRIVE_BASE, dir_name)\n",
    "\n",
    "        if os.path.exists(local_path) and not os.path.islink(local_path):\n",
    "            !cp -r {local_path}/* {drive_path}/ 2>/dev/null || true\n",
    "            !rm -rf {local_path}\n",
    "        elif os.path.islink(local_path):\n",
    "            os.unlink(local_path)\n",
    "\n",
    "        os.symlink(drive_path, local_path)\n",
    "        print(f\"  {dir_name} -> Drive (mounted)\")\n",
    "\n",
    "elif DRIVE_MODE == \"service_account\":\n",
    "    # Service-account mode: create local dirs, restore existing data from Drive\n",
    "    print(\"Setting up local storage + Drive API restore...\")\n",
    "    for subdir in DRIVE_SUBDIRS:\n",
    "        os.makedirs(os.path.join(PROJECT_ROOT, subdir), exist_ok=True)\n",
    "        drive_helper.ensure_dir(subdir)\n",
    "\n",
    "    for dir_name in [\"checkpoints\", \"data\", \"logs\", \"evals\"]:\n",
    "        local_path = os.path.join(PROJECT_ROOT, dir_name)\n",
    "        # Remove stale symlinks from previous mounted runs\n",
    "        if os.path.islink(local_path):\n",
    "            os.unlink(local_path)\n",
    "            os.makedirs(local_path, exist_ok=True)\n",
    "        print(f\"  {dir_name} -> local (backed up via Drive API)\")\n",
    "\n",
    "    print(\"\\nRestoring existing data from Drive...\")\n",
    "    for subdir in DRIVE_SUBDIRS:\n",
    "        local_target = os.path.join(PROJECT_ROOT, subdir)\n",
    "        drive_helper.restore(subdir, local_target)\n",
    "    print(\"Restore complete.\")\n",
    "\n",
    "else:\n",
    "    # Local-only mode — no Drive\n",
    "    for d in [\"checkpoints\", \"data/rust\", \"logs\", \"evals/rust_agent\"]:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "    print(\"Local directories created (no Drive backup).\")\n",
    "\n",
    "print(\"\\nStorage ready!\")\n"
   ],
   "id": "ef3dbe47b760cca0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5 Check GPU & Configure MoE Backend\n",
    "\n",
    "v2: Auto-detects H100 for FP8 RL and sets the optimal Split LoRA backend.\n"
   ],
   "id": "ec0e7828c971b71a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    capability = torch.cuda.get_device_capability()\n",
    "    is_h100 = \"H100\" in gpu_name or \"H200\" in gpu_name or \"B200\" in gpu_name\n",
    "\n",
    "    CONFIG[\"use_fp8\"] = capability[0] >= 9 and is_h100\n",
    "\n",
    "    # v2: Auto-detect GPU tier (now includes H100)\n",
    "    if is_h100:\n",
    "        detected_tier = \"h100_80gb\"\n",
    "    elif gpu_memory >= 70:\n",
    "        detected_tier = \"a100_80gb\"\n",
    "    else:\n",
    "        detected_tier = \"a100_40gb\"\n",
    "\n",
    "    if detected_tier != CONFIG[\"gpu_tier\"]:\n",
    "        print(f\"NOTE: Auto-detected {detected_tier}, overriding configured {CONFIG['gpu_tier']}\")\n",
    "        CONFIG[\"gpu_tier\"] = detected_tier\n",
    "        # Re-derive tier-specific settings\n",
    "        gpu_cfg = GPU_CONFIGS[detected_tier]\n",
    "        CONFIG[\"moe_backend\"] = gpu_cfg[\"moe_backend\"]\n",
    "        CONFIG[\"load_mode\"] = gpu_cfg[\"load_mode\"]\n",
    "        CONFIG[\"fast_inference\"] = gpu_cfg[\"fast_inference\"]\n",
    "\n",
    "    # v2: Set Split LoRA MoE backend\n",
    "    os.environ[\"UNSLOTH_MOE_BACKEND\"] = CONFIG[\"moe_backend\"]\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"GPU: {gpu_name} ({gpu_memory:.0f} GB)\")\n",
    "    print(f\"Compute capability: {capability[0]}.{capability[1]}\")\n",
    "    print(f\"Tier: {CONFIG['gpu_tier']}\")\n",
    "    print(f\"\\nv2 Optimisations:\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(f\"  Load mode: {CONFIG['load_mode']}\")\n",
    "    print(f\"  FP8 available: {CONFIG['use_fp8']}\")\n",
    "    print(f\"  Fast inference (vLLM): {CONFIG['fast_inference']}\")\n",
    "\n",
    "    if gpu_memory < 40:\n",
    "        print(\"\\nWARNING: <40 GB VRAM. Long-context training (16K+) may OOM.\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"No GPU detected!\")\n",
    "    CONFIG[\"use_fp8\"] = False\n",
    "    os.environ[\"UNSLOTH_MOE_BACKEND\"] = \"native_torch\""
   ],
   "id": "f189d07bd7e89131"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Data Generation\n",
    "\n",
    "Generates mutation data from curated Rust repos and agent trajectories.\n",
    "Skip this step if you have pre-generated data on Drive (`skip_data_generation=True`)."
   ],
   "id": "7f93fc1acaa43c5f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Generate Mutation Data\n",
    "\n",
    "Runs `cargo-mutants` on curated Rust repos to produce bug-fix training pairs.\n"
   ],
   "id": "3db2daedc13bc633"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"skip_data_generation\"]:\n",
    "    print(\"Skipping data generation (using pre-generated data from Drive)\")\n",
    "elif CONFIG[\"training_scope\"] in (\"skip_to_rl\",):\n",
    "    print(\"Skipping — not needed for this training scope\")\n",
    "else:\n",
    "    max_muts = CONFIG[\"max_mutations_per_repo\"]\n",
    "    jobs = CONFIG[\"mutation_jobs\"]\n",
    "\n",
    "    print(f\"Generating mutations (max {max_muts}/repo, {jobs} parallel jobs)...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/16_generate_mutations.py \\\n",
    "        --max_mutations_per_repo {max_muts} \\\n",
    "        --jobs {jobs}\n",
    "\n",
    "    drive_helper.backup(\"data/rust/mutations\", \"data/rust/mutations\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nBacked up mutations to Drive.\")\n"
   ],
   "id": "a06c4e99d8d41ffc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Generate Agent Trajectories\n",
    "\n",
    "Generates multi-turn agent trajectories from mutations + Strandset in Harmony format.\n"
   ],
   "id": "fc8a2b5d9669022f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"skip_data_generation\"]:\n",
    "    print(\"Skipping data generation (using pre-generated data from Drive)\")\n",
    "elif CONFIG[\"training_scope\"] in (\"skip_to_rl\",):\n",
    "    print(\"Skipping — not needed for this training scope\")\n",
    "else:\n",
    "    max_samples = 500 if CONFIG[\"training_scope\"] == \"quick_test\" else 5000\n",
    "\n",
    "    print(f\"Generating trajectories (max {max_samples} per source)...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    cmd = f\"python scripts/15_generate_trajectories.py --max_samples {max_samples}\"\n",
    "\n",
    "    mutations_path = \"data/rust/mutations/mutations.jsonl\"\n",
    "    if os.path.exists(mutations_path):\n",
    "        cmd += f\" --mutations_path {mutations_path}\"\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"data/rust/core_agent\", \"data/rust/core_agent\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nBacked up trajectories to Drive.\")\n"
   ],
   "id": "4ce7bebf4b7f876c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Verify Data\n"
   ],
   "id": "6de8a13dcfeedf86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_checks = [\n",
    "    (\"Mutations\", \"data/rust/mutations\"),\n",
    "    (\"Lang Rust train\", \"data/rust/lang_rust/train\"),\n",
    "    (\"Core Agent train\", \"data/rust/core_agent/train\"),\n",
    "    (\"IPO train\", \"data/rust/ipo/train\"),\n",
    "    (\"GRPO tasks\", \"data/rust/grpo\"),\n",
    "    (\"Eval tasks\", \"data/rust/eval\"),\n",
    "]\n",
    "\n",
    "print(\"Data Verification:\")\n",
    "print(\"=\" * 60)\n",
    "for name, path in data_checks:\n",
    "    exists = os.path.exists(path)\n",
    "    if exists and os.path.isdir(path):\n",
    "        items = os.listdir(path)\n",
    "        print(f\"  \\u2713 {name}: {path} ({len(items)} items)\")\n",
    "    elif exists:\n",
    "        size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "        print(f\"  \\u2713 {name}: {path} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        needed = True\n",
    "        if CONFIG[\"training_scope\"] == \"skip_to_rl\" and name in (\"Mutations\", \"Lang Rust train\", \"Core Agent train\"):\n",
    "            needed = False\n",
    "        if CONFIG[\"training_scope\"] == \"lang_adapter_only\" and name in (\"IPO train\", \"GRPO tasks\"):\n",
    "            needed = False\n",
    "        sym = \"\\u2717\" if needed else \"\\u2014\"\n",
    "        label = \"MISSING\" if needed else \"not needed\"\n",
    "        print(f\"  {sym} {name}: {label}\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "861c7d3cd76a69d8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Lang Adapter Training\n",
    "\n",
    "Train a QLoRA adapter (rank 64) to specialise GPT-OSS 20B on Rust syntax, stdlib, and idioms.\n",
    "Then merge the adapter into the base weights for downstream training.\n",
    "\n",
    "**v2:** Split LoRA backend auto-enabled for 7-12x faster MoE training."
   ],
   "id": "31fd053ef297615e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Train lang_rust Adapter\n",
    "\n",
    "v2: Split LoRA enabled via UNSLOTH_MOE_BACKEND env var (set in 0.5).\n"
   ],
   "id": "fd35fdde7902ddda"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "    print(\"Skipping — scope is skip_to_rl\")\n",
    "else:\n",
    "    batch = CONFIG[\"lang_rust_batch\"]\n",
    "    grad_accum = CONFIG[\"lang_rust_grad_accum\"]\n",
    "    max_steps = CONFIG[\"lang_rust_max_steps\"]\n",
    "    seq_len = CONFIG[\"lang_rust_seq_len\"]\n",
    "\n",
    "    cmd = f\"python scripts/13_train_lang_adapter.py\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    print(f\"Training lang_rust adapter...\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Seq length: {seq_len} (from config)\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/lang_rust\", \"checkpoints/lang_rust\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")\n"
   ],
   "id": "4719fb4569fd4a48"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Merge lang_rust into Base\n"
   ],
   "id": "6d945162fc6c4887"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "    print(\"Skipping — scope is skip_to_rl\")\n",
    "else:\n",
    "    print(\"Merging lang_rust adapter into base model...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/19_merge_adapter.py \\\n",
    "        --adapter_path checkpoints/lang_rust/final \\\n",
    "        --output_dir checkpoints/gpt-oss-20b-rust-merged \\\n",
    "        --export_formats hf\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/gpt-oss-20b-rust-merged\", \"checkpoints/gpt-oss-20b-rust-merged\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nMerged model backed up to Drive.\")\n"
   ],
   "id": "14dd6ab10bc01ad0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Verify Merge\n"
   ],
   "id": "8a649a98ab1ceee0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "    print(\"Skipping \\u2014 scope is skip_to_rl\")\n",
    "else:\n",
    "    merged_path = \"checkpoints/gpt-oss-20b-rust-merged\"\n",
    "    adapter_path = \"checkpoints/lang_rust/final\"\n",
    "\n",
    "    print(\"Merge Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(merged_path):\n",
    "        files = os.listdir(merged_path)\n",
    "        safetensors = [f for f in files if f.endswith(\".safetensors\")]\n",
    "        print(f\"  \\u2713 Merged model: {merged_path}\")\n",
    "        print(f\"    {len(safetensors)} safetensors shard(s), {len(files)} total files\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 Merged model not found at {merged_path}\")\n",
    "\n",
    "    if os.path.exists(adapter_path):\n",
    "        adapter_files = os.listdir(adapter_path)\n",
    "        print(f\"  \\u2713 Adapter: {adapter_path} ({len(adapter_files)} files)\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 Adapter not found at {adapter_path}\")\n",
    "\n",
    "    if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "        print(\"\\n\\u2713 lang_adapter_only scope complete. Stopping here.\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ],
   "id": "875f898a2de17db"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Core Agent SFT\n",
    "\n",
    "Train a higher-rank LoRA adapter (rank 128) on agent trajectories with tool use.\n",
    "Uses the merged lang_rust model as the base.\n",
    "\n",
    "**v2:** Auto uncontaminated packing (3x faster, zero-config). Flex Attention for long context."
   ],
   "id": "603346a356305aad"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Train core_agent Adapter\n",
    "\n",
    "v2: Auto packing (3x faster) + Split LoRA backend enabled.\n"
   ],
   "id": "2931e91685b890ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] in (\"lang_adapter_only\", \"skip_to_rl\"):\n",
    "    print(f\"Skipping — scope is {CONFIG['training_scope']}\")\n",
    "else:\n",
    "    batch = CONFIG[\"core_agent_batch\"]\n",
    "    grad_accum = CONFIG[\"core_agent_grad_accum\"]\n",
    "    max_steps = CONFIG[\"core_agent_max_steps\"]\n",
    "    seq_len = CONFIG[\"core_agent_seq_len\"]\n",
    "\n",
    "    cmd = f\"python scripts/14_train_core_agent.py\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    print(f\"Training core_agent adapter...\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Seq length: {seq_len} (from config)\")\n",
    "    print(f\"  LoRA rank: 128\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(f\"  Auto packing: enabled (uncontaminated)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/core_agent\", \"checkpoints/core_agent\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")\n"
   ],
   "id": "50b1d5d150eae5d0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Verify core_agent\n"
   ],
   "id": "9f890c4676329aad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] in (\"lang_adapter_only\", \"skip_to_rl\"):\n",
    "    print(f\"Skipping \\u2014 scope is {CONFIG['training_scope']}\")\n",
    "else:\n",
    "    ckpt_path = \"checkpoints/core_agent/final\"\n",
    "\n",
    "    print(\"Core Agent Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(ckpt_path):\n",
    "        files = os.listdir(ckpt_path)\n",
    "        print(f\"  \\u2713 Checkpoint: {ckpt_path} ({len(files)} files)\")\n",
    "\n",
    "        adapter_config = os.path.join(ckpt_path, \"adapter_config.json\")\n",
    "        if os.path.exists(adapter_config):\n",
    "            import json\n",
    "            with open(adapter_config) as f:\n",
    "                cfg = json.load(f)\n",
    "            print(f\"    LoRA rank: {cfg.get('r', '?')}\")\n",
    "            print(f\"    Alpha: {cfg.get('lora_alpha', '?')}\")\n",
    "            print(f\"    Target modules: {cfg.get('target_modules', '?')}\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 Checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ],
   "id": "7935e2cb9e427223"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Preference Optimisation (IPO)\n",
    "\n",
    "Train with Identity Preference Optimisation on ranked pairs.\n",
    "Very low learning rate (5e-7), 1 epoch only to avoid collapse.\n",
    "\n",
    "**v2:** FP8 weights on H100 (60% less VRAM). Expert utilisation monitoring."
   ],
   "id": "4dba79ff4175093b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Train with IPO\n",
    "\n",
    "v2: FP8 on H100, expert utilisation monitoring, Split LoRA.\n"
   ],
   "id": "54a5adb1ec756442"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping — scope is lang_adapter_only\")\n",
    "else:\n",
    "    batch = CONFIG[\"ipo_batch\"]\n",
    "    grad_accum = CONFIG[\"ipo_grad_accum\"]\n",
    "    max_steps = CONFIG[\"ipo_max_steps\"]\n",
    "\n",
    "    if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "        ipo_checkpoint = \"checkpoints/core_agent/final\"\n",
    "        print(\"Using existing core_agent checkpoint (skip_to_rl mode)\")\n",
    "    else:\n",
    "        ipo_checkpoint = \"checkpoints/core_agent/final\"\n",
    "\n",
    "    cmd = f\"python scripts/17_ipo_preference.py\"\n",
    "    cmd += f\" --checkpoint {ipo_checkpoint}\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    print(f\"Training with IPO...\")\n",
    "    print(f\"  Checkpoint: {ipo_checkpoint}\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Loss: IPO (beta=0.1)\")\n",
    "    print(f\"  Load mode: {CONFIG['load_mode']}\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/core_agent_ipo\", \"checkpoints/core_agent_ipo\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")\n"
   ],
   "id": "d1401e64737e0b4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Verify IPO\n"
   ],
   "id": "442df61074a8cd86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    ckpt_path = \"checkpoints/core_agent_ipo/final\"\n",
    "\n",
    "    print(\"IPO Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(ckpt_path):\n",
    "        files = os.listdir(ckpt_path)\n",
    "        print(f\"  \\u2713 IPO checkpoint: {ckpt_path} ({len(files)} files)\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 IPO checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "    # Check tensorboard logs for KL divergence\n",
    "    tb_dir = \"checkpoints/core_agent_ipo\"\n",
    "    tb_files = []\n",
    "    if os.path.exists(tb_dir):\n",
    "        for root, dirs, fnames in os.walk(tb_dir):\n",
    "            for fn in fnames:\n",
    "                if fn.startswith(\"events.out.tfevents\"):\n",
    "                    tb_files.append(os.path.join(root, fn))\n",
    "    if tb_files:\n",
    "        print(f\"  \\u2713 TensorBoard logs found ({len(tb_files)} event files)\")\n",
    "        print(f\"    Monitor KL divergence: warn >0.3, abort >0.5\")\n",
    "    else:\n",
    "        print(f\"  \\u2014 No TensorBoard logs found\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ],
   "id": "d83944ef11775585"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: GRPO RL\n",
    "\n",
    "Group Relative Policy Optimisation with execution-based rewards.\n",
    "Generates N completions per prompt, runs `cargo check/test/clippy`, computes group-relative advantages.\n",
    "\n",
    "**v2 Optimisations:**\n",
    "- FP8 RL with vLLM inference on H100 (1.6x throughput)\n",
    "- Chunked batching for 7x longer context\n",
    "- Extended curriculum: 65K context on H100 (up from 32K)\n",
    "- Harmony format compliance reward to prevent infinite reasoning loops\n",
    "\n",
    "**This step is optional** (`include_grpo=False` to skip)."
   ],
   "id": "a10e7f0a4032c87e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Train with GRPO\n",
    "\n",
    "v2: FP8 RL + vLLM (H100), chunked batching, extended curriculum.\n"
   ],
   "id": "640c22f7ecce99e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping — scope is lang_adapter_only\")\n",
    "elif not CONFIG[\"include_grpo\"]:\n",
    "    print(\"Skipping — GRPO disabled (include_grpo=False)\")\n",
    "else:\n",
    "    batch = CONFIG[\"grpo_batch\"]\n",
    "    grad_accum = CONFIG[\"grpo_grad_accum\"]\n",
    "    max_steps = CONFIG[\"grpo_max_steps\"]\n",
    "    max_seq = CONFIG[\"grpo_seq_len\"]\n",
    "\n",
    "    grpo_checkpoint = \"checkpoints/core_agent_ipo/final\"\n",
    "\n",
    "    cmd = f\"python scripts/18_grpo_rl.py\"\n",
    "    cmd += f\" --checkpoint {grpo_checkpoint}\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    # v2: Note which optimisations are active\n",
    "    v2_features = []\n",
    "    v2_features.append(f\"Split LoRA ({CONFIG['moe_backend']})\")\n",
    "    if CONFIG[\"load_mode\"] == \"fp8\":\n",
    "        v2_features.append(\"FP8 weights\")\n",
    "    if CONFIG[\"fast_inference\"]:\n",
    "        v2_features.append(\"vLLM inference\")\n",
    "    v2_features.append(\"Chunked batching (auto)\")\n",
    "    v2_features.append(\"Auto packing\")\n",
    "\n",
    "    if CONFIG[\"gpu_tier\"] == \"a100_40gb\":\n",
    "        print(\"NOTE: 40GB GPU — GRPO sequence length capped at 16384\")\n",
    "\n",
    "    print(f\"Training with GRPO (v2)...\")\n",
    "    print(f\"  Checkpoint: {grpo_checkpoint}\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Max seq length: {max_seq}\")\n",
    "    print(f\"  Generations per prompt: {CONFIG['grpo_num_gen']}\")\n",
    "    print(f\"\\n  v2 features active:\")\n",
    "    for feat in v2_features:\n",
    "        print(f\"    ✓ {feat}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/core_agent_grpo\", \"checkpoints/core_agent_grpo\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")\n"
   ],
   "id": "ced528c03911d4d7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Verify GRPO\n"
   ],
   "id": "34e15c6770525e08"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "elif not CONFIG[\"include_grpo\"]:\n",
    "    print(\"Skipping \\u2014 GRPO disabled\")\n",
    "else:\n",
    "    ckpt_path = \"checkpoints/core_agent_grpo/final\"\n",
    "\n",
    "    print(\"GRPO Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(ckpt_path):\n",
    "        files = os.listdir(ckpt_path)\n",
    "        print(f\"  \\u2713 GRPO checkpoint: {ckpt_path} ({len(files)} files)\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 GRPO checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ],
   "id": "1f051b9601545463"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Evaluation\n",
    "\n",
    "Evaluate the best checkpoint on held-out Rust tasks using execution-based metrics\n",
    "(cargo check, cargo test, clippy)."
   ],
   "id": "c3395dfa3ecbc680"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Run Rust Evaluation\n"
   ],
   "id": "1fb7069b4d6fddef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping — scope is lang_adapter_only\")\n",
    "else:\n",
    "    # Determine best checkpoint\n",
    "    if CONFIG[\"include_grpo\"] and os.path.exists(\"checkpoints/core_agent_grpo/final\"):\n",
    "        eval_checkpoint = \"checkpoints/core_agent_grpo/final\"\n",
    "    elif os.path.exists(\"checkpoints/core_agent_ipo/final\"):\n",
    "        eval_checkpoint = \"checkpoints/core_agent_ipo/final\"\n",
    "    elif os.path.exists(\"checkpoints/core_agent/final\"):\n",
    "        eval_checkpoint = \"checkpoints/core_agent/final\"\n",
    "    else:\n",
    "        eval_checkpoint = \"checkpoints/core_agent_ipo/final\"\n",
    "\n",
    "    num_samples = CONFIG[\"eval_num_samples\"]\n",
    "\n",
    "    print(f\"Evaluating checkpoint: {eval_checkpoint}\")\n",
    "    print(f\"Samples: {num_samples}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/eval_rust_agent.py \\\n",
    "        --checkpoint {eval_checkpoint} \\\n",
    "        --num_samples {num_samples}\n",
    "\n",
    "    drive_helper.backup(\"evals/rust_agent\", \"evals/rust_agent\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nResults backed up to Drive.\")\n"
   ],
   "id": "bad0cdf0e5a54cb3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Check Promotion Gates\n"
   ],
   "id": "b0a563f4effe713f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    print(\"Checking promotion gates...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/12_check_gates.py rust_agent"
   ],
   "id": "4197d776a316d2cd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Display Results\n"
   ],
   "id": "d213ef309b2bacd7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    import json\n",
    "\n",
    "    metrics_path = \"evals/rust_agent/metrics.json\"\n",
    "\n",
    "    if os.path.exists(metrics_path):\n",
    "        with open(metrics_path) as f:\n",
    "            metrics = json.load(f)\n",
    "\n",
    "        targets = {\n",
    "            \"cargo_check_pass_rate\": (0.85, \"higher\"),\n",
    "            \"cargo_test_pass_rate\": (0.70, \"higher\"),\n",
    "            \"clippy_clean_rate\": (0.80, \"higher\"),\n",
    "            \"iterations_to_green_median\": (3, \"lower\"),\n",
    "            \"diff_size_median\": (50, \"lower\"),\n",
    "            \"tool_call_format_accuracy\": (0.99, \"higher\"),\n",
    "            \"hallucinated_api_rate\": (0.05, \"lower\"),\n",
    "        }\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "        print(\"EVALUATION RESULTS\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"{'Metric':<32} {'Value':>8} {'Target':>8} {'Status':>8}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        for key, (target, direction) in targets.items():\n",
    "            value = metrics.get(key)\n",
    "            if value is None:\n",
    "                print(f\"{key:<32} {'N/A':>8} {target:>8} {'\\u2014':>8}\")\n",
    "                continue\n",
    "\n",
    "            if direction == \"higher\":\n",
    "                passed = value >= target\n",
    "            else:\n",
    "                passed = value <= target\n",
    "\n",
    "            status = \"\\u2713 PASS\" if passed else \"\\u2717 FAIL\"\n",
    "            fmt_val = f\"{value:.2%}\" if isinstance(value, float) and value <= 1 else f\"{value}\"\n",
    "            fmt_tgt = f\"{target:.0%}\" if isinstance(target, float) and target <= 1 else f\"{target}\"\n",
    "            print(f\"{key:<32} {fmt_val:>8} {fmt_tgt:>8} {status:>8}\")\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "    else:\n",
    "        print(f\"\\u2717 Metrics file not found at {metrics_path}\")\n",
    "        print(\"Run evaluation (6.1) first.\")"
   ],
   "id": "6ff37a75b3dd990f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Test Model\n",
    "\n",
    "Load the trained model and generate Rust code interactively.\n",
    "\n",
    "**v2:** FP8 loading on H100 for faster inference. `fast_inference=True` enables vLLM backend."
   ],
   "id": "856732da9a5a7910"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Load Model\n",
    "\n",
    "v2: FP8 loading on H100, vLLM-backed inference.\n"
   ],
   "id": "454dd084fa7f33d0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "CHECKPOINT_PRIORITY = [\n",
    "    \"checkpoints/core_agent_grpo/final\",\n",
    "    \"checkpoints/core_agent_ipo/final\",\n",
    "    \"checkpoints/core_agent/final\",\n",
    "    \"checkpoints/gpt-oss-20b-rust-merged\",\n",
    "]\n",
    "\n",
    "MODEL_PATH = None\n",
    "for path in CHECKPOINT_PRIORITY:\n",
    "    if os.path.exists(path):\n",
    "        MODEL_PATH = path\n",
    "        break\n",
    "\n",
    "if MODEL_PATH is None:\n",
    "    print(\"\\u2717 No checkpoint found. Train the model first.\")\n",
    "else:\n",
    "    print(f\"Loading model from: {MODEL_PATH}\")\n",
    "\n",
    "    # v2: Use FP8 on H100, 4-bit otherwise\n",
    "    load_kwargs = {\n",
    "        \"max_seq_length\": 4096,\n",
    "        \"dtype\": torch.bfloat16,\n",
    "    }\n",
    "    if CONFIG.get(\"load_mode\") == \"fp8\" and CONFIG.get(\"use_fp8\"):\n",
    "        load_kwargs[\"load_in_fp8\"] = True\n",
    "        print(\"  Mode: FP8 (H100)\")\n",
    "    else:\n",
    "        load_kwargs[\"load_in_4bit\"] = True\n",
    "        print(\"  Mode: 4-bit QLoRA\")\n",
    "\n",
    "    if CONFIG.get(\"fast_inference\"):\n",
    "        load_kwargs[\"fast_inference\"] = True\n",
    "        print(\"  Inference: vLLM backend\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(MODEL_PATH, **load_kwargs)\n",
    "    FastLanguageModel.for_inference(model)\n",
    "\n",
    "    print(\"\\u2713 Model loaded!\")"
   ],
   "id": "58e85ca44306ad95"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Generate Rust Code\n",
    "\n",
    "Tests the model on 3 pre-defined Rust prompts using Harmony format.\n"
   ],
   "id": "2a8c82c63929e765"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"scripts\")\n",
    "from dataset_formatters.harmony import encode_harmony_messages\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    \"Write a Rust function `fn merge_sorted(a: &[i32], b: &[i32]) -> Vec<i32>` that merges two sorted slices into a single sorted vector.\",\n",
    "    \"This Rust code fails the borrow checker. Fix it:\\n```rust\\nfn main() {\\n    let mut v = vec![1, 2, 3];\\n    let first = &v[0];\\n    v.push(4);\\n    println!(\\\"{}\\\", first);\\n}\\n```\",\n",
    "    \"Write an async Rust function using tokio that fetches a URL with reqwest, retries up to 3 times on failure, and returns the response body as a String.\",\n",
    "]\n",
    "\n",
    "def generate_rust(prompt, max_tokens=1024):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted = encode_harmony_messages(\n",
    "        messages,\n",
    "        developer_instructions=\"You are a Rust programming expert. Write correct, idiomatic code.\",\n",
    "    )\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.3,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "for i, prompt in enumerate(TEST_PROMPTS, 1):\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Test {i}: {prompt[:80]}...\")\n",
    "    print(\"=\" * 60)\n",
    "    response = generate_rust(prompt)\n",
    "    print(response)\n",
    "    print()"
   ],
   "id": "fe79b1c5cd8fa9af"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Custom Prompt\n"
   ],
   "id": "f26961ac0a874651"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_PROMPT = \"Write a Rust function that reads a CSV file and returns the sum of a specified column.\"\n",
    "\n",
    "print(f\"Prompt: {CUSTOM_PROMPT}\")\n",
    "print(\"=\" * 60)\n",
    "print(generate_rust(CUSTOM_PROMPT))"
   ],
   "id": "4b4b7f32818a3d05"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Export\n",
    "\n",
    "Merge the final adapter and export to HuggingFace + GGUF formats.\n",
    "\n",
    "**v2:** Optional QAT export for 97-100% MXFP4 quality retention (vs 59-89% with PTQ)."
   ],
   "id": "2cf19b6ed97d019e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Export to GGUF\n",
    "\n",
    "Merges the best adapter and exports as HF safetensors + GGUF Q4.\n"
   ],
   "id": "91788a1701ee9837"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADAPTER_PRIORITY = [\n",
    "    \"checkpoints/core_agent_grpo/final\",\n",
    "    \"checkpoints/core_agent_ipo/final\",\n",
    "    \"checkpoints/core_agent/final\",\n",
    "    \"checkpoints/lang_rust/final\",\n",
    "]\n",
    "\n",
    "adapter_path = None\n",
    "for path in ADAPTER_PRIORITY:\n",
    "    if os.path.exists(path):\n",
    "        adapter_path = path\n",
    "        break\n",
    "\n",
    "if adapter_path is None:\n",
    "    print(\"✗ No adapter checkpoint found.\")\n",
    "else:\n",
    "    export_dir = \"checkpoints/gpt-oss-20b-rust-export-v2\"\n",
    "    print(f\"Exporting adapter: {adapter_path}\")\n",
    "    print(f\"Output: {export_dir}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/19_merge_adapter.py \\\n",
    "        --adapter_path {adapter_path} \\\n",
    "        --output_dir {export_dir} \\\n",
    "        --export_formats hf gguf_q4\n",
    "\n",
    "    drive_helper.backup(export_dir, \"checkpoints/gpt-oss-20b-rust-export-v2\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nExport backed up to Drive.\")\n"
   ],
   "id": "d1b8a168f48fae53"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 QAT Export (Optional)\n",
    "\n",
    "v2: Quantisation-Aware Training for MXFP4 deployment.\n",
    "Recovers 97-100% quality vs 59-89% with post-training quantisation.\n",
    "Requires: `pip install nvidia-modelopt`\n"
   ],
   "id": "c632af3d6ca89abd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CONFIG.get(\"enable_qat_export\"):\n",
    "    print(\"QAT export disabled. Enable via enable_qat_export=True in Step 0.3.\")\n",
    "    print(\"\\nQAT recovers 97-100% quality when deploying to MXFP4,\")\n",
    "    print(\"vs 59-89% with standard post-training quantisation (PTQ).\")\n",
    "else:\n",
    "    export_dir = \"checkpoints/gpt-oss-20b-rust-export-v2\"\n",
    "    qat_dir = \"checkpoints/gpt-oss-20b-rust-qat\"\n",
    "\n",
    "    if not os.path.exists(export_dir):\n",
    "        print(\"\\u2717 Run standard export (8.1) first.\")\n",
    "    else:\n",
    "        print(\"Running QAT pass on merged model...\")\n",
    "        print(\"  This fine-tunes with MXFP4-aware quantisation at reduced LR (1e-5).\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        try:\n",
    "            import modelopt.torch.quantization as mtq\n",
    "            print(\"\\u2713 nvidia-modelopt available\")\n",
    "\n",
    "            # QAT would be run here via mtq.quantize()\n",
    "            # For now, document the expected command:\n",
    "            print(\"\\nQAT pipeline (manual steps):\")\n",
    "            print(f\"  1. Load merged BF16 model from {export_dir}\")\n",
    "            print(f\"  2. mtq.quantize(model, config=mtq.MXFP4_DEFAULT_CFG)\")\n",
    "            print(f\"  3. Fine-tune for ~100 steps at LR 1e-5\")\n",
    "            print(f\"  4. Export to {qat_dir}\")\n",
    "        except ImportError:\n",
    "            print(\"\\u2717 nvidia-modelopt not installed.\")\n",
    "            print(\"  Install: pip install nvidia-modelopt\")\n",
    "            print(\"  See: https://developer.nvidia.com/blog/fine-tuning-gpt-oss-for-accuracy-and-performance-with-quantization-aware-training/\")"
   ],
   "id": "684742559682938a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Download GGUF\n"
   ],
   "id": "88ce6eb89550c5d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    import glob\n",
    "\n",
    "    export_dir = \"checkpoints/gpt-oss-20b-rust-export-v2\"\n",
    "    gguf_files = glob.glob(os.path.join(export_dir, \"*.gguf\"))\n",
    "\n",
    "    if gguf_files:\n",
    "        gguf_path = gguf_files[0]\n",
    "        size_gb = os.path.getsize(gguf_path) / (1024**3)\n",
    "        print(f\"Downloading: {os.path.basename(gguf_path)} ({size_gb:.1f} GB)\")\n",
    "        files.download(gguf_path)\n",
    "    else:\n",
    "        print(\"\\u2717 No GGUF file found. Run export (8.1) first.\")\n",
    "else:\n",
    "    print(\"Download not available outside Colab.\")\n",
    "    print(\"GGUF file is at: checkpoints/gpt-oss-20b-rust-export-v2/\")"
   ],
   "id": "1240d8a590699baf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Complete!\n",
    "\n",
    "Your GPT-OSS 20B Rust coding agent (v2) is trained and ready to use.\n",
    "\n",
    "**v2 Optimisations Applied:**\n",
    "- Split LoRA: 7-12x faster MoE training\n",
    "- FP8 RL: 1.6x throughput on H100 (60% less VRAM)\n",
    "- Auto packing: 3x faster SFT\n",
    "- Chunked GRPO: 65K context on H100 (up from 32K)\n",
    "- QAT export: 97-100% MXFP4 quality (if enabled)\n",
    "\n",
    "**Outputs:**\n",
    "- Checkpoints: `checkpoints/core_agent_{ipo,grpo}/final`\n",
    "- Evaluation: `evals/rust_agent/metrics.json`\n",
    "- Exported model: `checkpoints/gpt-oss-20b-rust-export-v2/`\n",
    "- All backed up to Google Drive: `gpt-oss-20b-rust-agent-v2/`\n",
    "\n",
    "**Next steps:**\n",
    "- Review evaluation metrics in Step 6.3\n",
    "- Test interactively in Step 7\n",
    "- Deploy the GGUF file with llama.cpp or Ollama\n",
    "- For MXFP4 deployment, enable QAT export in Step 8.2\n",
    "\n",
    "**References:**\n",
    "- [V2 Optimization Plan](../docs/V2_OPTIMIZATION_PLAN.md)\n",
    "- [Unsloth Split LoRA](https://unsloth.ai/docs/new/faster-moe)\n",
    "- [Unsloth FP8 RL](https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/fp8-reinforcement-learning)\n",
    "- [NVIDIA QAT for GPT-OSS](https://developer.nvidia.com/blog/fine-tuning-gpt-oss-for-accuracy-and-performance-with-quantization-aware-training/)"
   ],
   "id": "f2381c249430c4d6"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
