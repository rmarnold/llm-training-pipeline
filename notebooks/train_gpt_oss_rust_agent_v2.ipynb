{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GPT-OSS 20B → Rust Coding Agent (v2)\n",
    "\n",
    "End-to-end pipeline for training a Rust coding agent on OpenAI's GPT-OSS 20B (MoE, ~3.6B active params).\n",
    "\n",
    "**v2 Optimisations** (see `docs/V2_OPTIMIZATION_PLAN.md`):\n",
    "- **Split LoRA** — 7-12x faster MoE training via reordered LoRA computation\n",
    "- **FP8 RL** — 1.6x throughput, 60% less VRAM on H100 (auto-fallback to 4-bit on A100)\n",
    "- **GRPO long context** — Chunked batching enables 65K+ context (up from 32K)\n",
    "- **Flex Attention** — 8x longer sequences with attention sinks\n",
    "- **Auto packing** — 3x faster SFT with uncontaminated packing (zero-config)\n",
    "- **Expert monitoring** — Routing utilisation tracking across all phases\n",
    "- **QAT export** — 97-100% MXFP4 quality retention (vs 59-89% with PTQ)\n",
    "\n",
    "**4-Phase Pipeline:**\n",
    "1. **Lang Adapter** — Rust domain specialisation via QLoRA (script 13 + 19)\n",
    "2. **Core Agent SFT** — Agent trajectory training with tool use (script 14)\n",
    "3. **IPO Preference** — Identity Preference Optimisation on ranked pairs (script 17)\n",
    "4. **GRPO RL** — Group Relative Policy Optimisation with execution rewards (script 18)\n",
    "\n",
    "**Requirements:**\n",
    "- **GPU**: A100 40GB+ (H100 80GB recommended for FP8 + extended context)\n",
    "- **Storage**: Google Drive for persistent checkpoints\n",
    "- **Rust toolchain**: Installed automatically (rustup + cargo-mutants)"
   ],
   "id": "4426faf4d8d7ae35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Environment Setup"
   ],
   "id": "c68353567587459c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Mount Google Drive & Clone Repository\n",
    "\n",
    "**PyCharm / headless users:** If `drive.mount()` doesn't work (e.g. PyCharm Colab\n",
    "plugin can't relay the OAuth popup), set `use_service_account = True` and provide\n",
    "your service-account JSON key path in Step 0.3.\n"
   ],
   "id": "97b1ca5318fc7ab0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T07:24:32.622023Z",
     "start_time": "2026-02-12T07:24:31.502430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "use_service_account = True\n",
    "\n",
    "DRIVE_MOUNTED = False\n",
    "\n",
    "if IN_COLAB and not use_service_account:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        DRIVE_MOUNTED = True\n",
    "        print(\"Google Drive mounted\")\n",
    "    except Exception as e:\n",
    "        print(f\"drive.mount() failed: {e}\")\n",
    "        print(\"Falling back to local-only mode.\")\n",
    "        print(\"Tip: set use_service_account=True and provide a JSON key in Step 0.3.\")\n",
    "elif IN_COLAB and use_service_account:\n",
    "    print(\"Service-account mode selected — skipping drive.mount()\")\n",
    "    print(\"Configure credentials in Step 0.3.\")\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "\n",
    "REPO_URL = \"https://github.com/rmarnold/llm-training-pipeline.git\"\n",
    "BRANCH = \"main\"\n",
    "\n",
    "REPO_DIR = \"/content/llm-training-pipeline\"\n",
    "\n",
    "if IN_COLAB:\n",
    "    if os.path.exists(REPO_DIR):\n",
    "        %cd {REPO_DIR}\n",
    "        !git pull origin {BRANCH}\n",
    "    else:\n",
    "        !git clone -b {BRANCH} {REPO_URL} {REPO_DIR}\n",
    "        %cd {REPO_DIR}\n",
    "\n",
    "    PROJECT_ROOT = REPO_DIR\n",
    "else:\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"\\nProject root: {PROJECT_ROOT}\")\n"
   ],
   "id": "604e923187b87e5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service-account mode selected — skipping drive.mount()\n",
      "Configure credentials in Step 0.3.\n",
      "/content/llm-training-pipeline\n",
      "remote: Enumerating objects: 15, done.\u001b[K\n",
      "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 8 (delta 7), reused 8 (delta 7), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects: 100% (8/8), 867 bytes | 867.00 KiB/s, done.\n",
      "From https://github.com/rmarnold/llm-training-pipeline\n",
      " * branch            main       -> FETCH_HEAD\n",
      "   5c0647e..14453f8  main       -> origin/main\n",
      "Updating 5c0647e..14453f8\n",
      "Fast-forward\n",
      " scripts/pipeline_lib/text_cleaning/__init__.py | 10 \u001b[32m+++++\u001b[m\u001b[31m-----\u001b[m\n",
      " scripts/pipeline_lib/text_cleaning/toxicity.py |  2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " scripts/pipeline_lib/training_callbacks.py     |  2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " 3 files changed, 7 insertions(+), 7 deletions(-)\n",
      "\n",
      "Project root: /content/llm-training-pipeline\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Install Dependencies\n",
    "\n",
    "Installs pipeline deps, latest Unsloth (with Split LoRA + FP8 RL), and the Rust toolchain.\n",
    "\n",
    "**Note:** flash-attn is intentionally NOT installed. FA3 is incompatible with GPT-OSS\n",
    "backward passes (incorrect training loss). Unsloth's Flex Attention replaces it\n",
    "automatically — no compilation step needed.\n"
   ],
   "id": "4f8d5ba0fcc56075"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T07:26:05.667090Z",
     "start_time": "2026-02-12T07:24:38.333297Z"
    }
   },
   "source": [
    "if IN_COLAB:\n",
    "    print(\"Installing Python dependencies...\")\n",
    "    print(\"=\" * 60)\n",
    "    !pip install -q -e \".[gpt_oss,rust_eval,colab]\"\n",
    "\n",
    "    # Fix pyarrow binary incompatibility with datasets 4.x on Colab\n",
    "    # (Colab's pre-installed pyarrow C extension doesn't match the new header)\n",
    "    !pip install -q --force-reinstall pyarrow\n",
    "\n",
    "    # v2: Force latest Unsloth with Split LoRA + FP8 RL + GRPO long context\n",
    "    # Flex Attention (bundled with Unsloth) replaces Flash Attention for GPT-OSS\n",
    "    print(\"\\nInstalling latest Unsloth (Split LoRA + FP8 RL + Flex Attention)...\")\n",
    "    !pip install -q --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo\n",
    "    !pip install -q \"unsloth[colab-new]\"\n",
    "\n",
    "    # v2: vLLM for FP8 RL inference (H100 only, optional)\n",
    "    !pip install -q vllm>=0.12.0 2>/dev/null || true\n",
    "\n",
    "    print(\"\\nInstalling Rust toolchain...\")\n",
    "    print(\"=\" * 60)\n",
    "    !curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
    "    os.environ[\"PATH\"] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "    !cargo install cargo-mutants\n",
    "\n",
    "    # Verification — use importlib.metadata to check versions without importing\n",
    "    # (importing unsloth triggers heavy CUDA init that can hang in a notebook cell)\n",
    "    from importlib.metadata import version, PackageNotFoundError\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Dependency Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for pkg in [\"unsloth\", \"trl\", \"peft\", \"datasets\", \"tiktoken\", \"vllm\"]:\n",
    "        try:\n",
    "            ver = version(pkg)\n",
    "            print(f\"\\u2713 {pkg}: {ver}\")\n",
    "        except PackageNotFoundError:\n",
    "            if pkg == \"vllm\":\n",
    "                print(f\"\\u2014 {pkg}: not installed (optional, H100 FP8 RL only)\")\n",
    "            else:\n",
    "                print(f\"\\u2717 {pkg}: not installed\")\n",
    "\n",
    "    import subprocess\n",
    "    for cmd, label in [(\"cargo --version\", \"cargo\"), (\"cargo mutants --version\", \"cargo-mutants\")]:\n",
    "        result = subprocess.run(cmd.split(), capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"\\u2713 {label}: {result.stdout.strip()}\")\n",
    "        else:\n",
    "            print(f\"\\u2717 {label}: not found\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"Running locally \\u2014 ensure deps are installed:\")\n",
    "    print(\"  pip install -e '.[gpt_oss,rust_eval]'\")\n",
    "    print(\"  pip install --upgrade unsloth unsloth_zoo\")\n"
   ],
   "id": "fb54d828abbdfd4b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing Python dependencies...\n",
      "============================================================\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building editable for llm-training-pipeline (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.9.1 requires torch==2.9.1, but you have torch 2.10.0 which is incompatible.\n",
      "vllm 0.15.1 requires torch==2.9.1, but you have torch 2.10.0 which is incompatible.\n",
      "vllm 0.15.1 requires torchvision==0.24.1, but you have torchvision 0.25.0 which is incompatible.\n",
      "fastai 2.8.6 requires torch<2.10,>=1.10, but you have torch 2.10.0 which is incompatible.\n",
      "cuda-python 12.9.5 requires cuda-bindings~=12.9.5, but you have cuda-bindings 12.9.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "Installing latest Unsloth (Split LoRA + FP8 RL + Flex Attention)...\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.3/432.3 kB\u001b[0m \u001b[31m247.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.5/376.5 kB\u001b[0m \u001b[31m546.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33m    WARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: unsloth 2026.2.1 does not provide the extra 'triton'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nvshmem-cu12 (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "vllm 0.15.1 requires torch==2.9.1, but you have torch 2.10.0 which is incompatible.\n",
      "vllm 0.15.1 requires torchvision==0.24.1, but you have torchvision 0.25.0 which is incompatible.\n",
      "fastai 2.8.6 requires torch<2.10,>=1.10, but you have torch 2.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "Installing Rust toolchain...\n",
      "============================================================\n",
      "\u001b[1minfo:\u001b[0m downloading installer\n",
      "\u001b[0m\u001b[1m\u001b[33mwarn: \u001b[0mIt looks like you have an existing rustup settings file at:\n",
      "\u001b[0m\u001b[1m\u001b[33mwarn: \u001b[0m/root/.rustup/settings.toml\n",
      "\u001b[0m\u001b[1m\u001b[33mwarn: \u001b[0mRustup will install the default toolchain as specified in the settings file,\n",
      "\u001b[0m\u001b[1m\u001b[33mwarn: \u001b[0minstead of the one inferred from the default host triple.\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mprofile set to 'default'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdefault host triple is x86_64-unknown-linux-gnu\n",
      "\u001b[0m\u001b[1m\u001b[33mwarn: \u001b[0mUpdating existing toolchain, profile choice will be ignored\n",
      "\u001b[0m\u001b[1minfo: \u001b[0msyncing channel updates for 'stable-x86_64-unknown-linux-gnu'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdefault toolchain set to 'stable-x86_64-unknown-linux-gnu'\n",
      "\n",
      "  \u001b[0m\u001b[1mstable-x86_64-unknown-linux-gnu unchanged\u001b[0m - rustc 1.93.0 (254b59607 2026-01-19)\n",
      "\n",
      "\u001b[0m\u001b[1m\n",
      "Rust is installed now. Great!\n",
      "\u001b[0m\n",
      "To get started you may need to restart your current shell.\n",
      "This would reload your \u001b[0m\u001b[1mPATH\u001b[0m environment variable to include\n",
      "Cargo's bin directory ($HOME/.cargo/bin).\n",
      "\n",
      "To configure your current shell, you need to source\n",
      "the corresponding \u001b[0m\u001b[1menv\u001b[0m file under $HOME/.cargo.\n",
      "\n",
      "This is usually done by running one of the following (note the leading DOT):\n",
      ". \"$HOME/.cargo/env\"            # For sh/bash/zsh/ash/dash/pdksh\n",
      "source \"$HOME/.cargo/env.fish\"  # For fish\n",
      "source $\"($nu.home-path)/.cargo/env.nu\"  # For nushell\n",
      "\u001b[1m\u001b[92m    Updating\u001b[0m crates.io index\n",
      "\u001b[1m\u001b[92m     Ignored\u001b[0m package `cargo-mutants v26.2.0` is already installed, use --force to override\n",
      "\n",
      "============================================================\n",
      "Dependency Verification:\n",
      "============================================================\n",
      "✓ unsloth: 2026.2.1\n",
      "✓ trl: 0.24.0\n",
      "✓ peft: 0.18.1\n",
      "✓ datasets: 4.3.0\n",
      "✓ tiktoken: 0.12.0\n",
      "✓ vllm: 0.15.1\n",
      "✓ cargo: cargo 1.93.0 (083ac5135 2025-12-15)\n",
      "✓ cargo-mutants: cargo-mutants 26.2.0\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Configure Pipeline\n",
    "\n",
    "Edit the variables below to configure the training run.\n",
    "\n",
    "**Training Scope** (`training_scope`):\n",
    "- `full` — All 4 phases end-to-end\n",
    "- `quick_test` — Short runs (100 steps each) to verify setup\n",
    "- `lang_adapter_only` — Only train lang_rust adapter + merge\n",
    "- `skip_to_rl` — Start from existing core_agent checkpoint (IPO + GRPO only)\n",
    "\n",
    "**Other settings:**\n",
    "- `gpu_tier` — Auto-detected below; override if needed\n",
    "- `max_steps_override` — Set >0 to cap all training stages (0 = use defaults)\n",
    "- `skip_data_generation` — Use pre-generated data from Drive\n",
    "- `include_grpo` — GRPO RL is slow; set `False` to skip\n",
    "- `enable_qat_export` — v2: QAT for MXFP4 export (97-100% quality vs 59-89% PTQ)\n",
    "\n",
    "**Service Account** (PyCharm / headless):\n",
    "- `service_account_key` — Path to JSON key on the VM, or leave empty to upload\n",
    "- `drive_folder_id` — Google Drive folder ID for `gpt-oss-20b-rust-agent-v2`\n",
    "\n",
    "> **Note:** The key file must exist on the Colab VM, not your local machine.\n",
    "> If the path doesn't exist, the cell will prompt you to upload via `files.upload()`.\n",
    "> For PyCharm plugin (where `files.upload()` can't show a dialog), manually upload\n",
    "> the key to `/content/service_account.json` before running this cell.\n"
   ],
   "id": "3d85f6b5a5ecf6cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T07:26:40.323817Z",
     "start_time": "2026-02-12T07:26:36.045543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_scope = \"quick_test\"  # \"full\", \"quick_test\", \"lang_adapter_only\", \"skip_to_rl\"\n",
    "\n",
    "gpu_tier = \"h100_80gb\"  # \"a100_40gb\", \"a100_80gb\", \"h100_80gb\"\n",
    "\n",
    "max_steps_override = 0  # Set >0 to cap all stages (0 = use defaults)\n",
    "\n",
    "skip_data_generation = False  # True to use pre-generated data from Drive\n",
    "\n",
    "include_grpo = True  # False to skip GRPO RL (slow)\n",
    "\n",
    "enable_qat_export = False  # True for MXFP4 QAT export\n",
    "\n",
    "\n",
    "service_account_key = \"\"  # Path to service-account JSON key (PyCharm/headless)\n",
    "\n",
    "drive_folder_id = \"\"  # Google Drive folder ID for gpt-oss-20b-rust-agent-v2\n",
    "\n",
    "# ============================================================\n",
    "# SERVICE ACCOUNT KEY — upload to VM if needed\n",
    "# ============================================================\n",
    "# The key file lives on your local machine but the notebook runs on a\n",
    "# remote Colab VM.  This block gets the key onto the VM.\n",
    "\n",
    "_SA_VM_PATH = \"/content/service_account.json\"\n",
    "\n",
    "if use_service_account and not service_account_key:\n",
    "    # No path specified — check if a previous upload exists on the VM\n",
    "    if os.path.exists(_SA_VM_PATH):\n",
    "        service_account_key = _SA_VM_PATH\n",
    "        print(f\"Using previously uploaded key: {_SA_VM_PATH}\")\n",
    "    elif IN_COLAB:\n",
    "        # Upload via browser (works in standard Colab, not PyCharm plugin)\n",
    "        try:\n",
    "            from google.colab import files as _files\n",
    "            print(\"Upload your service-account JSON key file:\")\n",
    "            _uploaded = _files.upload()\n",
    "            if _uploaded:\n",
    "                _name = list(_uploaded.keys())[0]\n",
    "                with open(_SA_VM_PATH, \"wb\") as _f:\n",
    "                    _f.write(_uploaded[_name])\n",
    "                service_account_key = _SA_VM_PATH\n",
    "                print(f\"Saved to {_SA_VM_PATH}\")\n",
    "            else:\n",
    "                print(\"No file uploaded — falling back to local mode (no Drive backup).\")\n",
    "        except Exception as _e:\n",
    "            print(f\"files.upload() failed: {_e}\")\n",
    "            print(\"For PyCharm/headless: paste your key JSON into service_account_json below,\")\n",
    "            print(\"or upload the file manually to the VM at /content/service_account.json\")\n",
    "    else:\n",
    "        print(\"Running locally — set service_account_key to your JSON key path.\")\n",
    "\n",
    "elif use_service_account and service_account_key:\n",
    "    if not os.path.exists(service_account_key):\n",
    "        # Path doesn't exist on the VM — it's probably a local machine path\n",
    "        if os.path.exists(_SA_VM_PATH):\n",
    "            print(f\"Key not found at {service_account_key}\")\n",
    "            print(f\"Using previously uploaded key: {_SA_VM_PATH}\")\n",
    "            service_account_key = _SA_VM_PATH\n",
    "        elif IN_COLAB:\n",
    "            print(f\"Key not found at {service_account_key} (local path?)\")\n",
    "            print(f\"Upload it to the Colab VM:\")\n",
    "            try:\n",
    "                from google.colab import files as _files\n",
    "                _uploaded = _files.upload()\n",
    "                if _uploaded:\n",
    "                    _name = list(_uploaded.keys())[0]\n",
    "                    with open(_SA_VM_PATH, \"wb\") as _f:\n",
    "                        _f.write(_uploaded[_name])\n",
    "                    service_account_key = _SA_VM_PATH\n",
    "                    print(f\"Saved to {_SA_VM_PATH}\")\n",
    "            except Exception as _e:\n",
    "                print(f\"files.upload() failed: {_e}\")\n",
    "                print(\"Upload manually to /content/service_account.json and re-run.\")\n",
    "                service_account_key = \"\"\n",
    "\n",
    "# ============================================================\n",
    "# DRIVE MODE\n",
    "# ============================================================\n",
    "from scripts.pipeline_lib.drive_utils import DriveHelper\n",
    "\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/gpt-oss-20b-rust-agent-v2\"\n",
    "\n",
    "if DRIVE_MOUNTED:\n",
    "    DRIVE_MODE = \"mounted\"\n",
    "elif use_service_account and service_account_key and drive_folder_id:\n",
    "    DRIVE_MODE = \"service_account\"\n",
    "else:\n",
    "    DRIVE_MODE = \"local\"\n",
    "\n",
    "drive_helper = DriveHelper(\n",
    "    mode=DRIVE_MODE,\n",
    "    drive_base=DRIVE_BASE,\n",
    "    credentials_path=service_account_key or None,\n",
    "    folder_id=drive_folder_id or None,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# v2 GPU TIER CONFIGS (with H100 FP8 tier)\n",
    "# ============================================================\n",
    "\n",
    "GPU_CONFIGS = {\n",
    "    \"a100_40gb\": {\n",
    "        \"moe_backend\": \"unsloth_triton\",\n",
    "        \"load_mode\": \"4bit\",\n",
    "        \"fast_inference\": False,\n",
    "        \"lang_rust\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 8192, \"max_steps\": 3000},\n",
    "        \"core_agent\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 12288, \"max_steps\": 2000},\n",
    "        \"ipo\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 12288, \"max_steps\": 1000},\n",
    "        \"grpo\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 16384, \"max_steps\": 2000, \"num_gen\": 2},\n",
    "    },\n",
    "    \"a100_80gb\": {\n",
    "        \"moe_backend\": \"unsloth_triton\",\n",
    "        \"load_mode\": \"4bit\",\n",
    "        \"fast_inference\": False,\n",
    "        \"lang_rust\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 8192, \"max_steps\": 5000},\n",
    "        \"core_agent\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 16384, \"max_steps\": 3000},\n",
    "        \"ipo\": {\"batch\": 1, \"grad_accum\": 16, \"seq_len\": 16384, \"max_steps\": 2000},\n",
    "        \"grpo\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 32768, \"max_steps\": 5000, \"num_gen\": 4},\n",
    "    },\n",
    "    \"h100_80gb\": {\n",
    "        \"moe_backend\": \"grouped_mm\",\n",
    "        \"load_mode\": \"fp8\",\n",
    "        \"fast_inference\": True,\n",
    "        \"lang_rust\": {\"batch\": 2, \"grad_accum\": 4, \"seq_len\": 8192, \"max_steps\": 5000},\n",
    "        \"core_agent\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 16384, \"max_steps\": 3000},\n",
    "        \"ipo\": {\"batch\": 1, \"grad_accum\": 16, \"seq_len\": 16384, \"max_steps\": 2000},\n",
    "        \"grpo\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 65536, \"max_steps\": 7000, \"num_gen\": 4},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Quick test overrides\n",
    "if training_scope == \"quick_test\":\n",
    "    max_steps_override = 100\n",
    "\n",
    "gpu_cfg = GPU_CONFIGS[gpu_tier]\n",
    "\n",
    "# Detect CPU count and RAM for parallel mutation jobs.\n",
    "# Each cargo-mutants worker spawns cargo build/test subprocesses that can\n",
    "# each use 1-2 GB RAM.  Cap jobs to avoid OOM kills on Colab instances.\n",
    "import multiprocessing\n",
    "import os as _os\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "\n",
    "# RAM-aware job limit: allow ~20 GB per mutation worker for headroom\n",
    "try:\n",
    "    _mem_bytes = _os.sysconf('SC_PAGE_SIZE') * _os.sysconf('SC_PHYS_PAGES')\n",
    "    total_ram_gb = _mem_bytes / (1024**3)\n",
    "    ram_based_jobs = max(1, int(total_ram_gb / 20))\n",
    "except (ValueError, OSError):\n",
    "    total_ram_gb = 0\n",
    "    ram_based_jobs = cpu_count\n",
    "\n",
    "mutation_jobs = min(max(1, cpu_count - 2), ram_based_jobs)\n",
    "\n",
    "# Build CONFIG dict\n",
    "CONFIG = {\n",
    "    \"training_scope\": training_scope,\n",
    "    \"gpu_tier\": gpu_tier,\n",
    "    \"include_grpo\": include_grpo,\n",
    "    \"skip_data_generation\": skip_data_generation,\n",
    "    \"enable_qat_export\": enable_qat_export,\n",
    "    # v2: MoE backend + load mode\n",
    "    \"moe_backend\": gpu_cfg[\"moe_backend\"],\n",
    "    \"load_mode\": gpu_cfg[\"load_mode\"],\n",
    "    \"fast_inference\": gpu_cfg[\"fast_inference\"],\n",
    "    # Lang adapter\n",
    "    \"lang_rust_batch\": gpu_cfg[\"lang_rust\"][\"batch\"],\n",
    "    \"lang_rust_grad_accum\": gpu_cfg[\"lang_rust\"][\"grad_accum\"],\n",
    "    \"lang_rust_seq_len\": gpu_cfg[\"lang_rust\"][\"seq_len\"],\n",
    "    \"lang_rust_max_steps\": max_steps_override or gpu_cfg[\"lang_rust\"][\"max_steps\"],\n",
    "    # Core agent\n",
    "    \"core_agent_batch\": gpu_cfg[\"core_agent\"][\"batch\"],\n",
    "    \"core_agent_grad_accum\": gpu_cfg[\"core_agent\"][\"grad_accum\"],\n",
    "    \"core_agent_seq_len\": gpu_cfg[\"core_agent\"][\"seq_len\"],\n",
    "    \"core_agent_max_steps\": max_steps_override or gpu_cfg[\"core_agent\"][\"max_steps\"],\n",
    "    # IPO\n",
    "    \"ipo_batch\": gpu_cfg[\"ipo\"][\"batch\"],\n",
    "    \"ipo_grad_accum\": gpu_cfg[\"ipo\"][\"grad_accum\"],\n",
    "    \"ipo_seq_len\": gpu_cfg[\"ipo\"][\"seq_len\"],\n",
    "    \"ipo_max_steps\": max_steps_override or gpu_cfg[\"ipo\"][\"max_steps\"],\n",
    "    # GRPO\n",
    "    \"grpo_batch\": gpu_cfg[\"grpo\"][\"batch\"],\n",
    "    \"grpo_grad_accum\": gpu_cfg[\"grpo\"][\"grad_accum\"],\n",
    "    \"grpo_seq_len\": gpu_cfg[\"grpo\"][\"seq_len\"],\n",
    "    \"grpo_max_steps\": max_steps_override or gpu_cfg[\"grpo\"][\"max_steps\"],\n",
    "    \"grpo_num_gen\": gpu_cfg[\"grpo\"][\"num_gen\"],\n",
    "    # Mutation generation — balance CPU parallelism with RAM headroom\n",
    "    \"max_mutations_per_repo\": 50 if training_scope == \"quick_test\" else 100,\n",
    "    \"mutation_jobs\": mutation_jobs,\n",
    "    # Eval\n",
    "    \"eval_num_samples\": 10 if training_scope == \"quick_test\" else 50,\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PIPELINE CONFIGURATION (v2)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nScope: {training_scope.upper()}\")\n",
    "print(f\"GPU tier: {gpu_tier}\")\n",
    "ram_str = f\" | RAM: {total_ram_gb:.0f} GB\" if total_ram_gb else \"\"\n",
    "print(f\"CPUs: {cpu_count}{ram_str} (mutation jobs: {CONFIG['mutation_jobs']})\")\n",
    "print(f\"MoE backend: {CONFIG['moe_backend']}\")\n",
    "print(f\"Load mode: {CONFIG['load_mode']}\")\n",
    "print(f\"Fast inference (vLLM): {CONFIG['fast_inference']}\")\n",
    "print(f\"Include GRPO: {include_grpo}\")\n",
    "print(f\"QAT export: {enable_qat_export}\")\n",
    "print(f\"Skip data gen: {skip_data_generation}\")\n",
    "print(f\"Drive mode: {DRIVE_MODE}\")\n",
    "if max_steps_override:\n",
    "    print(f\"Max steps override: {max_steps_override}\")\n",
    "print(f\"\\nLang Adapter:  batch={CONFIG['lang_rust_batch']} x grad_accum={CONFIG['lang_rust_grad_accum']}, seq={CONFIG['lang_rust_seq_len']}, steps={CONFIG['lang_rust_max_steps']}\")\n",
    "print(f\"Core Agent:    batch={CONFIG['core_agent_batch']} x grad_accum={CONFIG['core_agent_grad_accum']}, seq={CONFIG['core_agent_seq_len']}, steps={CONFIG['core_agent_max_steps']}\")\n",
    "print(f\"IPO:           batch={CONFIG['ipo_batch']} x grad_accum={CONFIG['ipo_grad_accum']}, seq={CONFIG['ipo_seq_len']}, steps={CONFIG['ipo_max_steps']}\")\n",
    "if include_grpo:\n",
    "    print(f\"GRPO:          batch={CONFIG['grpo_batch']} x grad_accum={CONFIG['grpo_grad_accum']}, seq={CONFIG['grpo_seq_len']}, steps={CONFIG['grpo_max_steps']}, gen={CONFIG['grpo_num_gen']}\")\n",
    "print(\"=\" * 60)\n"
   ],
   "id": "93dd91bddbc4ed84",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Set Up Persistent Storage\n"
   ],
   "id": "aa77eea7bd7df89e"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T07:27:02.013162Z",
     "start_time": "2026-02-12T07:27:01.756122Z"
    }
   },
   "source": [
    "DRIVE_SUBDIRS = [\n",
    "    \"checkpoints/lang_rust\",\n",
    "    \"checkpoints/core_agent\",\n",
    "    \"checkpoints/core_agent_ipo\",\n",
    "    \"checkpoints/core_agent_grpo\",\n",
    "    \"checkpoints/gpt-oss-20b-rust-merged\",\n",
    "    \"data/rust/lang_rust\",\n",
    "    \"data/rust/core_agent\",\n",
    "    \"data/rust/mutations\",\n",
    "    \"data/rust/ipo\",\n",
    "    \"data/rust/grpo\",\n",
    "    \"data/rust/eval\",\n",
    "    \"data/rust/repos\",\n",
    "    \"logs\",\n",
    "    \"evals/rust_agent\",\n",
    "]\n",
    "\n",
    "if DRIVE_MODE == \"mounted\":\n",
    "    # Mounted mode: create Drive dirs + symlink local → Drive (original behaviour)\n",
    "    print(f\"Setting up storage at: {DRIVE_BASE}\")\n",
    "    for subdir in DRIVE_SUBDIRS:\n",
    "        os.makedirs(os.path.join(DRIVE_BASE, subdir), exist_ok=True)\n",
    "\n",
    "    for dir_name in [\"checkpoints\", \"data\", \"logs\", \"evals\"]:\n",
    "        local_path = os.path.join(PROJECT_ROOT, dir_name)\n",
    "        drive_path = os.path.join(DRIVE_BASE, dir_name)\n",
    "\n",
    "        if os.path.exists(local_path) and not os.path.islink(local_path):\n",
    "            !cp -r {local_path}/* {drive_path}/ 2>/dev/null || true\n",
    "            !rm -rf {local_path}\n",
    "        elif os.path.islink(local_path):\n",
    "            os.unlink(local_path)\n",
    "\n",
    "        os.symlink(drive_path, local_path)\n",
    "        print(f\"  {dir_name} -> Drive (mounted)\")\n",
    "\n",
    "elif DRIVE_MODE == \"service_account\":\n",
    "    # Service-account mode: create local dirs, restore existing data from Drive\n",
    "    print(\"Setting up local storage + Drive API restore...\")\n",
    "    for subdir in DRIVE_SUBDIRS:\n",
    "        os.makedirs(os.path.join(PROJECT_ROOT, subdir), exist_ok=True)\n",
    "        drive_helper.ensure_dir(subdir)\n",
    "\n",
    "    for dir_name in [\"checkpoints\", \"data\", \"logs\", \"evals\"]:\n",
    "        local_path = os.path.join(PROJECT_ROOT, dir_name)\n",
    "        # Remove stale symlinks from previous mounted runs\n",
    "        if os.path.islink(local_path):\n",
    "            os.unlink(local_path)\n",
    "            os.makedirs(local_path, exist_ok=True)\n",
    "        print(f\"  {dir_name} -> local (backed up via Drive API)\")\n",
    "\n",
    "    print(\"\\nRestoring existing data from Drive...\")\n",
    "    for subdir in DRIVE_SUBDIRS:\n",
    "        local_target = os.path.join(PROJECT_ROOT, subdir)\n",
    "        drive_helper.restore(subdir, local_target)\n",
    "    print(\"Restore complete.\")\n",
    "\n",
    "else:\n",
    "    # Local-only mode — no Drive\n",
    "    for d in [\"checkpoints\", \"data/rust\", \"logs\", \"evals/rust_agent\"]:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "    print(\"Local directories created (no Drive backup).\")\n",
    "\n",
    "print(\"\\nStorage ready!\")\n"
   ],
   "id": "ef3dbe47b760cca0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local directories created (no Drive backup).\n",
      "\n",
      "Storage ready!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5 Check GPU & Configure MoE Backend\n",
    "\n",
    "v2: Auto-detects H100 for FP8 RL and sets the optimal Split LoRA backend.\n"
   ],
   "id": "ec0e7828c971b71a"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T07:27:11.060018Z",
     "start_time": "2026-02-12T07:27:10.633136Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    capability = torch.cuda.get_device_capability()\n",
    "    is_h100 = \"H100\" in gpu_name or \"H200\" in gpu_name or \"B200\" in gpu_name\n",
    "\n",
    "    CONFIG[\"use_fp8\"] = capability[0] >= 9 and is_h100\n",
    "\n",
    "    # v2: Auto-detect GPU tier (now includes H100)\n",
    "    if is_h100:\n",
    "        detected_tier = \"h100_80gb\"\n",
    "    elif gpu_memory >= 70:\n",
    "        detected_tier = \"a100_80gb\"\n",
    "    else:\n",
    "        detected_tier = \"a100_40gb\"\n",
    "\n",
    "    if detected_tier != CONFIG[\"gpu_tier\"]:\n",
    "        print(f\"NOTE: Auto-detected {detected_tier}, overriding configured {CONFIG['gpu_tier']}\")\n",
    "        CONFIG[\"gpu_tier\"] = detected_tier\n",
    "        # Re-derive tier-specific settings\n",
    "        gpu_cfg = GPU_CONFIGS[detected_tier]\n",
    "        CONFIG[\"moe_backend\"] = gpu_cfg[\"moe_backend\"]\n",
    "        CONFIG[\"load_mode\"] = gpu_cfg[\"load_mode\"]\n",
    "        CONFIG[\"fast_inference\"] = gpu_cfg[\"fast_inference\"]\n",
    "\n",
    "    # v2: Set Split LoRA MoE backend\n",
    "    os.environ[\"UNSLOTH_MOE_BACKEND\"] = CONFIG[\"moe_backend\"]\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"GPU: {gpu_name} ({gpu_memory:.0f} GB)\")\n",
    "    print(f\"Compute capability: {capability[0]}.{capability[1]}\")\n",
    "    print(f\"Tier: {CONFIG['gpu_tier']}\")\n",
    "    print(f\"\\nv2 Optimisations:\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(f\"  Load mode: {CONFIG['load_mode']}\")\n",
    "    print(f\"  FP8 available: {CONFIG['use_fp8']}\")\n",
    "    print(f\"  Fast inference (vLLM): {CONFIG['fast_inference']}\")\n",
    "\n",
    "    if gpu_memory < 40:\n",
    "        print(\"\\nWARNING: <40 GB VRAM. Long-context training (16K+) may OOM.\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"No GPU detected!\")\n",
    "    CONFIG[\"use_fp8\"] = False\n",
    "    os.environ[\"UNSLOTH_MOE_BACKEND\"] = \"native_torch\""
   ],
   "id": "f189d07bd7e89131",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GPU: NVIDIA H100 80GB HBM3 (79 GB)\n",
      "Compute capability: 9.0\n",
      "Tier: h100_80gb\n",
      "\n",
      "v2 Optimisations:\n",
      "  Split LoRA backend: grouped_mm\n",
      "  Load mode: fp8\n",
      "  FP8 available: True\n",
      "  Fast inference (vLLM): True\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Data Generation\n",
    "\n",
    "Generates mutation data from curated Rust repos and agent trajectories.\n",
    "Skip this step if you have pre-generated data on Drive (`skip_data_generation=True`)."
   ],
   "id": "7f93fc1acaa43c5f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Generate Mutation Data\n",
    "\n",
    "Runs `cargo-mutants` on curated Rust repos to produce bug-fix training pairs.\n"
   ],
   "id": "3db2daedc13bc633"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T08:02:49.404007Z",
     "start_time": "2026-02-12T07:27:18.660955Z"
    }
   },
   "source": [
    "if CONFIG[\"skip_data_generation\"]:\n",
    "    print(\"Skipping data generation (using pre-generated data from Drive)\")\n",
    "elif CONFIG[\"training_scope\"] in (\"skip_to_rl\",):\n",
    "    print(\"Skipping — not needed for this training scope\")\n",
    "else:\n",
    "    max_muts = CONFIG[\"max_mutations_per_repo\"]\n",
    "    jobs = CONFIG[\"mutation_jobs\"]\n",
    "\n",
    "    print(f\"Generating mutations (max {max_muts}/repo, {jobs} parallel jobs)...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/16_generate_mutations.py \\\n",
    "        --max_mutations_per_repo {max_muts} \\\n",
    "        --jobs {jobs}\n",
    "\n",
    "    drive_helper.backup(\"data/rust/mutations\", \"data/rust/mutations\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nBacked up mutations to Drive.\")\n"
   ],
   "id": "a06c4e99d8d41ffc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating mutations (max 50/repo, 11 parallel jobs)...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Generating Mutation Training Data\n",
      "============================================================\n",
      "\n",
      "Loaded 21 repos\n",
      "\n",
      "[1/21] Processing https://github.com/BurntSushi/bstr...\n",
      "  Cloned https://github.com/BurntSushi/bstr -> /tmp/rust_repos/bstr\n",
      "  Running: cargo mutants --timeout 300 --jobs 11 --output /tmp/mutants_vcspirs4 --json\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Generate Agent Trajectories\n",
    "\n",
    "Generates multi-turn agent trajectories from mutations + Strandset in Harmony format.\n"
   ],
   "id": "fc8a2b5d9669022f"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"skip_data_generation\"]:\n",
    "    print(\"Skipping data generation (using pre-generated data from Drive)\")\n",
    "elif CONFIG[\"training_scope\"] in (\"skip_to_rl\",):\n",
    "    print(\"Skipping — not needed for this training scope\")\n",
    "else:\n",
    "    max_samples = 500 if CONFIG[\"training_scope\"] == \"quick_test\" else 5000\n",
    "\n",
    "    print(f\"Generating trajectories (max {max_samples} per source)...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    cmd = f\"python scripts/15_generate_trajectories.py --max_samples {max_samples}\"\n",
    "\n",
    "    mutations_path = \"data/rust/mutations/mutations.jsonl\"\n",
    "    if os.path.exists(mutations_path):\n",
    "        cmd += f\" --mutations_path {mutations_path}\"\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"data/rust/core_agent\", \"data/rust/core_agent\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nBacked up trajectories to Drive.\")\n"
   ],
   "id": "4ce7bebf4b7f876c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Verify Data\n"
   ],
   "id": "6de8a13dcfeedf86"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data_checks = [\n",
    "    (\"Mutations\", \"data/rust/mutations\"),\n",
    "    (\"Lang Rust train\", \"data/rust/lang_rust/train\"),\n",
    "    (\"Core Agent train\", \"data/rust/core_agent/train\"),\n",
    "    (\"IPO train\", \"data/rust/ipo/train\"),\n",
    "    (\"GRPO tasks\", \"data/rust/grpo\"),\n",
    "    (\"Eval tasks\", \"data/rust/eval\"),\n",
    "]\n",
    "\n",
    "print(\"Data Verification:\")\n",
    "print(\"=\" * 60)\n",
    "for name, path in data_checks:\n",
    "    exists = os.path.exists(path)\n",
    "    if exists and os.path.isdir(path):\n",
    "        items = os.listdir(path)\n",
    "        print(f\"  \\u2713 {name}: {path} ({len(items)} items)\")\n",
    "    elif exists:\n",
    "        size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "        print(f\"  \\u2713 {name}: {path} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        needed = True\n",
    "        if CONFIG[\"training_scope\"] == \"skip_to_rl\" and name in (\"Mutations\", \"Lang Rust train\", \"Core Agent train\"):\n",
    "            needed = False\n",
    "        if CONFIG[\"training_scope\"] == \"lang_adapter_only\" and name in (\"IPO train\", \"GRPO tasks\"):\n",
    "            needed = False\n",
    "        sym = \"\\u2717\" if needed else \"\\u2014\"\n",
    "        label = \"MISSING\" if needed else \"not needed\"\n",
    "        print(f\"  {sym} {name}: {label}\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "861c7d3cd76a69d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Lang Adapter Training\n",
    "\n",
    "Train a QLoRA adapter (rank 64) to specialise GPT-OSS 20B on Rust syntax, stdlib, and idioms.\n",
    "Then merge the adapter into the base weights for downstream training.\n",
    "\n",
    "**v2:** Split LoRA backend auto-enabled for 7-12x faster MoE training."
   ],
   "id": "31fd053ef297615e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Train lang_rust Adapter\n",
    "\n",
    "v2: Split LoRA enabled via UNSLOTH_MOE_BACKEND env var (set in 0.5).\n"
   ],
   "id": "fd35fdde7902ddda"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "    print(\"Skipping — scope is skip_to_rl\")\n",
    "else:\n",
    "    batch = CONFIG[\"lang_rust_batch\"]\n",
    "    grad_accum = CONFIG[\"lang_rust_grad_accum\"]\n",
    "    max_steps = CONFIG[\"lang_rust_max_steps\"]\n",
    "    seq_len = CONFIG[\"lang_rust_seq_len\"]\n",
    "\n",
    "    cmd = f\"python scripts/13_train_lang_adapter.py\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    print(f\"Training lang_rust adapter...\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Seq length: {seq_len} (from config)\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/lang_rust\", \"checkpoints/lang_rust\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")\n"
   ],
   "id": "4719fb4569fd4a48",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Merge lang_rust into Base\n"
   ],
   "id": "6d945162fc6c4887"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "    print(\"Skipping — scope is skip_to_rl\")\n",
    "else:\n",
    "    print(\"Merging lang_rust adapter into base model...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/19_merge_adapter.py \\\n",
    "        --adapter_path checkpoints/lang_rust/final \\\n",
    "        --output_dir checkpoints/gpt-oss-20b-rust-merged \\\n",
    "        --export_formats hf\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/gpt-oss-20b-rust-merged\", \"checkpoints/gpt-oss-20b-rust-merged\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nMerged model backed up to Drive.\")\n"
   ],
   "id": "14dd6ab10bc01ad0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Verify Merge\n"
   ],
   "id": "8a649a98ab1ceee0"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "    print(\"Skipping \\u2014 scope is skip_to_rl\")\n",
    "else:\n",
    "    merged_path = \"checkpoints/gpt-oss-20b-rust-merged\"\n",
    "    adapter_path = \"checkpoints/lang_rust/final\"\n",
    "\n",
    "    print(\"Merge Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(merged_path):\n",
    "        files = os.listdir(merged_path)\n",
    "        safetensors = [f for f in files if f.endswith(\".safetensors\")]\n",
    "        print(f\"  \\u2713 Merged model: {merged_path}\")\n",
    "        print(f\"    {len(safetensors)} safetensors shard(s), {len(files)} total files\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 Merged model not found at {merged_path}\")\n",
    "\n",
    "    if os.path.exists(adapter_path):\n",
    "        adapter_files = os.listdir(adapter_path)\n",
    "        print(f\"  \\u2713 Adapter: {adapter_path} ({len(adapter_files)} files)\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 Adapter not found at {adapter_path}\")\n",
    "\n",
    "    if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "        print(\"\\n\\u2713 lang_adapter_only scope complete. Stopping here.\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ],
   "id": "875f898a2de17db",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Core Agent SFT\n",
    "\n",
    "Train a higher-rank LoRA adapter (rank 128) on agent trajectories with tool use.\n",
    "Uses the merged lang_rust model as the base.\n",
    "\n",
    "**v2:** Auto uncontaminated packing (3x faster, zero-config). Flex Attention for long context."
   ],
   "id": "603346a356305aad"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Train core_agent Adapter\n",
    "\n",
    "v2: Auto packing (3x faster) + Split LoRA backend enabled.\n"
   ],
   "id": "2931e91685b890ab"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] in (\"lang_adapter_only\", \"skip_to_rl\"):\n",
    "    print(f\"Skipping — scope is {CONFIG['training_scope']}\")\n",
    "else:\n",
    "    batch = CONFIG[\"core_agent_batch\"]\n",
    "    grad_accum = CONFIG[\"core_agent_grad_accum\"]\n",
    "    max_steps = CONFIG[\"core_agent_max_steps\"]\n",
    "    seq_len = CONFIG[\"core_agent_seq_len\"]\n",
    "\n",
    "    cmd = f\"python scripts/14_train_core_agent.py\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    print(f\"Training core_agent adapter...\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Seq length: {seq_len} (from config)\")\n",
    "    print(f\"  LoRA rank: 128\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(f\"  Auto packing: enabled (uncontaminated)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/core_agent\", \"checkpoints/core_agent\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")\n"
   ],
   "id": "50b1d5d150eae5d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Verify core_agent\n"
   ],
   "id": "9f890c4676329aad"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] in (\"lang_adapter_only\", \"skip_to_rl\"):\n",
    "    print(f\"Skipping \\u2014 scope is {CONFIG['training_scope']}\")\n",
    "else:\n",
    "    ckpt_path = \"checkpoints/core_agent/final\"\n",
    "\n",
    "    print(\"Core Agent Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(ckpt_path):\n",
    "        files = os.listdir(ckpt_path)\n",
    "        print(f\"  \\u2713 Checkpoint: {ckpt_path} ({len(files)} files)\")\n",
    "\n",
    "        adapter_config = os.path.join(ckpt_path, \"adapter_config.json\")\n",
    "        if os.path.exists(adapter_config):\n",
    "            import json\n",
    "            with open(adapter_config) as f:\n",
    "                cfg = json.load(f)\n",
    "            print(f\"    LoRA rank: {cfg.get('r', '?')}\")\n",
    "            print(f\"    Alpha: {cfg.get('lora_alpha', '?')}\")\n",
    "            print(f\"    Target modules: {cfg.get('target_modules', '?')}\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 Checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ],
   "id": "7935e2cb9e427223",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Preference Optimisation (IPO)\n",
    "\n",
    "Train with Identity Preference Optimisation on ranked pairs.\n",
    "Very low learning rate (5e-7), 1 epoch only to avoid collapse.\n",
    "\n",
    "**v2:** FP8 weights on H100 (60% less VRAM). Expert utilisation monitoring."
   ],
   "id": "4dba79ff4175093b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Train with IPO\n",
    "\n",
    "v2: FP8 on H100, expert utilisation monitoring, Split LoRA.\n"
   ],
   "id": "54a5adb1ec756442"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping — scope is lang_adapter_only\")\n",
    "else:\n",
    "    batch = CONFIG[\"ipo_batch\"]\n",
    "    grad_accum = CONFIG[\"ipo_grad_accum\"]\n",
    "    max_steps = CONFIG[\"ipo_max_steps\"]\n",
    "\n",
    "    if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "        ipo_checkpoint = \"checkpoints/core_agent/final\"\n",
    "        print(\"Using existing core_agent checkpoint (skip_to_rl mode)\")\n",
    "    else:\n",
    "        ipo_checkpoint = \"checkpoints/core_agent/final\"\n",
    "\n",
    "    cmd = f\"python scripts/17_ipo_preference.py\"\n",
    "    cmd += f\" --checkpoint {ipo_checkpoint}\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    print(f\"Training with IPO...\")\n",
    "    print(f\"  Checkpoint: {ipo_checkpoint}\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Loss: IPO (beta=0.1)\")\n",
    "    print(f\"  Load mode: {CONFIG['load_mode']}\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/core_agent_ipo\", \"checkpoints/core_agent_ipo\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")\n"
   ],
   "id": "d1401e64737e0b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Verify IPO\n"
   ],
   "id": "442df61074a8cd86"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    ckpt_path = \"checkpoints/core_agent_ipo/final\"\n",
    "\n",
    "    print(\"IPO Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(ckpt_path):\n",
    "        files = os.listdir(ckpt_path)\n",
    "        print(f\"  \\u2713 IPO checkpoint: {ckpt_path} ({len(files)} files)\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 IPO checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "    # Check tensorboard logs for KL divergence\n",
    "    tb_dir = \"checkpoints/core_agent_ipo\"\n",
    "    tb_files = []\n",
    "    if os.path.exists(tb_dir):\n",
    "        for root, dirs, fnames in os.walk(tb_dir):\n",
    "            for fn in fnames:\n",
    "                if fn.startswith(\"events.out.tfevents\"):\n",
    "                    tb_files.append(os.path.join(root, fn))\n",
    "    if tb_files:\n",
    "        print(f\"  \\u2713 TensorBoard logs found ({len(tb_files)} event files)\")\n",
    "        print(f\"    Monitor KL divergence: warn >0.3, abort >0.5\")\n",
    "    else:\n",
    "        print(f\"  \\u2014 No TensorBoard logs found\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ],
   "id": "d83944ef11775585",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: GRPO RL\n",
    "\n",
    "Group Relative Policy Optimisation with execution-based rewards.\n",
    "Generates N completions per prompt, runs `cargo check/test/clippy`, computes group-relative advantages.\n",
    "\n",
    "**v2 Optimisations:**\n",
    "- FP8 RL with vLLM inference on H100 (1.6x throughput)\n",
    "- Chunked batching for 7x longer context\n",
    "- Extended curriculum: 65K context on H100 (up from 32K)\n",
    "- Harmony format compliance reward to prevent infinite reasoning loops\n",
    "\n",
    "**This step is optional** (`include_grpo=False` to skip)."
   ],
   "id": "a10e7f0a4032c87e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Train with GRPO\n",
    "\n",
    "v2: FP8 RL + vLLM (H100), chunked batching, extended curriculum.\n"
   ],
   "id": "640c22f7ecce99e7"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping — scope is lang_adapter_only\")\n",
    "elif not CONFIG[\"include_grpo\"]:\n",
    "    print(\"Skipping — GRPO disabled (include_grpo=False)\")\n",
    "else:\n",
    "    batch = CONFIG[\"grpo_batch\"]\n",
    "    grad_accum = CONFIG[\"grpo_grad_accum\"]\n",
    "    max_steps = CONFIG[\"grpo_max_steps\"]\n",
    "    max_seq = CONFIG[\"grpo_seq_len\"]\n",
    "\n",
    "    grpo_checkpoint = \"checkpoints/core_agent_ipo/final\"\n",
    "\n",
    "    cmd = f\"python scripts/18_grpo_rl.py\"\n",
    "    cmd += f\" --checkpoint {grpo_checkpoint}\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    # v2: Note which optimisations are active\n",
    "    v2_features = []\n",
    "    v2_features.append(f\"Split LoRA ({CONFIG['moe_backend']})\")\n",
    "    if CONFIG[\"load_mode\"] == \"fp8\":\n",
    "        v2_features.append(\"FP8 weights\")\n",
    "    if CONFIG[\"fast_inference\"]:\n",
    "        v2_features.append(\"vLLM inference\")\n",
    "    v2_features.append(\"Chunked batching (auto)\")\n",
    "    v2_features.append(\"Auto packing\")\n",
    "\n",
    "    if CONFIG[\"gpu_tier\"] == \"a100_40gb\":\n",
    "        print(\"NOTE: 40GB GPU — GRPO sequence length capped at 16384\")\n",
    "\n",
    "    print(f\"Training with GRPO (v2)...\")\n",
    "    print(f\"  Checkpoint: {grpo_checkpoint}\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Max seq length: {max_seq}\")\n",
    "    print(f\"  Generations per prompt: {CONFIG['grpo_num_gen']}\")\n",
    "    print(f\"\\n  v2 features active:\")\n",
    "    for feat in v2_features:\n",
    "        print(f\"    ✓ {feat}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/core_agent_grpo\", \"checkpoints/core_agent_grpo\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")\n"
   ],
   "id": "ced528c03911d4d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Verify GRPO\n"
   ],
   "id": "34e15c6770525e08"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "elif not CONFIG[\"include_grpo\"]:\n",
    "    print(\"Skipping \\u2014 GRPO disabled\")\n",
    "else:\n",
    "    ckpt_path = \"checkpoints/core_agent_grpo/final\"\n",
    "\n",
    "    print(\"GRPO Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(ckpt_path):\n",
    "        files = os.listdir(ckpt_path)\n",
    "        print(f\"  \\u2713 GRPO checkpoint: {ckpt_path} ({len(files)} files)\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 GRPO checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ],
   "id": "1f051b9601545463",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Evaluation\n",
    "\n",
    "Evaluate the best checkpoint on held-out Rust tasks using execution-based metrics\n",
    "(cargo check, cargo test, clippy)."
   ],
   "id": "c3395dfa3ecbc680"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Run Rust Evaluation\n"
   ],
   "id": "1fb7069b4d6fddef"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping — scope is lang_adapter_only\")\n",
    "else:\n",
    "    # Determine best checkpoint\n",
    "    if CONFIG[\"include_grpo\"] and os.path.exists(\"checkpoints/core_agent_grpo/final\"):\n",
    "        eval_checkpoint = \"checkpoints/core_agent_grpo/final\"\n",
    "    elif os.path.exists(\"checkpoints/core_agent_ipo/final\"):\n",
    "        eval_checkpoint = \"checkpoints/core_agent_ipo/final\"\n",
    "    elif os.path.exists(\"checkpoints/core_agent/final\"):\n",
    "        eval_checkpoint = \"checkpoints/core_agent/final\"\n",
    "    else:\n",
    "        eval_checkpoint = \"checkpoints/core_agent_ipo/final\"\n",
    "\n",
    "    num_samples = CONFIG[\"eval_num_samples\"]\n",
    "\n",
    "    print(f\"Evaluating checkpoint: {eval_checkpoint}\")\n",
    "    print(f\"Samples: {num_samples}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/eval_rust_agent.py \\\n",
    "        --checkpoint {eval_checkpoint} \\\n",
    "        --num_samples {num_samples}\n",
    "\n",
    "    drive_helper.backup(\"evals/rust_agent\", \"evals/rust_agent\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nResults backed up to Drive.\")\n"
   ],
   "id": "bad0cdf0e5a54cb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Check Promotion Gates\n"
   ],
   "id": "b0a563f4effe713f"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    print(\"Checking promotion gates...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/12_check_gates.py rust_agent"
   ],
   "id": "4197d776a316d2cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Display Results\n"
   ],
   "id": "d213ef309b2bacd7"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    import json\n",
    "\n",
    "    metrics_path = \"evals/rust_agent/metrics.json\"\n",
    "\n",
    "    if os.path.exists(metrics_path):\n",
    "        with open(metrics_path) as f:\n",
    "            metrics = json.load(f)\n",
    "\n",
    "        targets = {\n",
    "            \"cargo_check_pass_rate\": (0.85, \"higher\"),\n",
    "            \"cargo_test_pass_rate\": (0.70, \"higher\"),\n",
    "            \"clippy_clean_rate\": (0.80, \"higher\"),\n",
    "            \"iterations_to_green_median\": (3, \"lower\"),\n",
    "            \"diff_size_median\": (50, \"lower\"),\n",
    "            \"tool_call_format_accuracy\": (0.99, \"higher\"),\n",
    "            \"hallucinated_api_rate\": (0.05, \"lower\"),\n",
    "        }\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "        print(\"EVALUATION RESULTS\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"{'Metric':<32} {'Value':>8} {'Target':>8} {'Status':>8}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        for key, (target, direction) in targets.items():\n",
    "            value = metrics.get(key)\n",
    "            if value is None:\n",
    "                print(f\"{key:<32} {'N/A':>8} {target:>8} {'\\u2014':>8}\")\n",
    "                continue\n",
    "\n",
    "            if direction == \"higher\":\n",
    "                passed = value >= target\n",
    "            else:\n",
    "                passed = value <= target\n",
    "\n",
    "            status = \"\\u2713 PASS\" if passed else \"\\u2717 FAIL\"\n",
    "            fmt_val = f\"{value:.2%}\" if isinstance(value, float) and value <= 1 else f\"{value}\"\n",
    "            fmt_tgt = f\"{target:.0%}\" if isinstance(target, float) and target <= 1 else f\"{target}\"\n",
    "            print(f\"{key:<32} {fmt_val:>8} {fmt_tgt:>8} {status:>8}\")\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "    else:\n",
    "        print(f\"\\u2717 Metrics file not found at {metrics_path}\")\n",
    "        print(\"Run evaluation (6.1) first.\")"
   ],
   "id": "6ff37a75b3dd990f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Test Model\n",
    "\n",
    "Load the trained model and generate Rust code interactively.\n",
    "\n",
    "**v2:** FP8 loading on H100 for faster inference. `fast_inference=True` enables vLLM backend."
   ],
   "id": "856732da9a5a7910"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Load Model\n",
    "\n",
    "v2: FP8 loading on H100, vLLM-backed inference.\n"
   ],
   "id": "454dd084fa7f33d0"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "CHECKPOINT_PRIORITY = [\n",
    "    \"checkpoints/core_agent_grpo/final\",\n",
    "    \"checkpoints/core_agent_ipo/final\",\n",
    "    \"checkpoints/core_agent/final\",\n",
    "    \"checkpoints/gpt-oss-20b-rust-merged\",\n",
    "]\n",
    "\n",
    "MODEL_PATH = None\n",
    "for path in CHECKPOINT_PRIORITY:\n",
    "    if os.path.exists(path):\n",
    "        MODEL_PATH = path\n",
    "        break\n",
    "\n",
    "if MODEL_PATH is None:\n",
    "    print(\"\\u2717 No checkpoint found. Train the model first.\")\n",
    "else:\n",
    "    print(f\"Loading model from: {MODEL_PATH}\")\n",
    "\n",
    "    # v2: Use FP8 on H100, 4-bit otherwise\n",
    "    load_kwargs = {\n",
    "        \"max_seq_length\": 4096,\n",
    "        \"dtype\": torch.bfloat16,\n",
    "    }\n",
    "    if CONFIG.get(\"load_mode\") == \"fp8\" and CONFIG.get(\"use_fp8\"):\n",
    "        load_kwargs[\"load_in_fp8\"] = True\n",
    "        print(\"  Mode: FP8 (H100)\")\n",
    "    else:\n",
    "        load_kwargs[\"load_in_4bit\"] = True\n",
    "        print(\"  Mode: 4-bit QLoRA\")\n",
    "\n",
    "    if CONFIG.get(\"fast_inference\"):\n",
    "        load_kwargs[\"fast_inference\"] = True\n",
    "        print(\"  Inference: vLLM backend\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(MODEL_PATH, **load_kwargs)\n",
    "    FastLanguageModel.for_inference(model)\n",
    "\n",
    "    print(\"\\u2713 Model loaded!\")"
   ],
   "id": "58e85ca44306ad95",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Generate Rust Code\n",
    "\n",
    "Tests the model on 3 pre-defined Rust prompts using Harmony format.\n"
   ],
   "id": "2a8c82c63929e765"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"scripts\")\n",
    "from dataset_formatters.harmony import encode_harmony_messages\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    \"Write a Rust function `fn merge_sorted(a: &[i32], b: &[i32]) -> Vec<i32>` that merges two sorted slices into a single sorted vector.\",\n",
    "    \"This Rust code fails the borrow checker. Fix it:\\n```rust\\nfn main() {\\n    let mut v = vec![1, 2, 3];\\n    let first = &v[0];\\n    v.push(4);\\n    println!(\\\"{}\\\", first);\\n}\\n```\",\n",
    "    \"Write an async Rust function using tokio that fetches a URL with reqwest, retries up to 3 times on failure, and returns the response body as a String.\",\n",
    "]\n",
    "\n",
    "def generate_rust(prompt, max_tokens=1024):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted = encode_harmony_messages(\n",
    "        messages,\n",
    "        developer_instructions=\"You are a Rust programming expert. Write correct, idiomatic code.\",\n",
    "    )\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.3,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "for i, prompt in enumerate(TEST_PROMPTS, 1):\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Test {i}: {prompt[:80]}...\")\n",
    "    print(\"=\" * 60)\n",
    "    response = generate_rust(prompt)\n",
    "    print(response)\n",
    "    print()"
   ],
   "id": "fe79b1c5cd8fa9af",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Custom Prompt\n"
   ],
   "id": "f26961ac0a874651"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "CUSTOM_PROMPT = \"Write a Rust function that reads a CSV file and returns the sum of a specified column.\"\n",
    "\n",
    "print(f\"Prompt: {CUSTOM_PROMPT}\")\n",
    "print(\"=\" * 60)\n",
    "print(generate_rust(CUSTOM_PROMPT))"
   ],
   "id": "4b4b7f32818a3d05",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Export\n",
    "\n",
    "Merge the final adapter and export to HuggingFace + GGUF formats.\n",
    "\n",
    "**v2:** Optional QAT export for 97-100% MXFP4 quality retention (vs 59-89% with PTQ)."
   ],
   "id": "2cf19b6ed97d019e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Export to GGUF\n",
    "\n",
    "Merges the best adapter and exports as HF safetensors + GGUF Q4.\n"
   ],
   "id": "91788a1701ee9837"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ADAPTER_PRIORITY = [\n",
    "    \"checkpoints/core_agent_grpo/final\",\n",
    "    \"checkpoints/core_agent_ipo/final\",\n",
    "    \"checkpoints/core_agent/final\",\n",
    "    \"checkpoints/lang_rust/final\",\n",
    "]\n",
    "\n",
    "adapter_path = None\n",
    "for path in ADAPTER_PRIORITY:\n",
    "    if os.path.exists(path):\n",
    "        adapter_path = path\n",
    "        break\n",
    "\n",
    "if adapter_path is None:\n",
    "    print(\"✗ No adapter checkpoint found.\")\n",
    "else:\n",
    "    export_dir = \"checkpoints/gpt-oss-20b-rust-export-v2\"\n",
    "    print(f\"Exporting adapter: {adapter_path}\")\n",
    "    print(f\"Output: {export_dir}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/19_merge_adapter.py \\\n",
    "        --adapter_path {adapter_path} \\\n",
    "        --output_dir {export_dir} \\\n",
    "        --export_formats hf gguf_q4\n",
    "\n",
    "    drive_helper.backup(export_dir, \"checkpoints/gpt-oss-20b-rust-export-v2\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nExport backed up to Drive.\")\n"
   ],
   "id": "d1b8a168f48fae53",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 QAT Export (Optional)\n",
    "\n",
    "v2: Quantisation-Aware Training for MXFP4 deployment.\n",
    "Recovers 97-100% quality vs 59-89% with post-training quantisation.\n",
    "Requires: `pip install nvidia-modelopt`\n"
   ],
   "id": "c632af3d6ca89abd"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if not CONFIG.get(\"enable_qat_export\"):\n",
    "    print(\"QAT export disabled. Enable via enable_qat_export=True in Step 0.3.\")\n",
    "    print(\"\\nQAT recovers 97-100% quality when deploying to MXFP4,\")\n",
    "    print(\"vs 59-89% with standard post-training quantisation (PTQ).\")\n",
    "else:\n",
    "    export_dir = \"checkpoints/gpt-oss-20b-rust-export-v2\"\n",
    "    qat_dir = \"checkpoints/gpt-oss-20b-rust-qat\"\n",
    "\n",
    "    if not os.path.exists(export_dir):\n",
    "        print(\"\\u2717 Run standard export (8.1) first.\")\n",
    "    else:\n",
    "        print(\"Running QAT pass on merged model...\")\n",
    "        print(\"  This fine-tunes with MXFP4-aware quantisation at reduced LR (1e-5).\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        try:\n",
    "            import modelopt.torch.quantization as mtq\n",
    "            print(\"\\u2713 nvidia-modelopt available\")\n",
    "\n",
    "            # QAT would be run here via mtq.quantize()\n",
    "            # For now, document the expected command:\n",
    "            print(\"\\nQAT pipeline (manual steps):\")\n",
    "            print(f\"  1. Load merged BF16 model from {export_dir}\")\n",
    "            print(f\"  2. mtq.quantize(model, config=mtq.MXFP4_DEFAULT_CFG)\")\n",
    "            print(f\"  3. Fine-tune for ~100 steps at LR 1e-5\")\n",
    "            print(f\"  4. Export to {qat_dir}\")\n",
    "        except ImportError:\n",
    "            print(\"\\u2717 nvidia-modelopt not installed.\")\n",
    "            print(\"  Install: pip install nvidia-modelopt\")\n",
    "            print(\"  See: https://developer.nvidia.com/blog/fine-tuning-gpt-oss-for-accuracy-and-performance-with-quantization-aware-training/\")"
   ],
   "id": "684742559682938a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Download GGUF\n"
   ],
   "id": "88ce6eb89550c5d9"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    import glob\n",
    "\n",
    "    export_dir = \"checkpoints/gpt-oss-20b-rust-export-v2\"\n",
    "    gguf_files = glob.glob(os.path.join(export_dir, \"*.gguf\"))\n",
    "\n",
    "    if gguf_files:\n",
    "        gguf_path = gguf_files[0]\n",
    "        size_gb = os.path.getsize(gguf_path) / (1024**3)\n",
    "        print(f\"Downloading: {os.path.basename(gguf_path)} ({size_gb:.1f} GB)\")\n",
    "        files.download(gguf_path)\n",
    "    else:\n",
    "        print(\"\\u2717 No GGUF file found. Run export (8.1) first.\")\n",
    "else:\n",
    "    print(\"Download not available outside Colab.\")\n",
    "    print(\"GGUF file is at: checkpoints/gpt-oss-20b-rust-export-v2/\")"
   ],
   "id": "1240d8a590699baf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Complete!\n",
    "\n",
    "Your GPT-OSS 20B Rust coding agent (v2) is trained and ready to use.\n",
    "\n",
    "**v2 Optimisations Applied:**\n",
    "- Split LoRA: 7-12x faster MoE training\n",
    "- FP8 RL: 1.6x throughput on H100 (60% less VRAM)\n",
    "- Auto packing: 3x faster SFT\n",
    "- Chunked GRPO: 65K context on H100 (up from 32K)\n",
    "- QAT export: 97-100% MXFP4 quality (if enabled)\n",
    "\n",
    "**Outputs:**\n",
    "- Checkpoints: `checkpoints/core_agent_{ipo,grpo}/final`\n",
    "- Evaluation: `evals/rust_agent/metrics.json`\n",
    "- Exported model: `checkpoints/gpt-oss-20b-rust-export-v2/`\n",
    "- All backed up to Google Drive: `gpt-oss-20b-rust-agent-v2/`\n",
    "\n",
    "**Next steps:**\n",
    "- Review evaluation metrics in Step 6.3\n",
    "- Test interactively in Step 7\n",
    "- Deploy the GGUF file with llama.cpp or Ollama\n",
    "- For MXFP4 deployment, enable QAT export in Step 8.2\n",
    "\n",
    "**References:**\n",
    "- [V2 Optimization Plan](../docs/V2_OPTIMIZATION_PLAN.md)\n",
    "- [Unsloth Split LoRA](https://unsloth.ai/docs/new/faster-moe)\n",
    "- [Unsloth FP8 RL](https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/fp8-reinforcement-learning)\n",
    "- [NVIDIA QAT for GPT-OSS](https://developer.nvidia.com/blog/fine-tuning-gpt-oss-for-accuracy-and-performance-with-quantization-aware-training/)"
   ],
   "id": "f2381c249430c4d6"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
