{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GPT-OSS 20B → Rust Coding Agent (v2)\n",
    "\n",
    "End-to-end pipeline for training a Rust coding agent on OpenAI's GPT-OSS 20B (MoE, ~3.6B active params).\n",
    "\n",
    "**v2 Optimisations** (see `docs/V2_OPTIMIZATION_PLAN.md`):\n",
    "- **Split LoRA** — 7-12x faster MoE training via reordered LoRA computation\n",
    "- **FP8 RL** — 1.6x throughput, 60% less VRAM on H100 (auto-fallback to 4-bit on A100)\n",
    "- **GRPO long context** — Chunked batching enables 65K+ context (up from 32K)\n",
    "- **Flex Attention** — 8x longer sequences with attention sinks\n",
    "- **Auto packing** — 3x faster SFT with uncontaminated packing (zero-config)\n",
    "- **Expert monitoring** — Routing utilisation tracking across all phases\n",
    "- **QAT export** — 97-100% MXFP4 quality retention (vs 59-89% with PTQ)\n",
    "\n",
    "**4-Phase Pipeline:**\n",
    "1. **Lang Adapter** — Rust domain specialisation via QLoRA (script 13 + 19)\n",
    "2. **Core Agent SFT** — Agent trajectory training with tool use (script 14)\n",
    "3. **IPO Preference** — Identity Preference Optimisation on ranked pairs (script 17)\n",
    "4. **GRPO RL** — Group Relative Policy Optimisation with execution rewards (script 18)\n",
    "\n",
    "**Requirements:**\n",
    "- **GPU**: A100 40GB+ (H100 80GB recommended for FP8 + extended context)\n",
    "- **Storage**: Google Drive for persistent checkpoints\n",
    "- **Rust toolchain**: Installed automatically (rustup + cargo-mutants)"
   ],
   "id": "4426faf4d8d7ae35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Environment Setup"
   ],
   "id": "c68353567587459c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Mount Google Drive & Clone Repository\n",
    "\n",
    "**PyCharm / headless users:** If `drive.mount()` doesn't work (e.g. PyCharm Colab\n",
    "plugin can't relay the OAuth popup), set `use_service_account = True` and provide\n",
    "your service-account JSON key path in Step 0.3.\n"
   ],
   "id": "97b1ca5318fc7ab0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T00:36:09.491049Z",
     "start_time": "2026-02-13T00:36:09.050554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "use_service_account = True\n",
    "\n",
    "DRIVE_MOUNTED = False\n",
    "\n",
    "if IN_COLAB and not use_service_account:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        DRIVE_MOUNTED = True\n",
    "        print(\"Google Drive mounted\")\n",
    "    except Exception as e:\n",
    "        print(f\"drive.mount() failed: {e}\")\n",
    "        print(\"Falling back to local-only mode.\")\n",
    "        print(\"Tip: set use_service_account=True and provide a JSON key in Step 0.3.\")\n",
    "elif IN_COLAB and use_service_account:\n",
    "    print(\"Service-account mode selected — skipping drive.mount()\")\n",
    "    print(\"Configure credentials in Step 0.3.\")\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "\n",
    "REPO_URL = \"https://github.com/rmarnold/llm-training-pipeline.git\"\n",
    "BRANCH = \"main\"\n",
    "\n",
    "REPO_DIR = \"/content/llm-training-pipeline\"\n",
    "\n",
    "if IN_COLAB:\n",
    "    if os.path.exists(REPO_DIR):\n",
    "        %cd {REPO_DIR}\n",
    "        !git pull origin {BRANCH}\n",
    "    else:\n",
    "        !git clone -b {BRANCH} {REPO_URL} {REPO_DIR}\n",
    "        %cd {REPO_DIR}\n",
    "\n",
    "    PROJECT_ROOT = REPO_DIR\n",
    "else:\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"\\nProject root: {PROJECT_ROOT}\")\n"
   ],
   "id": "604e923187b87e5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service-account mode selected — skipping drive.mount()\n",
      "Configure credentials in Step 0.3.\n",
      "/content/llm-training-pipeline\n",
      "remote: Enumerating objects: 7, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects: 100% (4/4), 1.88 KiB | 960.00 KiB/s, done.\n",
      "From https://github.com/rmarnold/llm-training-pipeline\n",
      " * branch            main       -> FETCH_HEAD\n",
      "   4865cf7..0dd1b77  main       -> origin/main\n",
      "Updating 4865cf7..0dd1b77\n",
      "Fast-forward\n",
      " notebooks/train_gpt_oss_rust_agent_v2.ipynb | 334 \u001b[32m+++++++\u001b[m\u001b[31m---------------------\u001b[m\n",
      " 1 file changed, 77 insertions(+), 257 deletions(-)\n",
      "\n",
      "Project root: /content/llm-training-pipeline\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Install Dependencies\n",
    "\n",
    "Installs pipeline deps, latest Unsloth (with Split LoRA + FP8 RL), and the Rust toolchain.\n",
    "\n",
    "**Note:** flash-attn is intentionally NOT installed. FA3 is incompatible with GPT-OSS\n",
    "backward passes (incorrect training loss). Unsloth's Flex Attention replaces it\n",
    "automatically — no compilation step needed.\n"
   ],
   "id": "4f8d5ba0fcc56075"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T00:41:24.078334Z",
     "start_time": "2026-02-13T00:36:09.511340Z"
    }
   },
   "source": [
    "if IN_COLAB:\n",
    "    print(\"Installing Python dependencies...\")\n",
    "    print(\"=\" * 60)\n",
    "    !pip install -q -e \".[gpt_oss,rust_eval,colab]\"\n",
    "\n",
    "    # Fix pyarrow binary incompatibility with datasets 4.x on Colab\n",
    "    # (Colab's pre-installed pyarrow C extension doesn't match the new header)\n",
    "    !pip install -q --force-reinstall pyarrow\n",
    "\n",
    "    # v2: Force latest Unsloth with Split LoRA + FP8 RL + GRPO long context\n",
    "    # Flex Attention (bundled with Unsloth) replaces Flash Attention for GPT-OSS\n",
    "    print(\"\\nInstalling latest Unsloth (Split LoRA + FP8 RL + Flex Attention)...\")\n",
    "    !pip install -q --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo\n",
    "    !pip install -q \"unsloth[colab-new]\"\n",
    "\n",
    "    # v2: vLLM for FP8 RL inference (H100 only, optional)\n",
    "    !pip install -q vllm>=0.12.0 2>/dev/null || true\n",
    "\n",
    "    print(\"\\nInstalling Rust toolchain...\")\n",
    "    print(\"=\" * 60)\n",
    "    !curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
    "    os.environ[\"PATH\"] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "    !cargo install cargo-mutants\n",
    "\n",
    "    # Verification — use importlib.metadata to check versions without importing\n",
    "    # (importing unsloth triggers heavy CUDA init that can hang in a notebook cell)\n",
    "    from importlib.metadata import version, PackageNotFoundError\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Dependency Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for pkg in [\"unsloth\", \"trl\", \"peft\", \"datasets\", \"tiktoken\", \"vllm\"]:\n",
    "        try:\n",
    "            ver = version(pkg)\n",
    "            print(f\"\\u2713 {pkg}: {ver}\")\n",
    "        except PackageNotFoundError:\n",
    "            if pkg == \"vllm\":\n",
    "                print(f\"\\u2014 {pkg}: not installed (optional, H100 FP8 RL only)\")\n",
    "            else:\n",
    "                print(f\"\\u2717 {pkg}: not installed\")\n",
    "\n",
    "    import subprocess\n",
    "    for cmd, label in [(\"cargo --version\", \"cargo\"), (\"cargo mutants --version\", \"cargo-mutants\")]:\n",
    "        result = subprocess.run(cmd.split(), capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"\\u2713 {label}: {result.stdout.strip()}\")\n",
    "        else:\n",
    "            print(f\"\\u2717 {label}: not found\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"Running locally \\u2014 ensure deps are installed:\")\n",
    "    print(\"  pip install -e '.[gpt_oss,rust_eval]'\")\n",
    "    print(\"  pip install --upgrade unsloth unsloth_zoo\")\n"
   ],
   "id": "fb54d828abbdfd4b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing Python dependencies...\n",
      "============================================================\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.3/432.3 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m150.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.5/376.5 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m915.7/915.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.9/837.9 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m121.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building editable for llm-training-pipeline (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for pyfastcopy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cuda-python 12.9.5 requires cuda-bindings~=12.9.5, but you have cuda-bindings 12.9.4 which is incompatible.\n",
      "torchaudio 2.9.0+cu128 requires torch==2.9.0, but you have torch 2.10.0 which is incompatible.\n",
      "fastai 2.8.6 requires torch<2.10,>=1.10, but you have torch 2.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "Installing latest Unsloth (Split LoRA + FP8 RL + Flex Attention)...\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.3/432.3 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.5/376.5 kB\u001b[0m \u001b[31m210.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: unsloth 2026.2.1 does not provide the extra 'triton'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "Installing Rust toolchain...\n",
      "============================================================\n",
      "\u001b[1minfo:\u001b[0m downloading installer\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mprofile set to 'default'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdefault host triple is x86_64-unknown-linux-gnu\n",
      "\u001b[0m\u001b[1minfo: \u001b[0msyncing channel updates for 'stable-x86_64-unknown-linux-gnu'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mlatest update on 2026-02-12, rust version 1.93.1 (01f6ddf75 2026-02-11)\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'cargo'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'clippy'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rust-docs'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rust-std'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rustc'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rustfmt'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'cargo'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'clippy'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rust-docs'\n",
      " 20.7 MiB /  20.7 MiB (100 %)   8.3 MiB/s in  2s         \n",
      "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rust-std'\n",
      " 28.1 MiB /  28.1 MiB (100 %)  10.2 MiB/s in  3s         \n",
      "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rustc'\n",
      " 74.4 MiB /  74.4 MiB (100 %)  11.1 MiB/s in  6s         \n",
      "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rustfmt'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdefault toolchain set to 'stable-x86_64-unknown-linux-gnu'\n",
      "\n",
      "  \u001b[0m\u001b[1m\u001b[0m\u001b[1m\u001b[32mstable-x86_64-unknown-linux-gnu installed\u001b[0m - rustc 1.93.1 (01f6ddf75 2026-02-11)\n",
      "\n",
      "\u001b[0m\u001b[1m\n",
      "Rust is installed now. Great!\n",
      "\u001b[0m\n",
      "To get started you may need to restart your current shell.\n",
      "This would reload your \u001b[0m\u001b[1mPATH\u001b[0m environment variable to include\n",
      "Cargo's bin directory ($HOME/.cargo/bin).\n",
      "\n",
      "To configure your current shell, you need to source\n",
      "the corresponding \u001b[0m\u001b[1menv\u001b[0m file under $HOME/.cargo.\n",
      "\n",
      "This is usually done by running one of the following (note the leading DOT):\n",
      ". \"$HOME/.cargo/env\"            # For sh/bash/zsh/ash/dash/pdksh\n",
      "source \"$HOME/.cargo/env.fish\"  # For fish\n",
      "source $\"($nu.home-path)/.cargo/env.nu\"  # For nushell\n",
      "\u001b[1m\u001b[92m    Updating\u001b[0m crates.io index\n",
      "\u001b[1m\u001b[92m Downloading\u001b[0m crates ...\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m cargo-mutants v26.2.0\n",
      "\u001b[1m\u001b[92m  Installing\u001b[0m cargo-mutants v26.2.0\n",
      "\u001b[1m\u001b[92m    Updating\u001b[0m crates.io index\n",
      "\u001b[1m\u001b[92m     Locking\u001b[0m 205 packages to latest compatible versions\n",
      "\u001b[1m\u001b[92m      Adding\u001b[0m cargo_metadata v0.19.2 \u001b[1m\u001b[33m(available: v0.23.1)\u001b[0m\n",
      "\u001b[1m\u001b[92m      Adding\u001b[0m console v0.15.11 \u001b[1m\u001b[33m(available: v0.16.2)\u001b[0m\n",
      "\u001b[1m\u001b[92m      Adding\u001b[0m ctrlc v3.5.1 \u001b[1m\u001b[33m(available: v3.5.2)\u001b[0m\n",
      "\u001b[1m\u001b[92m      Adding\u001b[0m itertools v0.13.0 \u001b[1m\u001b[33m(available: v0.14.0)\u001b[0m\n",
      "\u001b[1m\u001b[92m      Adding\u001b[0m jiff v0.1.29 \u001b[1m\u001b[33m(available: v0.2.20)\u001b[0m\n",
      "\u001b[1m\u001b[92m      Adding\u001b[0m mutants v0.0.3 \u001b[1m\u001b[33m(available: v0.0.4)\u001b[0m\n",
      "\u001b[1m\u001b[92m      Adding\u001b[0m nextest-metadata v0.12.3 \u001b[1m\u001b[33m(available: v0.13.3)\u001b[0m\n",
      "\u001b[1m\u001b[92m      Adding\u001b[0m nix v0.30.1 \u001b[1m\u001b[33m(available: v0.31.1)\u001b[0m\n",
      "\u001b[1m\u001b[92m      Adding\u001b[0m schemars v0.9.0 \u001b[1m\u001b[33m(available: v1.2.1)\u001b[0m\n",
      "\u001b[1m\u001b[92m      Adding\u001b[0m strum v0.26.3 \u001b[1m\u001b[33m(available: v0.27.2)\u001b[0m\n",
      "\u001b[1m\u001b[92m      Adding\u001b[0m target-lexicon v0.13.3 \u001b[1m\u001b[33m(available: v0.13.4)\u001b[0m\n",
      "\u001b[1m\u001b[92m      Adding\u001b[0m toml v0.8.23 \u001b[1m\u001b[33m(available: v1.0.1+spec-1.1.0)\u001b[0m\n",
      "\u001b[1m\u001b[92m      Adding\u001b[0m whoami v1.6.1 \u001b[1m\u001b[33m(available: v2.1.1)\u001b[0m\n",
      "\u001b[1m\u001b[92m Downloading\u001b[0m crates ...\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m anstyle-query v1.1.5\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m atty v0.2.14\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m color-print v0.3.7\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m anstyle-parse v0.2.7\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m anstyle v1.0.13\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m bitflags v1.3.2\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m semver v1.0.27\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m similar v2.7.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m nextest-metadata v0.12.3\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m flickzeug v0.4.5\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m aho-corasick v1.1.4\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m nextest-workspace-hack v0.1.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m bitflags v2.10.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m serde_spanned v0.6.9\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m mutants v0.0.3\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m globset v0.4.18\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m anyhow v1.0.101\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m anstream v0.6.21\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m unicode-ident v1.0.23\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m terminal_size v0.2.6\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m terminal_size v0.4.3\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m toml_write v0.1.2\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m test-log-macros v0.2.19\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m toml_datetime v0.6.11\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m utf8parse v0.2.2\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m test-log v0.2.19\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m whoami v1.6.1\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m zmij v1.0.21\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m yansi v0.5.1\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m toml v0.8.23\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m walkdir v2.5.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m tracing-log v0.2.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m tracing-core v0.1.36\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m toml_edit v0.22.27\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m itoa v1.0.17\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m winnow v0.7.14\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m same-file v1.0.6\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m strum v0.26.3\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m ref-cast v1.0.25\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m matchers v0.2.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m tracing-subscriber v0.3.22\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m is_terminal_polyfill v1.70.2\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m clap_lex v1.0.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m syn v2.0.115\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m smol_str v0.3.5\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m unicode-width v0.2.2\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m sharded-slab v0.1.7\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m nom v7.1.3\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m getrandom v0.4.1\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m serde_json v1.0.149\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m clap_builder v4.5.58\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m regex-syntax v0.8.9\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m rustix v1.1.3\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m serde v1.0.228\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m nu-ansi-term v0.50.3\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m serde_core v1.0.228\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m libc v0.2.181\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m tracing v0.1.44\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m strsim v0.11.1\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m rustversion v1.0.22\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m jiff v0.1.29\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m clap_complete v4.5.66\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m env_logger v0.11.9\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m ref-cast-impl v1.0.25\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m indoc v2.0.7\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m cfg_aliases v0.2.1\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m cfg-expr v0.20.6\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m itertools v0.13.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m crossbeam-epoch v0.9.18\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m reflink v0.1.3\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m linux-raw-sys v0.11.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m hashbrown v0.16.1\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m indexmap v2.13.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m nutmeg v0.1.5\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m bstr v1.12.1\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m regex v1.12.3\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m num_cpus v1.17.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m regex-automata v0.4.14\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m memchr v2.8.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m borsh v1.6.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m nix v0.30.1\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m console v0.15.11\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m minimal-lexical v0.2.1\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m fs2 v0.4.3\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m clap v4.5.58\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m serde_derive v1.0.228\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m schemars v0.9.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m proc-macro2 v1.0.106\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m log v0.4.29\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m lazy_static v1.5.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m ignore v0.4.25\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m io-lifetimes v1.0.11\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m crossbeam-utils v0.8.21\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m clap_derive v4.5.55\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m cargo_metadata v0.19.2\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m camino v1.2.2\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m smallvec v1.15.1\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m rustix v0.37.28\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m schemars_derive v0.9.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m quote v1.0.44\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m pin-project-lite v0.2.16\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m once_cell v1.21.3\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m jobserver v0.1.34\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m fastrand v2.3.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m either v1.15.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m color-print-proc-macro v0.3.7\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m strum_macros v0.26.4\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m serde_derive_internals v0.29.1\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m crossbeam-deque v0.8.6\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m cargo-platform v0.1.9\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m env_filter v1.0.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m dyn-clone v1.0.20\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m ctrlc v3.5.1\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m path-slash v0.2.1\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m heck v0.5.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m colorchoice v1.0.4\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m errno v0.3.14\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m equivalent v1.0.2\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m guppy-workspace-hack v0.1.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m cfg-if v1.0.4\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m tracing-attributes v0.1.31\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m thread_local v1.1.9\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m tempfile v3.25.0\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m target-spec v3.5.7\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m target-lexicon v0.13.3\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m thiserror-impl v2.0.18\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m thiserror v2.0.18\n",
      "\u001b[1m\u001b[92m  Downloaded\u001b[0m linux-raw-sys v0.3.8\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m quote v1.0.44\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m proc-macro2 v1.0.106\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m unicode-ident v1.0.23\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m serde_core v1.0.228\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m libc v0.2.181\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m memchr v2.8.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m serde v1.0.228\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m bitflags v2.10.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m log v0.4.29\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m regex-syntax v0.8.9\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m once_cell v1.21.3\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m rustix v1.1.3\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m utf8parse v0.2.2\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m cfg-if v1.0.4\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m zmij v1.0.21\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m anstyle-parse v0.2.7\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m colorchoice v1.0.4\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m linux-raw-sys v0.11.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m anstyle v1.0.13\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m serde_json v1.0.149\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m is_terminal_polyfill v1.70.2\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m anstyle-query v1.1.5\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m itoa v1.0.17\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m heck v0.5.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m crossbeam-utils v0.8.21\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m anstream v0.6.21\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m io-lifetimes v1.0.11\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m target-lexicon v0.13.3\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m smallvec v1.15.1\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m tracing-core v0.1.36\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m aho-corasick v1.1.4\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m syn v2.0.115\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m guppy-workspace-hack v0.1.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m strsim v0.11.1\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m camino v1.2.2\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m cfg_aliases v0.2.1\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m rustix v0.37.28\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m thiserror v2.0.18\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m rustversion v1.0.22\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m nix v0.30.1\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m getrandom v0.4.1\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m hashbrown v0.16.1\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m minimal-lexical v0.2.1\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m nu-ansi-term v0.50.3\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m pin-project-lite v0.2.16\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m equivalent v1.0.2\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m nextest-workspace-hack v0.1.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m target-spec v3.5.7\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m bitflags v1.3.2\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m linux-raw-sys v0.3.8\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m clap_lex v1.0.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m ref-cast v1.0.25\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m lazy_static v1.5.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m sharded-slab v0.1.7\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m regex-automata v0.4.14\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m indexmap v2.13.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m crossbeam-epoch v0.9.18\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m nom v7.1.3\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m cfg-expr v0.20.6\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m terminal_size v0.4.3\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m clap_builder v4.5.58\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m tracing-log v0.2.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m bstr v1.12.1\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m serde_derive_internals v0.29.1\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m env_filter v1.0.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m thread_local v1.1.9\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m anyhow v1.0.101\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m toml_write v0.1.2\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m same-file v1.0.6\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m winnow v0.7.14\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m env_logger v0.11.9\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m walkdir v2.5.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m terminal_size v0.2.6\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m semver v1.0.27\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m matchers v0.2.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m globset v0.4.18\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m smol_str v0.3.5\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m serde_derive v1.0.228\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m thiserror-impl v2.0.18\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m tracing-attributes v0.1.31\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m ref-cast-impl v1.0.25\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m clap_derive v4.5.55\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m color-print-proc-macro v0.3.7\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m schemars_derive v0.9.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m test-log-macros v0.2.19\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m strum_macros v0.26.4\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m tracing v0.1.44\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m crossbeam-deque v0.8.6\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m tracing-subscriber v0.3.22\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m atty v0.2.14\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m fastrand v2.3.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m yansi v0.5.1\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m unicode-width v0.2.2\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m dyn-clone v1.0.20\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m either v1.15.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m tempfile v3.25.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m nutmeg v0.1.5\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m itertools v0.13.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m console v0.15.11\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m clap v4.5.58\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m ignore v0.4.25\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m clap_complete v4.5.66\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m color-print v0.3.7\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m test-log v0.2.19\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m flickzeug v0.4.5\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m ctrlc v3.5.1\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m strum v0.26.3\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m regex v1.12.3\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m reflink v0.1.3\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m serde_spanned v0.6.9\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m toml_datetime v0.6.11\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m cargo-platform v0.1.9\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m toml_edit v0.22.27\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m cargo_metadata v0.19.2\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m nextest-metadata v0.12.3\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m jiff v0.1.29\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m toml v0.8.23\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m schemars v0.9.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m fs2 v0.4.3\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m num_cpus v1.17.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m jobserver v0.1.34\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m whoami v1.6.1\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m path-slash v0.2.1\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m indoc v2.0.7\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m mutants v0.0.3\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m similar v2.7.0\n",
      "\u001b[1m\u001b[92m   Compiling\u001b[0m cargo-mutants v26.2.0\n",
      "\u001b[1m\u001b[92m    Finished\u001b[0m `release` profile [optimized] target(s) in 40.09s\n",
      "\u001b[1m\u001b[92m  Installing\u001b[0m /root/.cargo/bin/cargo-mutants\n",
      "\u001b[1m\u001b[92m   Installed\u001b[0m package `cargo-mutants v26.2.0` (executable `cargo-mutants`)\n",
      "\n",
      "============================================================\n",
      "Dependency Verification:\n",
      "============================================================\n",
      "✓ unsloth: 2026.2.1\n",
      "✓ trl: 0.24.0\n",
      "✓ peft: 0.18.1\n",
      "✓ datasets: 4.3.0\n",
      "✓ tiktoken: 0.12.0\n",
      "✓ vllm: 0.15.1\n",
      "✓ cargo: cargo 1.93.1 (083ac5135 2025-12-15)\n",
      "✓ cargo-mutants: cargo-mutants 26.2.0\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Configure Pipeline\n",
    "\n",
    "Edit the variables below to configure the training run.\n",
    "\n",
    "**Training Scope** (`training_scope`):\n",
    "- `full` — All 4 phases end-to-end\n",
    "- `quick_test` — Short runs (100 steps each) to verify setup\n",
    "- `lang_adapter_only` — Only train lang_rust adapter + merge\n",
    "- `skip_to_rl` — Start from existing core_agent checkpoint (IPO + GRPO only)\n",
    "\n",
    "**Other settings:**\n",
    "- `gpu_tier` — Auto-detected below; override if needed\n",
    "- `max_steps_override` — Set >0 to cap all training stages (0 = use defaults)\n",
    "- `skip_data_generation` — Use pre-generated data from Drive\n",
    "- `include_grpo` — GRPO RL is slow; set `False` to skip\n",
    "- `enable_qat_export` — v2: QAT for MXFP4 export (97-100% quality vs 59-89% PTQ)\n",
    "\n",
    "**Service Account Setup** (for Drive backup):\n",
    "1. In Colab, click the **key icon** (Secrets) in the left sidebar\n",
    "2. Add a secret named `SERVICE_ACCOUNT_KEY` — paste the full JSON content of your key file\n",
    "3. Add a secret named `DRIVE_FOLDER_ID` — paste your Drive folder ID\n",
    "4. Toggle \"Notebook access\" ON for both\n",
    "5. Set `use_service_account = True` below\n",
    "\n",
    "The secrets persist across sessions — no file uploads needed."
   ],
   "id": "3d85f6b5a5ecf6cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-02-13T00:41:24.181513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_scope = \"quick_test\"  # \"full\", \"quick_test\", \"lang_adapter_only\", \"skip_to_rl\"\n",
    "\n",
    "gpu_tier = \"a100_80gb\"  # \"a100_40gb\", \"a100_80gb\", \"h100_80gb\"\n",
    "\n",
    "max_steps_override = 0  # Set >0 to cap all stages (0 = use defaults)\n",
    "\n",
    "skip_data_generation = False  # True to use pre-generated data from Drive\n",
    "\n",
    "include_grpo = True  # False to skip GRPO RL (slow)\n",
    "\n",
    "enable_qat_export = False  # True for MXFP4 QAT export\n",
    "\n",
    "# ============================================================\n",
    "# SERVICE ACCOUNT CREDENTIALS\n",
    "# ============================================================\n",
    "# Priority order:\n",
    "#   1. Colab Secrets (SERVICE_ACCOUNT_KEY + DRIVE_FOLDER_ID)\n",
    "#   2. File at /content/service_account.json + drive_folder_id below\n",
    "#   3. files.upload() prompt\n",
    "#   4. Fall back to local mode (no Drive backup)\n",
    "\n",
    "drive_folder_id = \"\"  # Override: Google Drive folder ID (if not using Colab Secrets)\n",
    "\n",
    "_SA_VM_PATH = \"/content/service_account.json\"\n",
    "service_account_key = \"\"\n",
    "\n",
    "if use_service_account and IN_COLAB:\n",
    "    # Try Colab Secrets first\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        _key_json = userdata.get(\"SERVICE_ACCOUNT_KEY\")\n",
    "        if _key_json:\n",
    "            with open(_SA_VM_PATH, \"w\") as _f:\n",
    "                _f.write(_key_json)\n",
    "            service_account_key = _SA_VM_PATH\n",
    "            print(\"Service account key loaded from Colab Secrets.\")\n",
    "        else:\n",
    "            print(\"  Colab Secret 'SERVICE_ACCOUNT_KEY' is empty or not set.\")\n",
    "        if not drive_folder_id:\n",
    "            drive_folder_id = userdata.get(\"DRIVE_FOLDER_ID\") or \"\"\n",
    "            if drive_folder_id:\n",
    "                print(f\"Drive folder ID loaded from Colab Secrets.\")\n",
    "    except Exception as _e:\n",
    "        print(f\"  Colab Secrets lookup failed: {type(_e).__name__}: {_e}\")\n",
    "\n",
    "    # Fall back to file on VM\n",
    "    if not service_account_key and os.path.exists(_SA_VM_PATH):\n",
    "        service_account_key = _SA_VM_PATH\n",
    "        print(f\"Using key file on VM: {_SA_VM_PATH}\")\n",
    "\n",
    "    # Fall back to files.upload()\n",
    "    if not service_account_key:\n",
    "        try:\n",
    "            from google.colab import files as _files\n",
    "            print(\"No service account key found.\")\n",
    "            print(\"Upload your JSON key file (or press Cancel to skip):\")\n",
    "            _uploaded = _files.upload()\n",
    "            if _uploaded:\n",
    "                _name = list(_uploaded.keys())[0]\n",
    "                with open(_SA_VM_PATH, \"wb\") as _f:\n",
    "                    _f.write(_uploaded[_name])\n",
    "                service_account_key = _SA_VM_PATH\n",
    "                print(f\"Saved to {_SA_VM_PATH}\")\n",
    "        except Exception as _e:\n",
    "            pass\n",
    "\n",
    "    if not service_account_key:\n",
    "        print(\"No service account key — Drive backup disabled.\")\n",
    "        print(\"Tip: Add SERVICE_ACCOUNT_KEY to Colab Secrets (key icon in sidebar).\")\n",
    "\n",
    "elif use_service_account:\n",
    "    # Running locally — look for key file\n",
    "    for _path in [_SA_VM_PATH, \"service_account.json\"]:\n",
    "        if os.path.exists(_path):\n",
    "            service_account_key = _path\n",
    "            print(f\"Using key file: {_path}\")\n",
    "            break\n",
    "    if not service_account_key:\n",
    "        print(\"Running locally — set service_account_key to your JSON key path.\")\n",
    "\n",
    "# ============================================================\n",
    "# DRIVE MODE\n",
    "# ============================================================\n",
    "from scripts.pipeline_lib.drive_utils import DriveHelper\n",
    "\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/gpt-oss-20b-rust-agent-v2\"\n",
    "\n",
    "if DRIVE_MOUNTED:\n",
    "    DRIVE_MODE = \"mounted\"\n",
    "elif use_service_account and service_account_key and drive_folder_id:\n",
    "    DRIVE_MODE = \"service_account\"\n",
    "else:\n",
    "    DRIVE_MODE = \"local\"\n",
    "\n",
    "drive_helper = DriveHelper(\n",
    "    mode=DRIVE_MODE,\n",
    "    drive_base=DRIVE_BASE,\n",
    "    credentials_path=service_account_key or None,\n",
    "    folder_id=drive_folder_id or None,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# v2 GPU TIER CONFIGS (with H100 FP8 tier)\n",
    "# ============================================================\n",
    "\n",
    "GPU_CONFIGS = {\n",
    "    \"a100_40gb\": {\n",
    "        \"moe_backend\": \"unsloth_triton\",\n",
    "        \"load_mode\": \"4bit\",\n",
    "        \"fast_inference\": False,\n",
    "        \"lang_rust\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 8192, \"max_steps\": 3000},\n",
    "        \"core_agent\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 12288, \"max_steps\": 2000},\n",
    "        \"ipo\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 12288, \"max_steps\": 1000},\n",
    "        \"grpo\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 16384, \"max_steps\": 2000, \"num_gen\": 2},\n",
    "    },\n",
    "    \"a100_80gb\": {\n",
    "        \"moe_backend\": \"unsloth_triton\",\n",
    "        \"load_mode\": \"4bit\",\n",
    "        \"fast_inference\": False,\n",
    "        \"lang_rust\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 8192, \"max_steps\": 5000},\n",
    "        \"core_agent\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 16384, \"max_steps\": 3000},\n",
    "        \"ipo\": {\"batch\": 1, \"grad_accum\": 16, \"seq_len\": 16384, \"max_steps\": 2000},\n",
    "        \"grpo\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 32768, \"max_steps\": 5000, \"num_gen\": 4},\n",
    "    },\n",
    "    \"h100_80gb\": {\n",
    "        \"moe_backend\": \"grouped_mm\",\n",
    "        \"load_mode\": \"fp8\",\n",
    "        \"fast_inference\": True,\n",
    "        \"lang_rust\": {\"batch\": 2, \"grad_accum\": 4, \"seq_len\": 8192, \"max_steps\": 5000},\n",
    "        \"core_agent\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 16384, \"max_steps\": 3000},\n",
    "        \"ipo\": {\"batch\": 1, \"grad_accum\": 16, \"seq_len\": 16384, \"max_steps\": 2000},\n",
    "        \"grpo\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 65536, \"max_steps\": 7000, \"num_gen\": 4},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Quick test overrides\n",
    "if training_scope == \"quick_test\":\n",
    "    max_steps_override = 100\n",
    "\n",
    "gpu_cfg = GPU_CONFIGS[gpu_tier]\n",
    "\n",
    "# Detect CPU count and RAM for parallel mutation jobs.\n",
    "# Each cargo-mutants worker spawns cargo build/test subprocesses that can\n",
    "# each use 1-2 GB RAM.  Cap jobs to avoid OOM kills on Colab instances.\n",
    "import multiprocessing\n",
    "import os as _os\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "\n",
    "# RAM-aware job limit: allow ~20 GB per mutation worker for headroom\n",
    "try:\n",
    "    _mem_bytes = _os.sysconf('SC_PAGE_SIZE') * _os.sysconf('SC_PHYS_PAGES')\n",
    "    total_ram_gb = _mem_bytes / (1024**3)\n",
    "    ram_based_jobs = max(1, int(total_ram_gb / 20))\n",
    "except (ValueError, OSError):\n",
    "    total_ram_gb = 0\n",
    "    ram_based_jobs = cpu_count\n",
    "\n",
    "mutation_jobs = min(max(1, cpu_count - 2), ram_based_jobs)\n",
    "\n",
    "# Build CONFIG dict\n",
    "CONFIG = {\n",
    "    \"training_scope\": training_scope,\n",
    "    \"gpu_tier\": gpu_tier,\n",
    "    \"include_grpo\": include_grpo,\n",
    "    \"skip_data_generation\": skip_data_generation,\n",
    "    \"enable_qat_export\": enable_qat_export,\n",
    "    # v2: MoE backend + load mode\n",
    "    \"moe_backend\": gpu_cfg[\"moe_backend\"],\n",
    "    \"load_mode\": gpu_cfg[\"load_mode\"],\n",
    "    \"fast_inference\": gpu_cfg[\"fast_inference\"],\n",
    "    # Lang adapter\n",
    "    \"lang_rust_batch\": gpu_cfg[\"lang_rust\"][\"batch\"],\n",
    "    \"lang_rust_grad_accum\": gpu_cfg[\"lang_rust\"][\"grad_accum\"],\n",
    "    \"lang_rust_seq_len\": gpu_cfg[\"lang_rust\"][\"seq_len\"],\n",
    "    \"lang_rust_max_steps\": max_steps_override or gpu_cfg[\"lang_rust\"][\"max_steps\"],\n",
    "    # Core agent\n",
    "    \"core_agent_batch\": gpu_cfg[\"core_agent\"][\"batch\"],\n",
    "    \"core_agent_grad_accum\": gpu_cfg[\"core_agent\"][\"grad_accum\"],\n",
    "    \"core_agent_seq_len\": gpu_cfg[\"core_agent\"][\"seq_len\"],\n",
    "    \"core_agent_max_steps\": max_steps_override or gpu_cfg[\"core_agent\"][\"max_steps\"],\n",
    "    # IPO\n",
    "    \"ipo_batch\": gpu_cfg[\"ipo\"][\"batch\"],\n",
    "    \"ipo_grad_accum\": gpu_cfg[\"ipo\"][\"grad_accum\"],\n",
    "    \"ipo_seq_len\": gpu_cfg[\"ipo\"][\"seq_len\"],\n",
    "    \"ipo_max_steps\": max_steps_override or gpu_cfg[\"ipo\"][\"max_steps\"],\n",
    "    # GRPO\n",
    "    \"grpo_batch\": gpu_cfg[\"grpo\"][\"batch\"],\n",
    "    \"grpo_grad_accum\": gpu_cfg[\"grpo\"][\"grad_accum\"],\n",
    "    \"grpo_seq_len\": gpu_cfg[\"grpo\"][\"seq_len\"],\n",
    "    \"grpo_max_steps\": max_steps_override or gpu_cfg[\"grpo\"][\"max_steps\"],\n",
    "    \"grpo_num_gen\": gpu_cfg[\"grpo\"][\"num_gen\"],\n",
    "    # Mutation generation — balance CPU parallelism with RAM headroom\n",
    "    \"max_mutations_per_repo\": 50 if training_scope == \"quick_test\" else 100,\n",
    "    \"mutation_jobs\": mutation_jobs,\n",
    "    # Eval\n",
    "    \"eval_num_samples\": 10 if training_scope == \"quick_test\" else 50,\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PIPELINE CONFIGURATION (v2)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nScope: {training_scope.upper()}\")\n",
    "print(f\"GPU tier: {gpu_tier}\")\n",
    "ram_str = f\" | RAM: {total_ram_gb:.0f} GB\" if total_ram_gb else \"\"\n",
    "print(f\"CPUs: {cpu_count}{ram_str} (mutation jobs: {CONFIG['mutation_jobs']})\")\n",
    "print(f\"MoE backend: {CONFIG['moe_backend']}\")\n",
    "print(f\"Load mode: {CONFIG['load_mode']}\")\n",
    "print(f\"Fast inference (vLLM): {CONFIG['fast_inference']}\")\n",
    "print(f\"Include GRPO: {include_grpo}\")\n",
    "print(f\"QAT export: {enable_qat_export}\")\n",
    "print(f\"Skip data gen: {skip_data_generation}\")\n",
    "print(f\"Drive mode: {DRIVE_MODE}\")\n",
    "if max_steps_override:\n",
    "    print(f\"Max steps override: {max_steps_override}\")\n",
    "print(f\"\\nLang Adapter:  batch={CONFIG['lang_rust_batch']} x grad_accum={CONFIG['lang_rust_grad_accum']}, seq={CONFIG['lang_rust_seq_len']}, steps={CONFIG['lang_rust_max_steps']}\")\n",
    "print(f\"Core Agent:    batch={CONFIG['core_agent_batch']} x grad_accum={CONFIG['core_agent_grad_accum']}, seq={CONFIG['core_agent_seq_len']}, steps={CONFIG['core_agent_max_steps']}\")\n",
    "print(f\"IPO:           batch={CONFIG['ipo_batch']} x grad_accum={CONFIG['ipo_grad_accum']}, seq={CONFIG['ipo_seq_len']}, steps={CONFIG['ipo_max_steps']}\")\n",
    "if include_grpo:\n",
    "    print(f\"GRPO:          batch={CONFIG['grpo_batch']} x grad_accum={CONFIG['grpo_grad_accum']}, seq={CONFIG['grpo_seq_len']}, steps={CONFIG['grpo_max_steps']}, gen={CONFIG['grpo_num_gen']}\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "93dd91bddbc4ed84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No service account key found.\n",
      "Upload your JSON key file (or press Cancel to skip):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-802c8f45-e6c4-4e4b-a4fb-545623db1953\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-802c8f45-e6c4-4e4b-a4fb-545623db1953\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Set Up Persistent Storage\n"
   ],
   "id": "aa77eea7bd7df89e"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "DRIVE_SUBDIRS = [\n",
    "    \"checkpoints/lang_rust\",\n",
    "    \"checkpoints/core_agent\",\n",
    "    \"checkpoints/core_agent_ipo\",\n",
    "    \"checkpoints/core_agent_grpo\",\n",
    "    \"checkpoints/gpt-oss-20b-rust-merged\",\n",
    "    \"data/rust/lang_rust\",\n",
    "    \"data/rust/core_agent\",\n",
    "    \"data/rust/mutations\",\n",
    "    \"data/rust/ipo\",\n",
    "    \"data/rust/grpo\",\n",
    "    \"data/rust/eval\",\n",
    "    \"data/rust/repos\",\n",
    "    \"logs\",\n",
    "    \"evals/rust_agent\",\n",
    "]\n",
    "\n",
    "if DRIVE_MODE == \"mounted\":\n",
    "    # Mounted mode: create Drive dirs + symlink local → Drive (original behaviour)\n",
    "    print(f\"Setting up storage at: {DRIVE_BASE}\")\n",
    "    for subdir in DRIVE_SUBDIRS:\n",
    "        os.makedirs(os.path.join(DRIVE_BASE, subdir), exist_ok=True)\n",
    "\n",
    "    for dir_name in [\"checkpoints\", \"data\", \"logs\", \"evals\"]:\n",
    "        local_path = os.path.join(PROJECT_ROOT, dir_name)\n",
    "        drive_path = os.path.join(DRIVE_BASE, dir_name)\n",
    "\n",
    "        if os.path.exists(local_path) and not os.path.islink(local_path):\n",
    "            !cp -r {local_path}/* {drive_path}/ 2>/dev/null || true\n",
    "            !rm -rf {local_path}\n",
    "        elif os.path.islink(local_path):\n",
    "            os.unlink(local_path)\n",
    "\n",
    "        os.symlink(drive_path, local_path)\n",
    "        print(f\"  {dir_name} -> Drive (mounted)\")\n",
    "\n",
    "elif DRIVE_MODE == \"service_account\":\n",
    "    # Service-account mode: create local dirs, restore existing data from Drive\n",
    "    print(\"Setting up local storage + Drive API restore...\")\n",
    "    for subdir in DRIVE_SUBDIRS:\n",
    "        os.makedirs(os.path.join(PROJECT_ROOT, subdir), exist_ok=True)\n",
    "        drive_helper.ensure_dir(subdir)\n",
    "\n",
    "    for dir_name in [\"checkpoints\", \"data\", \"logs\", \"evals\"]:\n",
    "        local_path = os.path.join(PROJECT_ROOT, dir_name)\n",
    "        # Remove stale symlinks from previous mounted runs\n",
    "        if os.path.islink(local_path):\n",
    "            os.unlink(local_path)\n",
    "            os.makedirs(local_path, exist_ok=True)\n",
    "        print(f\"  {dir_name} -> local (backed up via Drive API)\")\n",
    "\n",
    "    print(\"\\nRestoring existing data from Drive...\")\n",
    "    for subdir in DRIVE_SUBDIRS:\n",
    "        local_target = os.path.join(PROJECT_ROOT, subdir)\n",
    "        drive_helper.restore(subdir, local_target)\n",
    "    print(\"Restore complete.\")\n",
    "\n",
    "else:\n",
    "    # Local-only mode — no Drive\n",
    "    for d in [\"checkpoints\", \"data/rust\", \"logs\", \"evals/rust_agent\"]:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "    print(\"Local directories created (no Drive backup).\")\n",
    "\n",
    "print(\"\\nStorage ready!\")\n"
   ],
   "id": "ef3dbe47b760cca0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5 Check GPU & Configure MoE Backend\n",
    "\n",
    "v2: Auto-detects H100 for FP8 RL and sets the optimal Split LoRA backend.\n"
   ],
   "id": "ec0e7828c971b71a"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    capability = torch.cuda.get_device_capability()\n",
    "    is_h100 = \"H100\" in gpu_name or \"H200\" in gpu_name or \"B200\" in gpu_name\n",
    "\n",
    "    CONFIG[\"use_fp8\"] = capability[0] >= 9 and is_h100\n",
    "\n",
    "    # v2: Auto-detect GPU tier (now includes H100)\n",
    "    if is_h100:\n",
    "        detected_tier = \"h100_80gb\"\n",
    "    elif gpu_memory >= 70:\n",
    "        detected_tier = \"a100_80gb\"\n",
    "    else:\n",
    "        detected_tier = \"a100_40gb\"\n",
    "\n",
    "    if detected_tier != CONFIG[\"gpu_tier\"]:\n",
    "        print(f\"NOTE: Auto-detected {detected_tier}, overriding configured {CONFIG['gpu_tier']}\")\n",
    "        CONFIG[\"gpu_tier\"] = detected_tier\n",
    "        # Re-derive tier-specific settings\n",
    "        gpu_cfg = GPU_CONFIGS[detected_tier]\n",
    "        CONFIG[\"moe_backend\"] = gpu_cfg[\"moe_backend\"]\n",
    "        CONFIG[\"load_mode\"] = gpu_cfg[\"load_mode\"]\n",
    "        CONFIG[\"fast_inference\"] = gpu_cfg[\"fast_inference\"]\n",
    "\n",
    "    # v2: Set Split LoRA MoE backend\n",
    "    os.environ[\"UNSLOTH_MOE_BACKEND\"] = CONFIG[\"moe_backend\"]\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"GPU: {gpu_name} ({gpu_memory:.0f} GB)\")\n",
    "    print(f\"Compute capability: {capability[0]}.{capability[1]}\")\n",
    "    print(f\"Tier: {CONFIG['gpu_tier']}\")\n",
    "    print(f\"\\nv2 Optimisations:\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(f\"  Load mode: {CONFIG['load_mode']}\")\n",
    "    print(f\"  FP8 available: {CONFIG['use_fp8']}\")\n",
    "    print(f\"  Fast inference (vLLM): {CONFIG['fast_inference']}\")\n",
    "\n",
    "    if gpu_memory < 40:\n",
    "        print(\"\\nWARNING: <40 GB VRAM. Long-context training (16K+) may OOM.\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"No GPU detected!\")\n",
    "    CONFIG[\"use_fp8\"] = False\n",
    "    os.environ[\"UNSLOTH_MOE_BACKEND\"] = \"native_torch\""
   ],
   "id": "f189d07bd7e89131",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Data Generation\n",
    "\n",
    "Generates mutation data from curated Rust repos and agent trajectories.\n",
    "Skip this step if you have pre-generated data on Drive (`skip_data_generation=True`)."
   ],
   "id": "7f93fc1acaa43c5f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Generate Mutation Data\n",
    "\n",
    "Runs `cargo-mutants` on curated Rust repos to produce bug-fix training pairs.\n"
   ],
   "id": "3db2daedc13bc633"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"skip_data_generation\"]:\n",
    "    print(\"Skipping data generation (using pre-generated data from Drive)\")\n",
    "elif CONFIG[\"training_scope\"] in (\"skip_to_rl\",):\n",
    "    print(\"Skipping — not needed for this training scope\")\n",
    "else:\n",
    "    max_muts = CONFIG[\"max_mutations_per_repo\"]\n",
    "    jobs = CONFIG[\"mutation_jobs\"]\n",
    "\n",
    "    print(f\"Generating mutations (max {max_muts}/repo, {jobs} parallel jobs)...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/16_generate_mutations.py \\\n",
    "        --max_mutations_per_repo {max_muts} \\\n",
    "        --jobs {jobs}\n",
    "\n",
    "    drive_helper.backup(\"data/rust/mutations\", \"data/rust/mutations\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nBacked up mutations to Drive.\")\n"
   ],
   "id": "a06c4e99d8d41ffc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Generate Agent Trajectories\n",
    "\n",
    "Generates multi-turn agent trajectories from mutations + Strandset in Harmony format.\n"
   ],
   "id": "fc8a2b5d9669022f"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"skip_data_generation\"]:\n",
    "    print(\"Skipping data generation (using pre-generated data from Drive)\")\n",
    "elif CONFIG[\"training_scope\"] in (\"skip_to_rl\",):\n",
    "    print(\"Skipping — not needed for this training scope\")\n",
    "else:\n",
    "    max_samples = 500 if CONFIG[\"training_scope\"] == \"quick_test\" else 5000\n",
    "\n",
    "    print(f\"Generating trajectories (max {max_samples} per source)...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    cmd = f\"python scripts/15_generate_trajectories.py --max_samples {max_samples}\"\n",
    "\n",
    "    mutations_path = \"data/rust/mutations/mutations.jsonl\"\n",
    "    if os.path.exists(mutations_path):\n",
    "        cmd += f\" --mutations_path {mutations_path}\"\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"data/rust/core_agent\", \"data/rust/core_agent\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nBacked up trajectories to Drive.\")\n"
   ],
   "id": "4ce7bebf4b7f876c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Verify Data\n"
   ],
   "id": "6de8a13dcfeedf86"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data_checks = [\n",
    "    (\"Mutations\", \"data/rust/mutations\"),\n",
    "    (\"Lang Rust train\", \"data/rust/lang_rust/train\"),\n",
    "    (\"Core Agent train\", \"data/rust/core_agent/train\"),\n",
    "    (\"IPO train\", \"data/rust/ipo/train\"),\n",
    "    (\"GRPO tasks\", \"data/rust/grpo\"),\n",
    "    (\"Eval tasks\", \"data/rust/eval\"),\n",
    "]\n",
    "\n",
    "print(\"Data Verification:\")\n",
    "print(\"=\" * 60)\n",
    "for name, path in data_checks:\n",
    "    exists = os.path.exists(path)\n",
    "    if exists and os.path.isdir(path):\n",
    "        items = os.listdir(path)\n",
    "        print(f\"  \\u2713 {name}: {path} ({len(items)} items)\")\n",
    "    elif exists:\n",
    "        size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "        print(f\"  \\u2713 {name}: {path} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        needed = True\n",
    "        if CONFIG[\"training_scope\"] == \"skip_to_rl\" and name in (\"Mutations\", \"Lang Rust train\", \"Core Agent train\"):\n",
    "            needed = False\n",
    "        if CONFIG[\"training_scope\"] == \"lang_adapter_only\" and name in (\"IPO train\", \"GRPO tasks\"):\n",
    "            needed = False\n",
    "        sym = \"\\u2717\" if needed else \"\\u2014\"\n",
    "        label = \"MISSING\" if needed else \"not needed\"\n",
    "        print(f\"  {sym} {name}: {label}\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "861c7d3cd76a69d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Lang Adapter Training\n",
    "\n",
    "Train a QLoRA adapter (rank 64) to specialise GPT-OSS 20B on Rust syntax, stdlib, and idioms.\n",
    "Then merge the adapter into the base weights for downstream training.\n",
    "\n",
    "**v2:** Split LoRA backend auto-enabled for 7-12x faster MoE training."
   ],
   "id": "31fd053ef297615e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Train lang_rust Adapter\n",
    "\n",
    "v2: Split LoRA enabled via UNSLOTH_MOE_BACKEND env var (set in 0.5).\n"
   ],
   "id": "fd35fdde7902ddda"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "    print(\"Skipping — scope is skip_to_rl\")\n",
    "else:\n",
    "    batch = CONFIG[\"lang_rust_batch\"]\n",
    "    grad_accum = CONFIG[\"lang_rust_grad_accum\"]\n",
    "    max_steps = CONFIG[\"lang_rust_max_steps\"]\n",
    "    seq_len = CONFIG[\"lang_rust_seq_len\"]\n",
    "\n",
    "    cmd = f\"python scripts/13_train_lang_adapter.py\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    print(f\"Training lang_rust adapter...\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Seq length: {seq_len} (from config)\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/lang_rust\", \"checkpoints/lang_rust\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")\n"
   ],
   "id": "4719fb4569fd4a48",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Merge lang_rust into Base\n"
   ],
   "id": "6d945162fc6c4887"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "    print(\"Skipping — scope is skip_to_rl\")\n",
    "else:\n",
    "    print(\"Merging lang_rust adapter into base model...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/19_merge_adapter.py \\\n",
    "        --adapter_path checkpoints/lang_rust/final \\\n",
    "        --output_dir checkpoints/gpt-oss-20b-rust-merged \\\n",
    "        --export_formats hf\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/gpt-oss-20b-rust-merged\", \"checkpoints/gpt-oss-20b-rust-merged\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nMerged model backed up to Drive.\")\n"
   ],
   "id": "14dd6ab10bc01ad0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Verify Merge\n"
   ],
   "id": "8a649a98ab1ceee0"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "    print(\"Skipping \\u2014 scope is skip_to_rl\")\n",
    "else:\n",
    "    merged_path = \"checkpoints/gpt-oss-20b-rust-merged\"\n",
    "    adapter_path = \"checkpoints/lang_rust/final\"\n",
    "\n",
    "    print(\"Merge Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(merged_path):\n",
    "        files = os.listdir(merged_path)\n",
    "        safetensors = [f for f in files if f.endswith(\".safetensors\")]\n",
    "        print(f\"  \\u2713 Merged model: {merged_path}\")\n",
    "        print(f\"    {len(safetensors)} safetensors shard(s), {len(files)} total files\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 Merged model not found at {merged_path}\")\n",
    "\n",
    "    if os.path.exists(adapter_path):\n",
    "        adapter_files = os.listdir(adapter_path)\n",
    "        print(f\"  \\u2713 Adapter: {adapter_path} ({len(adapter_files)} files)\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 Adapter not found at {adapter_path}\")\n",
    "\n",
    "    if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "        print(\"\\n\\u2713 lang_adapter_only scope complete. Stopping here.\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ],
   "id": "875f898a2de17db",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Core Agent SFT\n",
    "\n",
    "Train a higher-rank LoRA adapter (rank 128) on agent trajectories with tool use.\n",
    "Uses the merged lang_rust model as the base.\n",
    "\n",
    "**v2:** Auto uncontaminated packing (3x faster, zero-config). Flex Attention for long context."
   ],
   "id": "603346a356305aad"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Train core_agent Adapter\n",
    "\n",
    "v2: Auto packing (3x faster) + Split LoRA backend enabled.\n"
   ],
   "id": "2931e91685b890ab"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] in (\"lang_adapter_only\", \"skip_to_rl\"):\n",
    "    print(f\"Skipping — scope is {CONFIG['training_scope']}\")\n",
    "else:\n",
    "    batch = CONFIG[\"core_agent_batch\"]\n",
    "    grad_accum = CONFIG[\"core_agent_grad_accum\"]\n",
    "    max_steps = CONFIG[\"core_agent_max_steps\"]\n",
    "    seq_len = CONFIG[\"core_agent_seq_len\"]\n",
    "\n",
    "    cmd = f\"python scripts/14_train_core_agent.py\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    print(f\"Training core_agent adapter...\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Seq length: {seq_len} (from config)\")\n",
    "    print(f\"  LoRA rank: 128\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(f\"  Auto packing: enabled (uncontaminated)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/core_agent\", \"checkpoints/core_agent\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")\n"
   ],
   "id": "50b1d5d150eae5d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Verify core_agent\n"
   ],
   "id": "9f890c4676329aad"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] in (\"lang_adapter_only\", \"skip_to_rl\"):\n",
    "    print(f\"Skipping \\u2014 scope is {CONFIG['training_scope']}\")\n",
    "else:\n",
    "    ckpt_path = \"checkpoints/core_agent/final\"\n",
    "\n",
    "    print(\"Core Agent Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(ckpt_path):\n",
    "        files = os.listdir(ckpt_path)\n",
    "        print(f\"  \\u2713 Checkpoint: {ckpt_path} ({len(files)} files)\")\n",
    "\n",
    "        adapter_config = os.path.join(ckpt_path, \"adapter_config.json\")\n",
    "        if os.path.exists(adapter_config):\n",
    "            import json\n",
    "            with open(adapter_config) as f:\n",
    "                cfg = json.load(f)\n",
    "            print(f\"    LoRA rank: {cfg.get('r', '?')}\")\n",
    "            print(f\"    Alpha: {cfg.get('lora_alpha', '?')}\")\n",
    "            print(f\"    Target modules: {cfg.get('target_modules', '?')}\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 Checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ],
   "id": "7935e2cb9e427223",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Preference Optimisation (IPO)\n",
    "\n",
    "Train with Identity Preference Optimisation on ranked pairs.\n",
    "Very low learning rate (5e-7), 1 epoch only to avoid collapse.\n",
    "\n",
    "**v2:** FP8 weights on H100 (60% less VRAM). Expert utilisation monitoring."
   ],
   "id": "4dba79ff4175093b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Train with IPO\n",
    "\n",
    "v2: FP8 on H100, expert utilisation monitoring, Split LoRA.\n"
   ],
   "id": "54a5adb1ec756442"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping — scope is lang_adapter_only\")\n",
    "else:\n",
    "    batch = CONFIG[\"ipo_batch\"]\n",
    "    grad_accum = CONFIG[\"ipo_grad_accum\"]\n",
    "    max_steps = CONFIG[\"ipo_max_steps\"]\n",
    "\n",
    "    if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "        ipo_checkpoint = \"checkpoints/core_agent/final\"\n",
    "        print(\"Using existing core_agent checkpoint (skip_to_rl mode)\")\n",
    "    else:\n",
    "        ipo_checkpoint = \"checkpoints/core_agent/final\"\n",
    "\n",
    "    cmd = f\"python scripts/17_ipo_preference.py\"\n",
    "    cmd += f\" --checkpoint {ipo_checkpoint}\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    print(f\"Training with IPO...\")\n",
    "    print(f\"  Checkpoint: {ipo_checkpoint}\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Loss: IPO (beta=0.1)\")\n",
    "    print(f\"  Load mode: {CONFIG['load_mode']}\")\n",
    "    print(f\"  Split LoRA backend: {CONFIG['moe_backend']}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/core_agent_ipo\", \"checkpoints/core_agent_ipo\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")\n"
   ],
   "id": "d1401e64737e0b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Verify IPO\n"
   ],
   "id": "442df61074a8cd86"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    ckpt_path = \"checkpoints/core_agent_ipo/final\"\n",
    "\n",
    "    print(\"IPO Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(ckpt_path):\n",
    "        files = os.listdir(ckpt_path)\n",
    "        print(f\"  \\u2713 IPO checkpoint: {ckpt_path} ({len(files)} files)\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 IPO checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "    # Check tensorboard logs for KL divergence\n",
    "    tb_dir = \"checkpoints/core_agent_ipo\"\n",
    "    tb_files = []\n",
    "    if os.path.exists(tb_dir):\n",
    "        for root, dirs, fnames in os.walk(tb_dir):\n",
    "            for fn in fnames:\n",
    "                if fn.startswith(\"events.out.tfevents\"):\n",
    "                    tb_files.append(os.path.join(root, fn))\n",
    "    if tb_files:\n",
    "        print(f\"  \\u2713 TensorBoard logs found ({len(tb_files)} event files)\")\n",
    "        print(f\"    Monitor KL divergence: warn >0.3, abort >0.5\")\n",
    "    else:\n",
    "        print(f\"  \\u2014 No TensorBoard logs found\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ],
   "id": "d83944ef11775585",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: GRPO RL\n",
    "\n",
    "Group Relative Policy Optimisation with execution-based rewards.\n",
    "Generates N completions per prompt, runs `cargo check/test/clippy`, computes group-relative advantages.\n",
    "\n",
    "**v2 Optimisations:**\n",
    "- FP8 RL with vLLM inference on H100 (1.6x throughput)\n",
    "- Chunked batching for 7x longer context\n",
    "- Extended curriculum: 65K context on H100 (up from 32K)\n",
    "- Harmony format compliance reward to prevent infinite reasoning loops\n",
    "\n",
    "**This step is optional** (`include_grpo=False` to skip)."
   ],
   "id": "a10e7f0a4032c87e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Train with GRPO\n",
    "\n",
    "v2: FP8 RL + vLLM (H100), chunked batching, extended curriculum.\n"
   ],
   "id": "640c22f7ecce99e7"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping — scope is lang_adapter_only\")\n",
    "elif not CONFIG[\"include_grpo\"]:\n",
    "    print(\"Skipping — GRPO disabled (include_grpo=False)\")\n",
    "else:\n",
    "    batch = CONFIG[\"grpo_batch\"]\n",
    "    grad_accum = CONFIG[\"grpo_grad_accum\"]\n",
    "    max_steps = CONFIG[\"grpo_max_steps\"]\n",
    "    max_seq = CONFIG[\"grpo_seq_len\"]\n",
    "\n",
    "    grpo_checkpoint = \"checkpoints/core_agent_ipo/final\"\n",
    "\n",
    "    cmd = f\"python scripts/18_grpo_rl.py\"\n",
    "    cmd += f\" --checkpoint {grpo_checkpoint}\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    # v2: Note which optimisations are active\n",
    "    v2_features = []\n",
    "    v2_features.append(f\"Split LoRA ({CONFIG['moe_backend']})\")\n",
    "    if CONFIG[\"load_mode\"] == \"fp8\":\n",
    "        v2_features.append(\"FP8 weights\")\n",
    "    if CONFIG[\"fast_inference\"]:\n",
    "        v2_features.append(\"vLLM inference\")\n",
    "    v2_features.append(\"Chunked batching (auto)\")\n",
    "    v2_features.append(\"Auto packing\")\n",
    "\n",
    "    if CONFIG[\"gpu_tier\"] == \"a100_40gb\":\n",
    "        print(\"NOTE: 40GB GPU — GRPO sequence length capped at 16384\")\n",
    "\n",
    "    print(f\"Training with GRPO (v2)...\")\n",
    "    print(f\"  Checkpoint: {grpo_checkpoint}\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Max seq length: {max_seq}\")\n",
    "    print(f\"  Generations per prompt: {CONFIG['grpo_num_gen']}\")\n",
    "    print(f\"\\n  v2 features active:\")\n",
    "    for feat in v2_features:\n",
    "        print(f\"    ✓ {feat}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    drive_helper.backup(\"checkpoints/core_agent_grpo\", \"checkpoints/core_agent_grpo\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")\n"
   ],
   "id": "ced528c03911d4d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Verify GRPO\n"
   ],
   "id": "34e15c6770525e08"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "elif not CONFIG[\"include_grpo\"]:\n",
    "    print(\"Skipping \\u2014 GRPO disabled\")\n",
    "else:\n",
    "    ckpt_path = \"checkpoints/core_agent_grpo/final\"\n",
    "\n",
    "    print(\"GRPO Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(ckpt_path):\n",
    "        files = os.listdir(ckpt_path)\n",
    "        print(f\"  \\u2713 GRPO checkpoint: {ckpt_path} ({len(files)} files)\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 GRPO checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ],
   "id": "1f051b9601545463",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Evaluation\n",
    "\n",
    "Evaluate the best checkpoint on held-out Rust tasks using execution-based metrics\n",
    "(cargo check, cargo test, clippy)."
   ],
   "id": "c3395dfa3ecbc680"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Run Rust Evaluation\n"
   ],
   "id": "1fb7069b4d6fddef"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping — scope is lang_adapter_only\")\n",
    "else:\n",
    "    # Determine best checkpoint\n",
    "    if CONFIG[\"include_grpo\"] and os.path.exists(\"checkpoints/core_agent_grpo/final\"):\n",
    "        eval_checkpoint = \"checkpoints/core_agent_grpo/final\"\n",
    "    elif os.path.exists(\"checkpoints/core_agent_ipo/final\"):\n",
    "        eval_checkpoint = \"checkpoints/core_agent_ipo/final\"\n",
    "    elif os.path.exists(\"checkpoints/core_agent/final\"):\n",
    "        eval_checkpoint = \"checkpoints/core_agent/final\"\n",
    "    else:\n",
    "        eval_checkpoint = \"checkpoints/core_agent_ipo/final\"\n",
    "\n",
    "    num_samples = CONFIG[\"eval_num_samples\"]\n",
    "\n",
    "    print(f\"Evaluating checkpoint: {eval_checkpoint}\")\n",
    "    print(f\"Samples: {num_samples}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/eval_rust_agent.py \\\n",
    "        --checkpoint {eval_checkpoint} \\\n",
    "        --num_samples {num_samples}\n",
    "\n",
    "    drive_helper.backup(\"evals/rust_agent\", \"evals/rust_agent\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nResults backed up to Drive.\")\n"
   ],
   "id": "bad0cdf0e5a54cb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Check Promotion Gates\n"
   ],
   "id": "b0a563f4effe713f"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    print(\"Checking promotion gates...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/12_check_gates.py rust_agent"
   ],
   "id": "4197d776a316d2cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Display Results\n"
   ],
   "id": "d213ef309b2bacd7"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    import json\n",
    "\n",
    "    metrics_path = \"evals/rust_agent/metrics.json\"\n",
    "\n",
    "    if os.path.exists(metrics_path):\n",
    "        with open(metrics_path) as f:\n",
    "            metrics = json.load(f)\n",
    "\n",
    "        targets = {\n",
    "            \"cargo_check_pass_rate\": (0.85, \"higher\"),\n",
    "            \"cargo_test_pass_rate\": (0.70, \"higher\"),\n",
    "            \"clippy_clean_rate\": (0.80, \"higher\"),\n",
    "            \"iterations_to_green_median\": (3, \"lower\"),\n",
    "            \"diff_size_median\": (50, \"lower\"),\n",
    "            \"tool_call_format_accuracy\": (0.99, \"higher\"),\n",
    "            \"hallucinated_api_rate\": (0.05, \"lower\"),\n",
    "        }\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "        print(\"EVALUATION RESULTS\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"{'Metric':<32} {'Value':>8} {'Target':>8} {'Status':>8}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        for key, (target, direction) in targets.items():\n",
    "            value = metrics.get(key)\n",
    "            if value is None:\n",
    "                print(f\"{key:<32} {'N/A':>8} {target:>8} {'\\u2014':>8}\")\n",
    "                continue\n",
    "\n",
    "            if direction == \"higher\":\n",
    "                passed = value >= target\n",
    "            else:\n",
    "                passed = value <= target\n",
    "\n",
    "            status = \"\\u2713 PASS\" if passed else \"\\u2717 FAIL\"\n",
    "            fmt_val = f\"{value:.2%}\" if isinstance(value, float) and value <= 1 else f\"{value}\"\n",
    "            fmt_tgt = f\"{target:.0%}\" if isinstance(target, float) and target <= 1 else f\"{target}\"\n",
    "            print(f\"{key:<32} {fmt_val:>8} {fmt_tgt:>8} {status:>8}\")\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "    else:\n",
    "        print(f\"\\u2717 Metrics file not found at {metrics_path}\")\n",
    "        print(\"Run evaluation (6.1) first.\")"
   ],
   "id": "6ff37a75b3dd990f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Test Model\n",
    "\n",
    "Load the trained model and generate Rust code interactively.\n",
    "\n",
    "**v2:** FP8 loading on H100 for faster inference. `fast_inference=True` enables vLLM backend."
   ],
   "id": "856732da9a5a7910"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Load Model\n",
    "\n",
    "v2: FP8 loading on H100, vLLM-backed inference.\n"
   ],
   "id": "454dd084fa7f33d0"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "CHECKPOINT_PRIORITY = [\n",
    "    \"checkpoints/core_agent_grpo/final\",\n",
    "    \"checkpoints/core_agent_ipo/final\",\n",
    "    \"checkpoints/core_agent/final\",\n",
    "    \"checkpoints/gpt-oss-20b-rust-merged\",\n",
    "]\n",
    "\n",
    "MODEL_PATH = None\n",
    "for path in CHECKPOINT_PRIORITY:\n",
    "    if os.path.exists(path):\n",
    "        MODEL_PATH = path\n",
    "        break\n",
    "\n",
    "if MODEL_PATH is None:\n",
    "    print(\"\\u2717 No checkpoint found. Train the model first.\")\n",
    "else:\n",
    "    print(f\"Loading model from: {MODEL_PATH}\")\n",
    "\n",
    "    # v2: Use FP8 on H100, 4-bit otherwise\n",
    "    load_kwargs = {\n",
    "        \"max_seq_length\": 4096,\n",
    "        \"dtype\": torch.bfloat16,\n",
    "    }\n",
    "    if CONFIG.get(\"load_mode\") == \"fp8\" and CONFIG.get(\"use_fp8\"):\n",
    "        load_kwargs[\"load_in_fp8\"] = True\n",
    "        print(\"  Mode: FP8 (H100)\")\n",
    "    else:\n",
    "        load_kwargs[\"load_in_4bit\"] = True\n",
    "        print(\"  Mode: 4-bit QLoRA\")\n",
    "\n",
    "    if CONFIG.get(\"fast_inference\"):\n",
    "        load_kwargs[\"fast_inference\"] = True\n",
    "        print(\"  Inference: vLLM backend\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(MODEL_PATH, **load_kwargs)\n",
    "    FastLanguageModel.for_inference(model)\n",
    "\n",
    "    print(\"\\u2713 Model loaded!\")"
   ],
   "id": "58e85ca44306ad95",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Generate Rust Code\n",
    "\n",
    "Tests the model on 3 pre-defined Rust prompts using Harmony format.\n"
   ],
   "id": "2a8c82c63929e765"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"scripts\")\n",
    "from dataset_formatters.harmony import encode_harmony_messages\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    \"Write a Rust function `fn merge_sorted(a: &[i32], b: &[i32]) -> Vec<i32>` that merges two sorted slices into a single sorted vector.\",\n",
    "    \"This Rust code fails the borrow checker. Fix it:\\n```rust\\nfn main() {\\n    let mut v = vec![1, 2, 3];\\n    let first = &v[0];\\n    v.push(4);\\n    println!(\\\"{}\\\", first);\\n}\\n```\",\n",
    "    \"Write an async Rust function using tokio that fetches a URL with reqwest, retries up to 3 times on failure, and returns the response body as a String.\",\n",
    "]\n",
    "\n",
    "def generate_rust(prompt, max_tokens=1024):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted = encode_harmony_messages(\n",
    "        messages,\n",
    "        developer_instructions=\"You are a Rust programming expert. Write correct, idiomatic code.\",\n",
    "    )\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.3,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "for i, prompt in enumerate(TEST_PROMPTS, 1):\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Test {i}: {prompt[:80]}...\")\n",
    "    print(\"=\" * 60)\n",
    "    response = generate_rust(prompt)\n",
    "    print(response)\n",
    "    print()"
   ],
   "id": "fe79b1c5cd8fa9af",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Custom Prompt\n"
   ],
   "id": "f26961ac0a874651"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "CUSTOM_PROMPT = \"Write a Rust function that reads a CSV file and returns the sum of a specified column.\"\n",
    "\n",
    "print(f\"Prompt: {CUSTOM_PROMPT}\")\n",
    "print(\"=\" * 60)\n",
    "print(generate_rust(CUSTOM_PROMPT))"
   ],
   "id": "4b4b7f32818a3d05",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Export\n",
    "\n",
    "Merge the final adapter and export to HuggingFace + GGUF formats.\n",
    "\n",
    "**v2:** Optional QAT export for 97-100% MXFP4 quality retention (vs 59-89% with PTQ)."
   ],
   "id": "2cf19b6ed97d019e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Export to GGUF\n",
    "\n",
    "Merges the best adapter and exports as HF safetensors + GGUF Q4.\n"
   ],
   "id": "91788a1701ee9837"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ADAPTER_PRIORITY = [\n",
    "    \"checkpoints/core_agent_grpo/final\",\n",
    "    \"checkpoints/core_agent_ipo/final\",\n",
    "    \"checkpoints/core_agent/final\",\n",
    "    \"checkpoints/lang_rust/final\",\n",
    "]\n",
    "\n",
    "adapter_path = None\n",
    "for path in ADAPTER_PRIORITY:\n",
    "    if os.path.exists(path):\n",
    "        adapter_path = path\n",
    "        break\n",
    "\n",
    "if adapter_path is None:\n",
    "    print(\"✗ No adapter checkpoint found.\")\n",
    "else:\n",
    "    export_dir = \"checkpoints/gpt-oss-20b-rust-export-v2\"\n",
    "    print(f\"Exporting adapter: {adapter_path}\")\n",
    "    print(f\"Output: {export_dir}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/19_merge_adapter.py \\\n",
    "        --adapter_path {adapter_path} \\\n",
    "        --output_dir {export_dir} \\\n",
    "        --export_formats hf gguf_q4\n",
    "\n",
    "    drive_helper.backup(export_dir, \"checkpoints/gpt-oss-20b-rust-export-v2\")\n",
    "    if DRIVE_MODE != \"local\":\n",
    "        print(\"\\nExport backed up to Drive.\")\n"
   ],
   "id": "d1b8a168f48fae53",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 QAT Export (Optional)\n",
    "\n",
    "v2: Quantisation-Aware Training for MXFP4 deployment.\n",
    "Recovers 97-100% quality vs 59-89% with post-training quantisation.\n",
    "Requires: `pip install nvidia-modelopt`\n"
   ],
   "id": "c632af3d6ca89abd"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if not CONFIG.get(\"enable_qat_export\"):\n",
    "    print(\"QAT export disabled. Enable via enable_qat_export=True in Step 0.3.\")\n",
    "    print(\"\\nQAT recovers 97-100% quality when deploying to MXFP4,\")\n",
    "    print(\"vs 59-89% with standard post-training quantisation (PTQ).\")\n",
    "else:\n",
    "    export_dir = \"checkpoints/gpt-oss-20b-rust-export-v2\"\n",
    "    qat_dir = \"checkpoints/gpt-oss-20b-rust-qat\"\n",
    "\n",
    "    if not os.path.exists(export_dir):\n",
    "        print(\"\\u2717 Run standard export (8.1) first.\")\n",
    "    else:\n",
    "        print(\"Running QAT pass on merged model...\")\n",
    "        print(\"  This fine-tunes with MXFP4-aware quantisation at reduced LR (1e-5).\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        try:\n",
    "            import modelopt.torch.quantization as mtq\n",
    "            print(\"\\u2713 nvidia-modelopt available\")\n",
    "\n",
    "            # QAT would be run here via mtq.quantize()\n",
    "            # For now, document the expected command:\n",
    "            print(\"\\nQAT pipeline (manual steps):\")\n",
    "            print(f\"  1. Load merged BF16 model from {export_dir}\")\n",
    "            print(f\"  2. mtq.quantize(model, config=mtq.MXFP4_DEFAULT_CFG)\")\n",
    "            print(f\"  3. Fine-tune for ~100 steps at LR 1e-5\")\n",
    "            print(f\"  4. Export to {qat_dir}\")\n",
    "        except ImportError:\n",
    "            print(\"\\u2717 nvidia-modelopt not installed.\")\n",
    "            print(\"  Install: pip install nvidia-modelopt\")\n",
    "            print(\"  See: https://developer.nvidia.com/blog/fine-tuning-gpt-oss-for-accuracy-and-performance-with-quantization-aware-training/\")"
   ],
   "id": "684742559682938a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Download GGUF\n"
   ],
   "id": "88ce6eb89550c5d9"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    import glob\n",
    "\n",
    "    export_dir = \"checkpoints/gpt-oss-20b-rust-export-v2\"\n",
    "    gguf_files = glob.glob(os.path.join(export_dir, \"*.gguf\"))\n",
    "\n",
    "    if gguf_files:\n",
    "        gguf_path = gguf_files[0]\n",
    "        size_gb = os.path.getsize(gguf_path) / (1024**3)\n",
    "        print(f\"Downloading: {os.path.basename(gguf_path)} ({size_gb:.1f} GB)\")\n",
    "        files.download(gguf_path)\n",
    "    else:\n",
    "        print(\"\\u2717 No GGUF file found. Run export (8.1) first.\")\n",
    "else:\n",
    "    print(\"Download not available outside Colab.\")\n",
    "    print(\"GGUF file is at: checkpoints/gpt-oss-20b-rust-export-v2/\")"
   ],
   "id": "1240d8a590699baf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Complete!\n",
    "\n",
    "Your GPT-OSS 20B Rust coding agent (v2) is trained and ready to use.\n",
    "\n",
    "**v2 Optimisations Applied:**\n",
    "- Split LoRA: 7-12x faster MoE training\n",
    "- FP8 RL: 1.6x throughput on H100 (60% less VRAM)\n",
    "- Auto packing: 3x faster SFT\n",
    "- Chunked GRPO: 65K context on H100 (up from 32K)\n",
    "- QAT export: 97-100% MXFP4 quality (if enabled)\n",
    "\n",
    "**Outputs:**\n",
    "- Checkpoints: `checkpoints/core_agent_{ipo,grpo}/final`\n",
    "- Evaluation: `evals/rust_agent/metrics.json`\n",
    "- Exported model: `checkpoints/gpt-oss-20b-rust-export-v2/`\n",
    "- All backed up to Google Drive: `gpt-oss-20b-rust-agent-v2/`\n",
    "\n",
    "**Next steps:**\n",
    "- Review evaluation metrics in Step 6.3\n",
    "- Test interactively in Step 7\n",
    "- Deploy the GGUF file with llama.cpp or Ollama\n",
    "- For MXFP4 deployment, enable QAT export in Step 8.2\n",
    "\n",
    "**References:**\n",
    "- [V2 Optimization Plan](../docs/V2_OPTIMIZATION_PLAN.md)\n",
    "- [Unsloth Split LoRA](https://unsloth.ai/docs/new/faster-moe)\n",
    "- [Unsloth FP8 RL](https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/fp8-reinforcement-learning)\n",
    "- [NVIDIA QAT for GPT-OSS](https://developer.nvidia.com/blog/fine-tuning-gpt-oss-for-accuracy-and-performance-with-quantization-aware-training/)"
   ],
   "id": "f2381c249430c4d6"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
