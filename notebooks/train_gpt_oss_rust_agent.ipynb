{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GPT-OSS 20B \u2192 Rust Coding Agent\n",
    "\n",
    "End-to-end pipeline for training a Rust coding agent on top of OpenAI's GPT-OSS 20B (MoE, ~3.6B active params).\n",
    "\n",
    "**4-Phase Pipeline:**\n",
    "1. **Lang Adapter** \u2014 Rust domain specialisation via QLoRA (script 13 + 19)\n",
    "2. **Core Agent SFT** \u2014 Agent trajectory training with tool use (script 14)\n",
    "3. **IPO Preference** \u2014 Identity Preference Optimisation on ranked pairs (script 17)\n",
    "4. **GRPO RL** \u2014 Group Relative Policy Optimisation with execution rewards (script 18)\n",
    "\n",
    "**Requirements:**\n",
    "- **GPU**: A100 40GB+ (80GB recommended for full context lengths)\n",
    "- **Storage**: Google Drive for persistent checkpoints\n",
    "- **Rust toolchain**: Installed automatically (rustup + cargo-mutants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 0.1 Mount Google Drive & Clone Repository\n",
    "import os\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted\")\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "\n",
    "REPO_URL = \"https://github.com/rmarnold/llm-training-pipeline.git\"  #@param {type:\"string\"}\n",
    "BRANCH = \"main\"  #@param {type:\"string\"}\n",
    "\n",
    "REPO_DIR = \"/content/llm-training-pipeline\"\n",
    "\n",
    "if IN_COLAB:\n",
    "    if os.path.exists(REPO_DIR):\n",
    "        %cd {REPO_DIR}\n",
    "        !git pull origin {BRANCH}\n",
    "    else:\n",
    "        !git clone -b {BRANCH} {REPO_URL} {REPO_DIR}\n",
    "        %cd {REPO_DIR}\n",
    "\n",
    "    PROJECT_ROOT = REPO_DIR\n",
    "else:\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"\\nProject root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 0.2 Install Dependencies\n",
    "#@markdown Installs pipeline deps, Unsloth (Colab-optimised), and the Rust toolchain.\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Installing Python dependencies...\")\n",
    "    print(\"=\" * 60)\n",
    "    !pip install -q -e \".[gpt_oss,rust_eval,colab]\"\n",
    "    !pip install -q \"unsloth[colab-new]\"\n",
    "    !pip install -q flash-attn --no-build-isolation 2>/dev/null || true\n",
    "\n",
    "    print(\"\\nInstalling Rust toolchain...\")\n",
    "    print(\"=\" * 60)\n",
    "    !curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
    "    os.environ[\"PATH\"] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "    !cargo install cargo-mutants\n",
    "\n",
    "    # Verification\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Dependency Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for pkg in [\"unsloth\", \"trl\", \"peft\", \"datasets\", \"tiktoken\"]:\n",
    "        try:\n",
    "            mod = __import__(pkg)\n",
    "            ver = getattr(mod, \"__version__\", \"OK\")\n",
    "            print(f\"\\u2713 {pkg}: {ver}\")\n",
    "        except ImportError as e:\n",
    "            print(f\"\\u2717 {pkg}: {e}\")\n",
    "\n",
    "    import subprocess\n",
    "    for cmd, label in [(\"cargo --version\", \"cargo\"), (\"cargo-mutants --version\", \"cargo-mutants\")]:\n",
    "        result = subprocess.run(cmd.split(), capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"\\u2713 {label}: {result.stdout.strip()}\")\n",
    "        else:\n",
    "            print(f\"\\u2717 {label}: not found\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"Running locally \\u2014 ensure deps are installed: pip install -e '.[gpt_oss,rust_eval]'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 0.3 Configure Pipeline { run: \"auto\" }\n",
    "#@markdown ---\n",
    "#@markdown ### Training Scope\n",
    "\n",
    "training_scope = \"full\"  #@param [\"full\", \"quick_test\", \"lang_adapter_only\", \"skip_to_rl\"]\n",
    "#@markdown **Scopes:**\n",
    "#@markdown - `full`: All 4 phases end-to-end\n",
    "#@markdown - `quick_test`: Short runs (100 steps each) to verify setup\n",
    "#@markdown - `lang_adapter_only`: Only train lang_rust adapter + merge\n",
    "#@markdown - `skip_to_rl`: Start from existing core_agent checkpoint (IPO + GRPO only)\n",
    "\n",
    "gpu_tier = \"a100_80gb\"  #@param [\"a100_40gb\", \"a100_80gb\"]\n",
    "#@markdown **GPU Tier** (auto-detected below; override here if needed)\n",
    "\n",
    "max_steps_override = 0  #@param {type:\"integer\"}\n",
    "#@markdown Set >0 to cap all training stages at this many steps (0 = use defaults)\n",
    "\n",
    "skip_data_generation = False  #@param {type:\"boolean\"}\n",
    "#@markdown Use pre-generated data from Drive instead of running mutation/trajectory generation\n",
    "\n",
    "include_grpo = True  #@param {type:\"boolean\"}\n",
    "#@markdown GRPO RL is slow; set False to skip\n",
    "\n",
    "# ============================================================\n",
    "# GPU TIER CONFIGS\n",
    "# ============================================================\n",
    "\n",
    "GPU_CONFIGS = {\n",
    "    \"a100_40gb\": {\n",
    "        \"lang_rust\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 8192, \"max_steps\": 3000},\n",
    "        \"core_agent\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 12288, \"max_steps\": 2000},\n",
    "        \"ipo\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 12288, \"max_steps\": 1000},\n",
    "        \"grpo\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 16384, \"max_steps\": 2000, \"num_gen\": 2},\n",
    "    },\n",
    "    \"a100_80gb\": {\n",
    "        \"lang_rust\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 8192, \"max_steps\": 5000},\n",
    "        \"core_agent\": {\"batch\": 1, \"grad_accum\": 4, \"seq_len\": 16384, \"max_steps\": 3000},\n",
    "        \"ipo\": {\"batch\": 1, \"grad_accum\": 16, \"seq_len\": 16384, \"max_steps\": 2000},\n",
    "        \"grpo\": {\"batch\": 1, \"grad_accum\": 8, \"seq_len\": 32768, \"max_steps\": 5000, \"num_gen\": 4},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Quick test overrides\n",
    "if training_scope == \"quick_test\":\n",
    "    max_steps_override = 100\n",
    "\n",
    "gpu_cfg = GPU_CONFIGS[gpu_tier]\n",
    "\n",
    "# Build CONFIG dict\n",
    "CONFIG = {\n",
    "    \"training_scope\": training_scope,\n",
    "    \"gpu_tier\": gpu_tier,\n",
    "    \"include_grpo\": include_grpo,\n",
    "    \"skip_data_generation\": skip_data_generation,\n",
    "    # Lang adapter\n",
    "    \"lang_rust_batch\": gpu_cfg[\"lang_rust\"][\"batch\"],\n",
    "    \"lang_rust_grad_accum\": gpu_cfg[\"lang_rust\"][\"grad_accum\"],\n",
    "    \"lang_rust_seq_len\": gpu_cfg[\"lang_rust\"][\"seq_len\"],\n",
    "    \"lang_rust_max_steps\": max_steps_override or gpu_cfg[\"lang_rust\"][\"max_steps\"],\n",
    "    # Core agent\n",
    "    \"core_agent_batch\": gpu_cfg[\"core_agent\"][\"batch\"],\n",
    "    \"core_agent_grad_accum\": gpu_cfg[\"core_agent\"][\"grad_accum\"],\n",
    "    \"core_agent_seq_len\": gpu_cfg[\"core_agent\"][\"seq_len\"],\n",
    "    \"core_agent_max_steps\": max_steps_override or gpu_cfg[\"core_agent\"][\"max_steps\"],\n",
    "    # IPO\n",
    "    \"ipo_batch\": gpu_cfg[\"ipo\"][\"batch\"],\n",
    "    \"ipo_grad_accum\": gpu_cfg[\"ipo\"][\"grad_accum\"],\n",
    "    \"ipo_seq_len\": gpu_cfg[\"ipo\"][\"seq_len\"],\n",
    "    \"ipo_max_steps\": max_steps_override or gpu_cfg[\"ipo\"][\"max_steps\"],\n",
    "    # GRPO\n",
    "    \"grpo_batch\": gpu_cfg[\"grpo\"][\"batch\"],\n",
    "    \"grpo_grad_accum\": gpu_cfg[\"grpo\"][\"grad_accum\"],\n",
    "    \"grpo_seq_len\": gpu_cfg[\"grpo\"][\"seq_len\"],\n",
    "    \"grpo_max_steps\": max_steps_override or gpu_cfg[\"grpo\"][\"max_steps\"],\n",
    "    \"grpo_num_gen\": gpu_cfg[\"grpo\"][\"num_gen\"],\n",
    "    # Mutation generation\n",
    "    \"max_mutations_per_repo\": 50 if training_scope == \"quick_test\" else 100,\n",
    "    \"mutation_jobs\": 4,\n",
    "    # Eval\n",
    "    \"eval_num_samples\": 10 if training_scope == \"quick_test\" else 50,\n",
    "}\n",
    "\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/gpt-oss-20b-rust-agent\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PIPELINE CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nScope: {training_scope.upper()}\")\n",
    "print(f\"GPU tier: {gpu_tier}\")\n",
    "print(f\"Include GRPO: {include_grpo}\")\n",
    "print(f\"Skip data gen: {skip_data_generation}\")\n",
    "if max_steps_override:\n",
    "    print(f\"Max steps override: {max_steps_override}\")\n",
    "print(f\"\\nLang Adapter:  batch={CONFIG['lang_rust_batch']} x grad_accum={CONFIG['lang_rust_grad_accum']}, seq={CONFIG['lang_rust_seq_len']}, steps={CONFIG['lang_rust_max_steps']}\")\n",
    "print(f\"Core Agent:    batch={CONFIG['core_agent_batch']} x grad_accum={CONFIG['core_agent_grad_accum']}, seq={CONFIG['core_agent_seq_len']}, steps={CONFIG['core_agent_max_steps']}\")\n",
    "print(f\"IPO:           batch={CONFIG['ipo_batch']} x grad_accum={CONFIG['ipo_grad_accum']}, seq={CONFIG['ipo_seq_len']}, steps={CONFIG['ipo_max_steps']}\")\n",
    "if include_grpo:\n",
    "    print(f\"GRPO:          batch={CONFIG['grpo_batch']} x grad_accum={CONFIG['grpo_grad_accum']}, seq={CONFIG['grpo_seq_len']}, steps={CONFIG['grpo_max_steps']}, gen={CONFIG['grpo_num_gen']}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 0.4 Set Up Persistent Storage\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(f\"Setting up storage at: {DRIVE_BASE}\")\n",
    "\n",
    "    # Create Drive directories\n",
    "    drive_dirs = [\n",
    "        \"checkpoints/lang_rust\",\n",
    "        \"checkpoints/core_agent\",\n",
    "        \"checkpoints/core_agent_ipo\",\n",
    "        \"checkpoints/core_agent_grpo\",\n",
    "        \"checkpoints/gpt-oss-20b-rust-merged\",\n",
    "        \"data/rust/lang_rust\",\n",
    "        \"data/rust/core_agent\",\n",
    "        \"data/rust/mutations\",\n",
    "        \"data/rust/ipo\",\n",
    "        \"data/rust/grpo\",\n",
    "        \"data/rust/eval\",\n",
    "        \"data/rust/repos\",\n",
    "        \"logs\",\n",
    "        \"evals/rust_agent\",\n",
    "    ]\n",
    "    for subdir in drive_dirs:\n",
    "        os.makedirs(os.path.join(DRIVE_BASE, subdir), exist_ok=True)\n",
    "\n",
    "    # Create symlinks for key directories\n",
    "    for dir_name in [\"checkpoints\", \"data\", \"logs\", \"evals\"]:\n",
    "        local_path = os.path.join(PROJECT_ROOT, dir_name)\n",
    "        drive_path = os.path.join(DRIVE_BASE, dir_name)\n",
    "\n",
    "        if os.path.exists(local_path) and not os.path.islink(local_path):\n",
    "            !cp -r {local_path}/* {drive_path}/ 2>/dev/null || true\n",
    "            !rm -rf {local_path}\n",
    "        elif os.path.islink(local_path):\n",
    "            os.unlink(local_path)\n",
    "\n",
    "        os.symlink(drive_path, local_path)\n",
    "        print(f\"  {dir_name} -> Drive\")\n",
    "\n",
    "    print(\"\\nStorage ready!\")\n",
    "else:\n",
    "    for d in [\"checkpoints\", \"data/rust\", \"logs\", \"evals/rust_agent\"]:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "    print(\"Local directories created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 0.5 Check GPU\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    capability = torch.cuda.get_device_capability()\n",
    "\n",
    "    CONFIG[\"use_fp8\"] = capability[0] >= 9\n",
    "\n",
    "    # Auto-detect GPU tier\n",
    "    detected_tier = \"a100_80gb\" if gpu_memory >= 70 else \"a100_40gb\"\n",
    "    if detected_tier != CONFIG[\"gpu_tier\"]:\n",
    "        print(f\"NOTE: Auto-detected {detected_tier}, overriding configured {CONFIG['gpu_tier']}\")\n",
    "        CONFIG[\"gpu_tier\"] = detected_tier\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"GPU: {gpu_name} ({gpu_memory:.0f} GB)\")\n",
    "    print(f\"Compute capability: {capability[0]}.{capability[1]}\")\n",
    "    print(f\"FP8: {'Available' if CONFIG['use_fp8'] else 'Not available'}\")\n",
    "    print(f\"Tier: {CONFIG['gpu_tier']}\")\n",
    "\n",
    "    if gpu_memory < 40:\n",
    "        print(\"\\nWARNING: <40 GB VRAM. GPT-OSS 20B (4-bit) needs ~12 GB for weights\")\n",
    "        print(\"but long-context training (16K+) may OOM. Consider reducing seq lengths.\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"No GPU detected!\")\n",
    "    CONFIG[\"use_fp8\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Data Generation\n",
    "\n",
    "Generates mutation data from curated Rust repos and agent trajectories.\n",
    "Skip this step if you have pre-generated data on Drive (`skip_data_generation=True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 1.1 Generate Mutation Data\n",
    "#@markdown Runs `cargo-mutants` on curated Rust repos to produce bug-fix training pairs.\n",
    "\n",
    "if CONFIG[\"skip_data_generation\"]:\n",
    "    print(\"Skipping data generation (using pre-generated data from Drive)\")\n",
    "else:\n",
    "    if CONFIG[\"training_scope\"] in (\"skip_to_rl\",):\n",
    "        print(\"Skipping \\u2014 not needed for this training scope\")\n",
    "    else:\n",
    "        max_muts = CONFIG[\"max_mutations_per_repo\"]\n",
    "        jobs = CONFIG[\"mutation_jobs\"]\n",
    "\n",
    "        print(f\"Generating mutations (max {max_muts}/repo, {jobs} parallel jobs)...\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        !python scripts/16_generate_mutations.py \\\n",
    "            --max_mutations_per_repo {max_muts} \\\n",
    "            --jobs {jobs}\n",
    "\n",
    "        # Backup to Drive\n",
    "        if IN_COLAB:\n",
    "            drive_mut = os.path.join(DRIVE_BASE, \"data/rust/mutations\")\n",
    "            !cp -r data/rust/mutations/* {drive_mut}/ 2>/dev/null || true\n",
    "            print(\"\\nBacked up mutations to Drive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 1.2 Generate Agent Trajectories\n",
    "#@markdown Generates multi-turn agent trajectories from mutations + Strandset in Harmony format.\n",
    "\n",
    "if CONFIG[\"skip_data_generation\"]:\n",
    "    print(\"Skipping data generation (using pre-generated data from Drive)\")\n",
    "else:\n",
    "    if CONFIG[\"training_scope\"] in (\"skip_to_rl\",):\n",
    "        print(\"Skipping \\u2014 not needed for this training scope\")\n",
    "    else:\n",
    "        max_samples = 500 if CONFIG[\"training_scope\"] == \"quick_test\" else 5000\n",
    "\n",
    "        print(f\"Generating trajectories (max {max_samples} per source)...\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        cmd = f\"python scripts/15_generate_trajectories.py --max_samples {max_samples}\"\n",
    "\n",
    "        # Point to mutations if they exist\n",
    "        mutations_path = \"data/rust/mutations/mutations.jsonl\"\n",
    "        if os.path.exists(mutations_path):\n",
    "            cmd += f\" --mutations_path {mutations_path}\"\n",
    "\n",
    "        !{cmd}\n",
    "\n",
    "        # Backup to Drive\n",
    "        if IN_COLAB:\n",
    "            drive_agent = os.path.join(DRIVE_BASE, \"data/rust/core_agent\")\n",
    "            !cp -r data/rust/core_agent/* {drive_agent}/ 2>/dev/null || true\n",
    "            print(\"\\nBacked up trajectories to Drive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 1.3 Verify Data\n",
    "\n",
    "data_checks = [\n",
    "    (\"Mutations\", \"data/rust/mutations\"),\n",
    "    (\"Lang Rust train\", \"data/rust/lang_rust/train\"),\n",
    "    (\"Core Agent train\", \"data/rust/core_agent/train\"),\n",
    "    (\"IPO train\", \"data/rust/ipo/train\"),\n",
    "    (\"GRPO tasks\", \"data/rust/grpo\"),\n",
    "    (\"Eval tasks\", \"data/rust/eval\"),\n",
    "]\n",
    "\n",
    "print(\"Data Verification:\")\n",
    "print(\"=\" * 60)\n",
    "for name, path in data_checks:\n",
    "    exists = os.path.exists(path)\n",
    "    if exists and os.path.isdir(path):\n",
    "        items = os.listdir(path)\n",
    "        print(f\"  \\u2713 {name}: {path} ({len(items)} items)\")\n",
    "    elif exists:\n",
    "        size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "        print(f\"  \\u2713 {name}: {path} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        needed = True\n",
    "        if CONFIG[\"training_scope\"] == \"skip_to_rl\" and name in (\"Mutations\", \"Lang Rust train\", \"Core Agent train\"):\n",
    "            needed = False\n",
    "        if CONFIG[\"training_scope\"] == \"lang_adapter_only\" and name in (\"IPO train\", \"GRPO tasks\"):\n",
    "            needed = False\n",
    "        sym = \"\\u2717\" if needed else \"\\u2014\"\n",
    "        label = \"MISSING\" if needed else \"not needed\"\n",
    "        print(f\"  {sym} {name}: {label}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Lang Adapter Training\n",
    "\n",
    "Train a QLoRA adapter (rank 64) to specialise GPT-OSS 20B on Rust syntax, stdlib, and idioms.\n",
    "Then merge the adapter into the base weights for downstream training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 2.1 Train lang_rust Adapter\n",
    "#@markdown Trains a per-language QLoRA adapter on Rust data (rank 64, 1 epoch).\n",
    "\n",
    "if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "    print(\"Skipping \\u2014 scope is skip_to_rl\")\n",
    "else:\n",
    "    batch = CONFIG[\"lang_rust_batch\"]\n",
    "    grad_accum = CONFIG[\"lang_rust_grad_accum\"]\n",
    "    max_steps = CONFIG[\"lang_rust_max_steps\"]\n",
    "    seq_len = CONFIG[\"lang_rust_seq_len\"]\n",
    "\n",
    "    cmd = f\"python scripts/13_train_lang_adapter.py\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    print(f\"Training lang_rust adapter...\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Seq length: {seq_len} (from config)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    # Backup checkpoint to Drive\n",
    "    if IN_COLAB:\n",
    "        !cp -r checkpoints/lang_rust/* {DRIVE_BASE}/checkpoints/lang_rust/ 2>/dev/null || true\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 2.2 Merge lang_rust into Base\n",
    "#@markdown Merges the lang_rust adapter into the GPT-OSS 20B base weights.\n",
    "\n",
    "if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "    print(\"Skipping \\u2014 scope is skip_to_rl\")\n",
    "else:\n",
    "    print(\"Merging lang_rust adapter into base model...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/19_merge_adapter.py \\\n",
    "        --adapter_path checkpoints/lang_rust/final \\\n",
    "        --output_dir checkpoints/gpt-oss-20b-rust-merged \\\n",
    "        --export_formats hf\n",
    "\n",
    "    # Backup merged model to Drive\n",
    "    if IN_COLAB:\n",
    "        !cp -r checkpoints/gpt-oss-20b-rust-merged/* {DRIVE_BASE}/checkpoints/gpt-oss-20b-rust-merged/ 2>/dev/null || true\n",
    "        print(\"\\nMerged model backed up to Drive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 2.3 Verify Merge\n",
    "\n",
    "if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "    print(\"Skipping \\u2014 scope is skip_to_rl\")\n",
    "else:\n",
    "    merged_path = \"checkpoints/gpt-oss-20b-rust-merged\"\n",
    "    adapter_path = \"checkpoints/lang_rust/final\"\n",
    "\n",
    "    print(\"Merge Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(merged_path):\n",
    "        files = os.listdir(merged_path)\n",
    "        safetensors = [f for f in files if f.endswith(\".safetensors\")]\n",
    "        print(f\"  \\u2713 Merged model: {merged_path}\")\n",
    "        print(f\"    {len(safetensors)} safetensors shard(s), {len(files)} total files\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 Merged model not found at {merged_path}\")\n",
    "\n",
    "    if os.path.exists(adapter_path):\n",
    "        adapter_files = os.listdir(adapter_path)\n",
    "        print(f\"  \\u2713 Adapter: {adapter_path} ({len(adapter_files)} files)\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 Adapter not found at {adapter_path}\")\n",
    "\n",
    "    if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "        print(\"\\n\\u2713 lang_adapter_only scope complete. Stopping here.\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Core Agent SFT\n",
    "\n",
    "Train a higher-rank LoRA adapter (rank 128) on agent trajectories with tool use.\n",
    "Uses the merged lang_rust model as the base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 3.1 Train core_agent Adapter\n",
    "#@markdown Trains the core agent adapter on Harmony-formatted trajectories (rank 128, 2 epochs).\n",
    "\n",
    "if CONFIG[\"training_scope\"] in (\"lang_adapter_only\", \"skip_to_rl\"):\n",
    "    print(f\"Skipping \\u2014 scope is {CONFIG['training_scope']}\")\n",
    "else:\n",
    "    batch = CONFIG[\"core_agent_batch\"]\n",
    "    grad_accum = CONFIG[\"core_agent_grad_accum\"]\n",
    "    max_steps = CONFIG[\"core_agent_max_steps\"]\n",
    "    seq_len = CONFIG[\"core_agent_seq_len\"]\n",
    "\n",
    "    cmd = f\"python scripts/14_train_core_agent.py\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    print(f\"Training core_agent adapter...\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Seq length: {seq_len} (from config)\")\n",
    "    print(f\"  LoRA rank: 128\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    # Backup checkpoint to Drive\n",
    "    if IN_COLAB:\n",
    "        !cp -r checkpoints/core_agent/* {DRIVE_BASE}/checkpoints/core_agent/ 2>/dev/null || true\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 3.2 Verify core_agent\n",
    "\n",
    "if CONFIG[\"training_scope\"] in (\"lang_adapter_only\", \"skip_to_rl\"):\n",
    "    print(f\"Skipping \\u2014 scope is {CONFIG['training_scope']}\")\n",
    "else:\n",
    "    ckpt_path = \"checkpoints/core_agent/final\"\n",
    "\n",
    "    print(\"Core Agent Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(ckpt_path):\n",
    "        files = os.listdir(ckpt_path)\n",
    "        print(f\"  \\u2713 Checkpoint: {ckpt_path} ({len(files)} files)\")\n",
    "\n",
    "        # Check adapter config for trainable params\n",
    "        adapter_config = os.path.join(ckpt_path, \"adapter_config.json\")\n",
    "        if os.path.exists(adapter_config):\n",
    "            import json\n",
    "            with open(adapter_config) as f:\n",
    "                cfg = json.load(f)\n",
    "            print(f\"    LoRA rank: {cfg.get('r', '?')}\")\n",
    "            print(f\"    Alpha: {cfg.get('lora_alpha', '?')}\")\n",
    "            print(f\"    Target modules: {cfg.get('target_modules', '?')}\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 Checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Preference Optimisation (IPO)\n",
    "\n",
    "Train with Identity Preference Optimisation on ranked pairs.\n",
    "Very low learning rate (5e-7), 1 epoch only to avoid collapse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 4.1 Train with IPO\n",
    "#@markdown IPO training from the core_agent checkpoint. Single epoch, very low LR.\n",
    "\n",
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    batch = CONFIG[\"ipo_batch\"]\n",
    "    grad_accum = CONFIG[\"ipo_grad_accum\"]\n",
    "    max_steps = CONFIG[\"ipo_max_steps\"]\n",
    "\n",
    "    # Determine checkpoint source\n",
    "    if CONFIG[\"training_scope\"] == \"skip_to_rl\":\n",
    "        ipo_checkpoint = \"checkpoints/core_agent/final\"\n",
    "        print(\"Using existing core_agent checkpoint (skip_to_rl mode)\")\n",
    "    else:\n",
    "        ipo_checkpoint = \"checkpoints/core_agent/final\"\n",
    "\n",
    "    cmd = f\"python scripts/17_ipo_preference.py\"\n",
    "    cmd += f\" --checkpoint {ipo_checkpoint}\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    print(f\"Training with IPO...\")\n",
    "    print(f\"  Checkpoint: {ipo_checkpoint}\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Loss: IPO (beta=0.1)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    # Backup checkpoint to Drive\n",
    "    if IN_COLAB:\n",
    "        !cp -r checkpoints/core_agent_ipo/* {DRIVE_BASE}/checkpoints/core_agent_ipo/ 2>/dev/null || true\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 4.2 Verify IPO\n",
    "\n",
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    ckpt_path = \"checkpoints/core_agent_ipo/final\"\n",
    "\n",
    "    print(\"IPO Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(ckpt_path):\n",
    "        files = os.listdir(ckpt_path)\n",
    "        print(f\"  \\u2713 IPO checkpoint: {ckpt_path} ({len(files)} files)\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 IPO checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "    # Check tensorboard logs for KL divergence if available\n",
    "    tb_dir = \"checkpoints/core_agent_ipo\"\n",
    "    tb_files = []\n",
    "    if os.path.exists(tb_dir):\n",
    "        for root, dirs, fnames in os.walk(tb_dir):\n",
    "            for fn in fnames:\n",
    "                if fn.startswith(\"events.out.tfevents\"):\n",
    "                    tb_files.append(os.path.join(root, fn))\n",
    "    if tb_files:\n",
    "        print(f\"  \\u2713 TensorBoard logs found ({len(tb_files)} event files)\")\n",
    "        print(f\"    Monitor KL divergence: warn >0.3, abort >0.5\")\n",
    "    else:\n",
    "        print(f\"  \\u2014 No TensorBoard logs found\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: GRPO RL\n",
    "\n",
    "Group Relative Policy Optimisation with execution-based rewards.\n",
    "Generates N completions per prompt, runs `cargo check/test/clippy`, computes group-relative advantages.\n",
    "\n",
    "**This step is optional** (`include_grpo=False` to skip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 5.1 Train with GRPO\n",
    "#@markdown GRPO RL training from IPO checkpoint. Uses execution rewards.\n",
    "\n",
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "elif not CONFIG[\"include_grpo\"]:\n",
    "    print(\"Skipping \\u2014 GRPO disabled (include_grpo=False)\")\n",
    "else:\n",
    "    batch = CONFIG[\"grpo_batch\"]\n",
    "    grad_accum = CONFIG[\"grpo_grad_accum\"]\n",
    "    max_steps = CONFIG[\"grpo_max_steps\"]\n",
    "\n",
    "    grpo_checkpoint = \"checkpoints/core_agent_ipo/final\"\n",
    "\n",
    "    cmd = f\"python scripts/18_grpo_rl.py\"\n",
    "    cmd += f\" --checkpoint {grpo_checkpoint}\"\n",
    "    cmd += f\" --per_device_train_batch_size {batch}\"\n",
    "    cmd += f\" --gradient_accumulation_steps {grad_accum}\"\n",
    "    cmd += f\" --max_steps {max_steps}\"\n",
    "\n",
    "    # On 40GB, disable curriculum (cap at 16K)\n",
    "    if CONFIG[\"gpu_tier\"] == \"a100_40gb\":\n",
    "        print(\"NOTE: 40GB GPU \\u2014 GRPO sequence length capped at 16384\")\n",
    "\n",
    "    print(f\"Training with GRPO...\")\n",
    "    print(f\"  Checkpoint: {grpo_checkpoint}\")\n",
    "    print(f\"  Batch: {batch} x {grad_accum} = {batch * grad_accum}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Generations per prompt: {CONFIG['grpo_num_gen']}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !{cmd}\n",
    "\n",
    "    # Backup checkpoint to Drive\n",
    "    if IN_COLAB:\n",
    "        !cp -r checkpoints/core_agent_grpo/* {DRIVE_BASE}/checkpoints/core_agent_grpo/ 2>/dev/null || true\n",
    "        print(\"\\nCheckpoint backed up to Drive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 5.2 Verify GRPO\n",
    "\n",
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "elif not CONFIG[\"include_grpo\"]:\n",
    "    print(\"Skipping \\u2014 GRPO disabled\")\n",
    "else:\n",
    "    ckpt_path = \"checkpoints/core_agent_grpo/final\"\n",
    "\n",
    "    print(\"GRPO Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if os.path.exists(ckpt_path):\n",
    "        files = os.listdir(ckpt_path)\n",
    "        print(f\"  \\u2713 GRPO checkpoint: {ckpt_path} ({len(files)} files)\")\n",
    "    else:\n",
    "        print(f\"  \\u2717 GRPO checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Evaluation\n",
    "\n",
    "Evaluate the best checkpoint on held-out Rust tasks using execution-based metrics\n",
    "(cargo check, cargo test, clippy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 6.1 Run Rust Evaluation\n",
    "#@markdown Evaluates the best checkpoint on held-out Rust coding tasks.\n",
    "\n",
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    # Determine best checkpoint\n",
    "    if CONFIG[\"include_grpo\"] and os.path.exists(\"checkpoints/core_agent_grpo/final\"):\n",
    "        eval_checkpoint = \"checkpoints/core_agent_grpo/final\"\n",
    "    elif os.path.exists(\"checkpoints/core_agent_ipo/final\"):\n",
    "        eval_checkpoint = \"checkpoints/core_agent_ipo/final\"\n",
    "    elif os.path.exists(\"checkpoints/core_agent/final\"):\n",
    "        eval_checkpoint = \"checkpoints/core_agent/final\"\n",
    "    else:\n",
    "        eval_checkpoint = \"checkpoints/core_agent_ipo/final\"  # fallback\n",
    "\n",
    "    num_samples = CONFIG[\"eval_num_samples\"]\n",
    "\n",
    "    print(f\"Evaluating checkpoint: {eval_checkpoint}\")\n",
    "    print(f\"Samples: {num_samples}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/eval_rust_agent.py \\\n",
    "        --checkpoint {eval_checkpoint} \\\n",
    "        --num_samples {num_samples}\n",
    "\n",
    "    # Backup results to Drive\n",
    "    if IN_COLAB:\n",
    "        !cp -r evals/rust_agent/* {DRIVE_BASE}/evals/rust_agent/ 2>/dev/null || true\n",
    "        print(\"\\nResults backed up to Drive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 6.2 Check Promotion Gates\n",
    "#@markdown Checks whether the model meets the promotion thresholds.\n",
    "\n",
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    print(\"Checking promotion gates...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/12_check_gates.py rust_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 6.3 Display Results\n",
    "#@markdown Loads and pretty-prints evaluation metrics.\n",
    "\n",
    "if CONFIG[\"training_scope\"] == \"lang_adapter_only\":\n",
    "    print(\"Skipping \\u2014 scope is lang_adapter_only\")\n",
    "else:\n",
    "    import json\n",
    "\n",
    "    metrics_path = \"evals/rust_agent/metrics.json\"\n",
    "\n",
    "    if os.path.exists(metrics_path):\n",
    "        with open(metrics_path) as f:\n",
    "            metrics = json.load(f)\n",
    "\n",
    "        # Target thresholds from configs/rust_eval.yaml\n",
    "        targets = {\n",
    "            \"cargo_check_pass_rate\": (0.85, \"higher\"),\n",
    "            \"cargo_test_pass_rate\": (0.70, \"higher\"),\n",
    "            \"clippy_clean_rate\": (0.80, \"higher\"),\n",
    "            \"iterations_to_green_median\": (3, \"lower\"),\n",
    "            \"diff_size_median\": (50, \"lower\"),\n",
    "            \"tool_call_format_accuracy\": (0.99, \"higher\"),\n",
    "            \"hallucinated_api_rate\": (0.05, \"lower\"),\n",
    "        }\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "        print(\"EVALUATION RESULTS\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"{'Metric':<32} {'Value':>8} {'Target':>8} {'Status':>8}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        for key, (target, direction) in targets.items():\n",
    "            value = metrics.get(key)\n",
    "            if value is None:\n",
    "                print(f\"{key:<32} {'N/A':>8} {target:>8} {'\\u2014':>8}\")\n",
    "                continue\n",
    "\n",
    "            if direction == \"higher\":\n",
    "                passed = value >= target\n",
    "            else:\n",
    "                passed = value <= target\n",
    "\n",
    "            status = \"\\u2713 PASS\" if passed else \"\\u2717 FAIL\"\n",
    "            fmt_val = f\"{value:.2%}\" if isinstance(value, float) and value <= 1 else f\"{value}\"\n",
    "            fmt_tgt = f\"{target:.0%}\" if isinstance(target, float) and target <= 1 else f\"{target}\"\n",
    "            print(f\"{key:<32} {fmt_val:>8} {fmt_tgt:>8} {status:>8}\")\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "    else:\n",
    "        print(f\"\\u2717 Metrics file not found at {metrics_path}\")\n",
    "        print(\"Run evaluation (6.1) first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Test Model\n",
    "\n",
    "Load the trained model and generate Rust code interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 7.1 Load Model\n",
    "#@markdown Loads the best checkpoint via Unsloth for fast inference.\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Determine best checkpoint\n",
    "CHECKPOINT_PRIORITY = [\n",
    "    \"checkpoints/core_agent_grpo/final\",\n",
    "    \"checkpoints/core_agent_ipo/final\",\n",
    "    \"checkpoints/core_agent/final\",\n",
    "    \"checkpoints/gpt-oss-20b-rust-merged\",\n",
    "]\n",
    "\n",
    "MODEL_PATH = None\n",
    "for path in CHECKPOINT_PRIORITY:\n",
    "    if os.path.exists(path):\n",
    "        MODEL_PATH = path\n",
    "        break\n",
    "\n",
    "if MODEL_PATH is None:\n",
    "    print(\"\\u2717 No checkpoint found. Train the model first.\")\n",
    "else:\n",
    "    print(f\"Loading model from: {MODEL_PATH}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        max_seq_length=4096,\n",
    "        load_in_4bit=True,\n",
    "        dtype=torch.bfloat16,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model)\n",
    "\n",
    "    print(\"\\u2713 Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 7.2 Generate Rust Code\n",
    "#@markdown Tests the model on 3 pre-defined Rust prompts using Harmony format.\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"scripts\")\n",
    "from dataset_formatters.harmony import encode_harmony_messages\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    \"Write a Rust function `fn merge_sorted(a: &[i32], b: &[i32]) -> Vec<i32>` that merges two sorted slices into a single sorted vector.\",\n",
    "    \"This Rust code fails the borrow checker. Fix it:\\n```rust\\nfn main() {\\n    let mut v = vec![1, 2, 3];\\n    let first = &v[0];\\n    v.push(4);\\n    println!(\\\"{}\\\", first);\\n}\\n```\",\n",
    "    \"Write an async Rust function using tokio that fetches a URL with reqwest, retries up to 3 times on failure, and returns the response body as a String.\",\n",
    "]\n",
    "\n",
    "def generate_rust(prompt, max_tokens=1024):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted = encode_harmony_messages(\n",
    "        messages,\n",
    "        developer_instructions=\"You are a Rust programming expert. Write correct, idiomatic code.\",\n",
    "    )\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.3,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "for i, prompt in enumerate(TEST_PROMPTS, 1):\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Test {i}: {prompt[:80]}...\")\n",
    "    print(\"=\" * 60)\n",
    "    response = generate_rust(prompt)\n",
    "    print(response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 7.3 Custom Prompt\n",
    "\n",
    "CUSTOM_PROMPT = \"Write a Rust function that reads a CSV file and returns the sum of a specified column.\"  #@param {type:\"string\"}\n",
    "\n",
    "print(f\"Prompt: {CUSTOM_PROMPT}\")\n",
    "print(\"=\" * 60)\n",
    "print(generate_rust(CUSTOM_PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Export\n",
    "\n",
    "Merge the final adapter and export to HuggingFace + GGUF formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 8.1 Export to GGUF\n",
    "#@markdown Merges the best adapter into the base model and exports as HF safetensors + GGUF Q4.\n",
    "\n",
    "# Find best adapter\n",
    "ADAPTER_PRIORITY = [\n",
    "    \"checkpoints/core_agent_grpo/final\",\n",
    "    \"checkpoints/core_agent_ipo/final\",\n",
    "    \"checkpoints/core_agent/final\",\n",
    "    \"checkpoints/lang_rust/final\",\n",
    "]\n",
    "\n",
    "adapter_path = None\n",
    "for path in ADAPTER_PRIORITY:\n",
    "    if os.path.exists(path):\n",
    "        adapter_path = path\n",
    "        break\n",
    "\n",
    "if adapter_path is None:\n",
    "    print(\"\\u2717 No adapter checkpoint found.\")\n",
    "else:\n",
    "    export_dir = \"checkpoints/gpt-oss-20b-rust-export\"\n",
    "    print(f\"Exporting adapter: {adapter_path}\")\n",
    "    print(f\"Output: {export_dir}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    !python scripts/19_merge_adapter.py \\\n",
    "        --adapter_path {adapter_path} \\\n",
    "        --output_dir {export_dir} \\\n",
    "        --export_formats hf gguf_q4\n",
    "\n",
    "    # Backup to Drive\n",
    "    if IN_COLAB:\n",
    "        drive_export = os.path.join(DRIVE_BASE, \"checkpoints/gpt-oss-20b-rust-export\")\n",
    "        os.makedirs(drive_export, exist_ok=True)\n",
    "        !cp -r {export_dir}/* {drive_export}/ 2>/dev/null || true\n",
    "        print(\"\\nExport backed up to Drive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### 8.2 Download GGUF\n",
    "#@markdown Downloads the GGUF file directly (Colab only).\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    import glob\n",
    "\n",
    "    export_dir = \"checkpoints/gpt-oss-20b-rust-export\"\n",
    "    gguf_files = glob.glob(os.path.join(export_dir, \"*.gguf\"))\n",
    "\n",
    "    if gguf_files:\n",
    "        gguf_path = gguf_files[0]\n",
    "        size_gb = os.path.getsize(gguf_path) / (1024**3)\n",
    "        print(f\"Downloading: {os.path.basename(gguf_path)} ({size_gb:.1f} GB)\")\n",
    "        files.download(gguf_path)\n",
    "    else:\n",
    "        print(\"\\u2717 No GGUF file found. Run export (8.1) first.\")\n",
    "else:\n",
    "    print(\"Download not available outside Colab.\")\n",
    "    print(\"GGUF file is at: checkpoints/gpt-oss-20b-rust-export/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Complete!\n",
    "\n",
    "Your GPT-OSS 20B Rust coding agent is trained and ready to use.\n",
    "\n",
    "**Outputs:**\n",
    "- Checkpoints: `checkpoints/core_agent_{ipo,grpo}/final`\n",
    "- Evaluation: `evals/rust_agent/metrics.json`\n",
    "- Exported model: `checkpoints/gpt-oss-20b-rust-export/`\n",
    "- All backed up to Google Drive: `{DRIVE_BASE}`\n",
    "\n",
    "**Next steps:**\n",
    "- Review evaluation metrics in Step 6.3\n",
    "- Test interactively in Step 7\n",
    "- Deploy the GGUF file with llama.cpp or Ollama"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
