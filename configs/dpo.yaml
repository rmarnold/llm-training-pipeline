run_name: "llm-7b-dpo-v1"

model:
  checkpoint: "checkpoints/sft_final"
  gradient_checkpointing: true

data:
  num_workers: 8
  persistent_workers: true
  pin_memory: true

training:
  num_train_epochs: 1
  learning_rate: 5.0e-7  # Very conservative for DPO

  per_device_train_batch_size: 1
  gradient_accumulation_steps: 32

  bf16: true
  max_grad_norm: 1.0

  # DPO-specific
  beta: 0.1  # KL penalty coefficient
  loss_type: "sigmoid"  # or "hinge", "ipo"
  max_length: 2048
  max_prompt_length: 1024

  # PyTorch 2.x optimizations
  torch_compile: true
  torch_compile_backend: "inductor"
  torch_compile_mode: "default"  # Use "max-autotune" for H100

logging:
  logging_steps: 10
  eval_steps: 100
  save_steps: 200

eval:
  evaluation_strategy: "steps"
  eval_steps: 100
