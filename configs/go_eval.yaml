run_name: "eval-go-agent"
language: "go"

model:
  checkpoint: "checkpoints/core_agent_go_grpo"
  max_seq_length: 16384
  load_in_4bit: true
  dtype: null

evaluation:
  test_set: "data/go/eval/tasks.jsonl"
  num_samples: 200
  max_iterations_per_task: 10
  timeouts:
    go_build: 30
    go_test: 300
    go_vet: 30
    golangci_lint: 60
    per_task: 600

generation:
  temperature: 0.2
  max_new_tokens: 2048

output:
  results_dir: "evals/go_agent"
  save_trajectories: true

metrics:
  go_build_pass_rate:
    description: "Percentage of generated code that compiles successfully with go build"
    target: 0.85
    higher_is_better: true
  go_test_pass_rate:
    description: "Percentage of generated code that passes go test"
    target: 0.70
    higher_is_better: true
  go_vet_clean_rate:
    description: "Percentage of code with no go vet warnings"
    target: 0.85
    higher_is_better: true
  golangci_lint_clean_rate:
    description: "Percentage of code with no golangci-lint findings"
    target: 0.75
    higher_is_better: true
  iterations_to_green_median:
    description: "Median iterations needed to pass all tests"
    target: 3
    higher_is_better: false
  diff_size_median:
    description: "Median diff size in lines"
    target: 50
    higher_is_better: false
  tool_call_format_accuracy:
    description: "Percentage of tool calls with valid JSON"
    target: 0.99
    higher_is_better: true
  hallucinated_api_rate:
    description: "Percentage of calls to non-existent APIs"
    target: 0.05
    higher_is_better: false

regression_checks:
  humaneval_python:
    description: "HumanEval Python pass@1 (should not regress >5%)"
    max_regression: 0.05
  mmlu_subset:
    description: "MMLU accuracy (should not regress >5%)"
    max_regression: 0.05
