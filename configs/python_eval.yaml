run_name: "eval-python-agent"
language: "python"

model:
  checkpoint: "checkpoints/core_agent_python_grpo"
  max_seq_length: 16384
  load_in_4bit: true
  dtype: null

evaluation:
  test_set: "data/python/eval/tasks.jsonl"
  num_samples: 200
  max_iterations_per_task: 10
  timeouts:
    syntax_check: 10
    pytest: 300
    mypy: 60
    ruff: 30
    per_task: 600

generation:
  temperature: 0.2
  max_new_tokens: 2048

output:
  results_dir: "evals/python_agent"
  save_trajectories: true

metrics:
  syntax_check_pass_rate:
    description: "Percentage of generated code that is syntactically valid Python"
    target: 0.90
    higher_is_better: true
  pytest_pass_rate:
    description: "Percentage of generated code that passes pytest"
    target: 0.70
    higher_is_better: true
  mypy_clean_rate:
    description: "Percentage of code with no mypy type errors"
    target: 0.70
    higher_is_better: true
  ruff_clean_rate:
    description: "Percentage of code with no ruff lint warnings"
    target: 0.80
    higher_is_better: true
  iterations_to_green_median:
    description: "Median iterations needed to pass all tests"
    target: 3
    higher_is_better: false
  diff_size_median:
    description: "Median diff size in lines"
    target: 50
    higher_is_better: false
  tool_call_format_accuracy:
    description: "Percentage of tool calls with valid JSON"
    target: 0.99
    higher_is_better: true
  hallucinated_api_rate:
    description: "Percentage of calls to non-existent APIs"
    target: 0.05
    higher_is_better: false

regression_checks:
  humaneval_python:
    description: "HumanEval Python pass@1 (should not regress >5%)"
    max_regression: 0.05
  mmlu_subset:
    description: "MMLU accuracy (should not regress >5%)"
    max_regression: 0.05
