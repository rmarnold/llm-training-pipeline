# ==========================================================================
# PRETRAINING CONFIG FOR 1B REASONING MODEL (from scratch)
# ==========================================================================
# Target: Train 1B model on ~100B tokens for reasoning & tool-calling
#
# Expected time (A100 80GB):
#   - ~50K steps: ~20-25 hours
#   - ~100K steps: ~40-50 hours
#
# Expected outcomes:
#   - Perplexity < 12
#   - Basic language understanding
#   - Foundation for SFT reasoning capability
# ==========================================================================

run_name: "llm-1b-reasoning-pretrain"

model:
  size: "1b"  # Use 1B configuration from model_configs.py
  checkpoint: "checkpoints/init"  # Will be created by 04_init_model.py --size 1b
  gradient_checkpointing: true
  use_flash_attention: true

data:
  # Use the pretraining datasets from data_sources.yaml
  train_data: "data/packed/train_{seq_length}"
  val_data: "data/packed/val_{seq_length}"
  val_split: 0.01
  num_workers: 8
  prefetch_factor: 4
  persistent_workers: true
  pin_memory: true

training:
  # Training length
  num_train_epochs: 1
  max_steps: 50000  # ~50B tokens with current batch config

  # Learning rate schedule
  learning_rate: 3.0e-4
  lr_scheduler: "cosine"
  warmup_steps: 2500  # 5% of training
  min_lr: 3.0e-5

  # Batch sizing for A100 80GB (1B model = more room)
  per_device_train_batch_size: 16
  gradient_accumulation_steps: 4  # effective batch = 64
  max_seq_length: 2048

  # Precision & stability
  bf16: true
  tf32: true
  max_grad_norm: 1.0
  weight_decay: 0.1
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1.0e-8

  # Memory optimization
  optim: "adamw_torch_fused"
  fsdp: "full_shard auto_wrap"
  fsdp_transformer_layer_cls_to_wrap: "LlamaDecoderLayer"

  # PyTorch 2.x optimizations
  torch_compile: true
  torch_compile_backend: "inductor"
  torch_compile_mode: "default"  # Use "max-autotune" for H100

  # Kernel optimizations
  use_liger_kernel: true
  use_cce: true

logging:
  logging_steps: 10
  eval_steps: 500
  save_steps: 500
  save_total_limit: 5
  report_to: ["tensorboard"]
  logging_dir: "logs/pretrain_1b"

eval:
  evaluation_strategy: "steps"
  eval_steps: 500
  per_device_eval_batch_size: 16
  eval_accumulation_steps: 10

checkpointing:
  output_dir: "checkpoints/pretrain_1b"
  save_strategy: "steps"
  save_steps: 500
  resume_from_checkpoint: null
  load_best_model_at_end: false

# Context length curriculum (optional but recommended)
# Start with shorter sequences, gradually increase
curriculum:
  enabled: true
  data_pattern: "data/packed/train_{seq_length}"
  val_pattern: "data/packed/val_{seq_length}"
  auto_stop_at_boundary: true
  schedule:
    - {steps: 20000, seq_length: 512}
    - {steps: 35000, seq_length: 1024}
    - {steps: 50000, seq_length: 2048}
