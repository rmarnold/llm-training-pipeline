# ==========================================================================
# DPO CONFIG FOR 1B REASONING MODEL
# ==========================================================================
# Focus: Safety alignment + preference optimization
#
# Expected time (A100 80GB):
#   - 1 epoch on ~160K samples: ~3-4 hours
#
# Expected outcomes:
#   - Safety refusal rate > 80%
#   - Maintained reasoning capability
#   - Preference accuracy > 60%
# ==========================================================================

run_name: "llm-1b-reasoning-dpo"

model:
  checkpoint: "checkpoints/sft_1b_final"
  gradient_checkpointing: true

data:
  train_data: "data/dpo/train"
  val_data: "data/dpo/val"
  num_workers: 8
  persistent_workers: true
  pin_memory: true

training:
  num_train_epochs: 1
  learning_rate: 5.0e-7  # Very conservative for DPO

  per_device_train_batch_size: 2
  gradient_accumulation_steps: 16  # effective batch = 32

  bf16: true
  max_grad_norm: 1.0

  # DPO-specific
  beta: 0.1  # KL penalty coefficient
  loss_type: "sigmoid"
  max_length: 2048
  max_prompt_length: 1024

  # PyTorch 2.x optimizations
  torch_compile: true
  torch_compile_backend: "inductor"
  torch_compile_mode: "default"

logging:
  logging_steps: 10
  eval_steps: 100
  save_steps: 200

eval:
  evaluation_strategy: "steps"
  eval_steps: 100

checkpointing:
  output_dir: "checkpoints/dpo_1b"
  save_strategy: "steps"
