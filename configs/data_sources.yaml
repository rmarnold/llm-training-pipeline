datasets:
  # ==========================================================================
  # PRETRAINING DATA (~100B+ tokens for 1B model from scratch)
  # ==========================================================================
  pretraining:
    # SlimPajama - cleaned RedPajama, 627B tokens total
    # Note: This is a HUGE dataset. We stream a small portion.
    - name: "slimpajama"
      source: "cerebras/SlimPajama-627B"
      # No subset needed - use default config
      license: "Apache-2.0"
      tokens: 50B
      weight: 0.05  # Take 5% = ~30M samples
      streaming: true

    # FineWeb-Edu - High quality educational web text (replaces RedPajama)
    - name: "fineweb-edu-sample"
      source: "HuggingFaceFW/fineweb-edu"
      subset: "sample-10BT"
      license: "ODC-By-1.0"
      tokens: 10B
      weight: 0.3
      streaming: true

    # Wikipedia - factual grounding
    - name: "wikipedia-en"
      source: "wikimedia/wikipedia"
      version: "20231101.en"
      license: "CC-BY-SA-3.0"
      tokens: 3.5B
      weight: 1.0

    # OpenWebText - web text
    - name: "openwebtext"
      source: "Skylion007/openwebtext"
      license: "CC0"
      tokens: 8B
      weight: 0.7

    # C4 - Colossal Clean Crawled Corpus (replaces pg19 which has script issues)
    - name: "c4-en"
      source: "allenai/c4"
      subset: "en"
      license: "ODC-By-1.0"
      tokens: 150B
      weight: 0.02  # Take 2% = ~7M samples
      streaming: true

  # ==========================================================================
  # REASONING DATA (for SFT - chain-of-thought, math, logic)
  # ==========================================================================
  reasoning:
    # GSM8K - Grade school math with solutions
    - name: "gsm8k"
      source: "gsm8k"
      subset: "main"
      license: "MIT"
      samples: 7.5K
      weight: 1.0
      format: "gsm8k"

    # Orca Math - Synthetic math problems
    - name: "orca-math"
      source: "microsoft/orca-math-word-problems-200k"
      license: "MIT"
      samples: 200K
      weight: 0.8
      format: "orca-math"

    # OpenOrca - GPT-4 distillation (large, sample 30%)
    - name: "openorca"
      source: "Open-Orca/OpenOrca"
      license: "MIT"
      samples: 4.2M
      weight: 0.3
      format: "openorca"

    # MathInstruct - Replaces CoT-Collection (same purpose, works)
    - name: "mathinstruct"
      source: "TIGER-Lab/MathInstruct"
      license: "Apache-2.0"
      samples: 262K
      weight: 0.7
      format: "mathinstruct"

    # MetaMath - Augmented math problems
    - name: "metamath"
      source: "meta-math/MetaMathQA"
      license: "MIT"
      samples: 395K
      weight: 0.7
      format: "metamath"

  # ==========================================================================
  # FUNCTION/TOOL CALLING DATA
  # ==========================================================================
  function_calling:
    # Glaive function calling
    - name: "glaive-function-calling"
      source: "glaiveai/glaive-function-calling-v2"
      license: "Apache-2.0"
      samples: 113K
      weight: 1.0
      format: "glaive"

    # Hermes function calling
    - name: "hermes-function-calling"
      source: "NousResearch/hermes-function-calling-v1"
      license: "Apache-2.0"
      samples: 11K
      weight: 1.0
      format: "hermes"

    # Gorilla OpenFunctions - Replaces ToolBench (better maintained)
    - name: "gorilla-openfunctions"
      source: "gorilla-llm/Berkeley-Function-Calling-Leaderboard"
      license: "Apache-2.0"
      samples: 2K
      weight: 1.0
      format: "gorilla"

  # ==========================================================================
  # LOGIC AND STRUCTURED REASONING
  # ==========================================================================
  logic:
    # LogiQA2 - Updated version that works
    - name: "logiqa2"
      source: "logikon/logiqa2-nli"
      license: "Apache-2.0"
      samples: 10K
      weight: 1.0
      format: "logiqa"

    # ARC Challenge - Science reasoning
    - name: "arc-challenge"
      source: "allenai/ai2_arc"
      subset: "ARC-Challenge"
      license: "CC-BY-SA-4.0"
      samples: 2.6K
      weight: 1.0
      format: "arc"

  # ==========================================================================
  # INSTRUCTION FOLLOWING (general capability)
  # ==========================================================================
  instruction_tuning:
    - name: "oasst1"
      source: "OpenAssistant/oasst1"
      license: "Apache-2.0"
      samples: 88K
      weight: 0.5
      format: "oasst"

    - name: "alpaca-cleaned"
      source: "yahma/alpaca-cleaned"
      license: "CC-BY-NC-4.0"
      samples: 52K
      weight: 0.3
      format: "alpaca"

    - name: "dolly-15k"
      source: "databricks/databricks-dolly-15k"
      license: "CC-BY-SA-3.0"
      samples: 15K
      weight: 0.4
      format: "alpaca"

  # ==========================================================================
  # PREFERENCE DATA (for DPO)
  # ==========================================================================
  preference_data:
    - name: "hh-rlhf"
      source: "Anthropic/hh-rlhf"
      license: "MIT"
      samples: 169K
      weight: 1.0

    - name: "ultrafeedback"
      source: "openbmb/UltraFeedback"
      license: "MIT"
      samples: 64K
      weight: 0.6

governance:
  pii_detection: true
  toxicity_threshold: 0.7
  deduplication: "minhash"
  contamination_check: ["MMLU", "HumanEval", "GSM8K", "ARC"]

# ==========================================================================
# TRAINING RECOMMENDATIONS FOR 1B MODEL FROM SCRATCH
# ==========================================================================
#
# Target: ~50-100B tokens for compute-optimal 1B training
#
# Phase 1 - Pretraining:
#   - Use SlimPajama + Wikipedia + OpenWebText
#   - ~50-100B tokens
#   - Train for ~50K-100K steps with batch size 64
#
# Phase 2 - SFT:
#   - reasoning + function_calling + logic + instruction_tuning
#   - ~2-3M high-quality examples
#   - 3 epochs
#
# Phase 3 - DPO:
#   - hh-rlhf + ultrafeedback
#   - 1 epoch, low learning rate
