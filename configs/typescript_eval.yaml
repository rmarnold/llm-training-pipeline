run_name: "eval-typescript-agent"
language: "typescript"

model:
  checkpoint: "checkpoints/core_agent_typescript_grpo"
  max_seq_length: 16384
  load_in_4bit: true
  dtype: null

evaluation:
  test_set: "data/typescript/eval/tasks.jsonl"
  num_samples: 200
  max_iterations_per_task: 10
  timeouts:
    tsc: 30
    jest: 300
    eslint: 30
    per_task: 600

generation:
  temperature: 0.2
  max_new_tokens: 2048

output:
  results_dir: "evals/typescript_agent"
  save_trajectories: true

metrics:
  tsc_pass_rate:
    description: "Percentage of generated code that passes TypeScript compilation"
    target: 0.85
    higher_is_better: true
  jest_pass_rate:
    description: "Percentage of generated code that passes Jest tests"
    target: 0.70
    higher_is_better: true
  eslint_clean_rate:
    description: "Percentage of code with no ESLint warnings or errors"
    target: 0.80
    higher_is_better: true
  iterations_to_green_median:
    description: "Median iterations needed to pass all tests"
    target: 3
    higher_is_better: false
  diff_size_median:
    description: "Median diff size in lines"
    target: 50
    higher_is_better: false
  tool_call_format_accuracy:
    description: "Percentage of tool calls with valid JSON"
    target: 0.99
    higher_is_better: true
  hallucinated_api_rate:
    description: "Percentage of calls to non-existent APIs"
    target: 0.05
    higher_is_better: false

regression_checks:
  humaneval_python:
    description: "HumanEval Python pass@1 (should not regress >5%)"
    max_regression: 0.05
  mmlu_subset:
    description: "MMLU accuracy (should not regress >5%)"
    max_regression: 0.05
