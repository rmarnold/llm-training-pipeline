run_name: "llm-7b-pretrain-v1"

model:
  checkpoint: "checkpoints/init"
  gradient_checkpointing: true
  use_flash_attention: true

data:
  train_data: "data/packed/pretrain_ctx*.npy"
  val_split: 0.01
  num_workers: 8
  prefetch_factor: 4
  persistent_workers: true
  pin_memory: true

training:
  # Optimization
  num_train_epochs: 1
  max_steps: 100000
  learning_rate: 3.0e-4
  lr_scheduler: "cosine"
  warmup_steps: 5000  # Increased for better stability (5% of training)
  min_lr: 3.0e-5

  # Batch sizing for A100 80GB (optimized)
  # Increased batch size with 8-bit optimizer frees ~30GB memory
  per_device_train_batch_size: 8  # Doubled for better GPU utilization
  gradient_accumulation_steps: 4  # effective batch = 32
  max_seq_length: 2048

  # Precision & stability
  bf16: true
  tf32: true
  max_grad_norm: 1.0
  weight_decay: 0.1
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1.0e-8

  # Memory optimization
  # Use 8-bit Adam for 4x optimizer memory reduction (~30GB saved)
  # Falls back to adamw_torch_fused if bitsandbytes not installed
  optim: "adamw_bnb_8bit"
  fsdp: "full_shard auto_wrap"
  fsdp_transformer_layer_cls_to_wrap: "LlamaDecoderLayer"

  # PyTorch 2.x optimizations
  torch_compile: true
  torch_compile_backend: "inductor"
  torch_compile_mode: "default"  # Use "max-autotune" for H100

  # Kernel optimizations (Liger + CCE work with torch.compile)
  use_liger_kernel: true   # +20% throughput, -60% memory
  use_cce: true            # -95% memory on cross-entropy loss

logging:
  logging_steps: 10
  eval_steps: 1000  # Reduced frequency to minimize overhead
  save_steps: 1000
  save_total_limit: 5
  report_to: ["tensorboard"]
  logging_dir: "logs/pretrain"

eval:
  evaluation_strategy: "steps"
  eval_steps: 1000  # Reduced frequency to minimize overhead
  per_device_eval_batch_size: 8  # Increased for faster eval
  eval_accumulation_steps: 10  # Reduced for faster eval

checkpointing:
  output_dir: "checkpoints/pretrain"
  save_strategy: "steps"
  save_steps: 1000
  resume_from_checkpoint: null
  load_best_model_at_end: false

# Context length curriculum
# When enabled, training will automatically stop at curriculum boundaries
# to reload data with the appropriate sequence length.
curriculum:
  enabled: true
  # Data paths with {seq_length} placeholder
  data_pattern: "data/packed/train_{seq_length}"
  val_pattern: "data/packed/val_{seq_length}"
  # Auto-stop at stage boundaries for data reload (recommended)
  auto_stop_at_boundary: true
  schedule:
    - {steps: 40000, seq_length: 512}
    - {steps: 70000, seq_length: 1024}
    - {steps: 100000, seq_length: 2048}
