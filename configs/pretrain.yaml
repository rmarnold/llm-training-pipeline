run_name: "llm-7b-pretrain-v1"

model:
  checkpoint: "checkpoints/init"
  gradient_checkpointing: true
  use_flash_attention: true

data:
  train_data: "data/packed/pretrain_ctx*.npy"
  val_split: 0.01
  num_workers: 8
  prefetch_factor: 4
  persistent_workers: true
  pin_memory: true

training:
  # Optimization
  num_train_epochs: 1
  max_steps: 100000
  learning_rate: 3.0e-4
  lr_scheduler: "cosine"
  warmup_steps: 5000  # Increased for better stability (5% of training)
  min_lr: 3.0e-5

  # Batch sizing for A100 80GB (optimized)
  per_device_train_batch_size: 4  # increased for better GPU utilization
  gradient_accumulation_steps: 8  # effective batch = 32
  max_seq_length: 2048  # start short, curriculum later

  # Precision & stability
  bf16: true
  tf32: true
  max_grad_norm: 1.0
  weight_decay: 0.1
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1.0e-8

  # Memory optimization
  optim: "adamw_torch_fused"
  fsdp: "full_shard auto_wrap"
  fsdp_transformer_layer_cls_to_wrap: "LlamaDecoderLayer"

  # PyTorch 2.x optimizations
  torch_compile: true
  torch_compile_backend: "inductor"
  torch_compile_mode: "default"  # Use "max-autotune" for H100

logging:
  logging_steps: 10
  eval_steps: 1000  # Reduced frequency to minimize overhead
  save_steps: 1000
  save_total_limit: 5
  report_to: ["tensorboard"]
  logging_dir: "logs/pretrain"

eval:
  evaluation_strategy: "steps"
  eval_steps: 1000  # Reduced frequency to minimize overhead
  per_device_eval_batch_size: 4  # Increased for faster eval
  eval_accumulation_steps: 20  # Batch eval results

checkpointing:
  output_dir: "checkpoints/pretrain"
  save_strategy: "steps"
  save_steps: 1000
  resume_from_checkpoint: null
  load_best_model_at_end: false

# Context length curriculum
curriculum:
  enabled: true
  schedule:
    - {steps: 40000, seq_length: 512}
    - {steps: 70000, seq_length: 1024}
    - {steps: 100000, seq_length: 2048}
