# Production Training Pipeline - Complete Guide

## Overview
This guide contains all instructions to run full production training of the 7B model with optimized GPU utilization (85-100%).

## Current System Status

### âœ… Completed Setup
- **Model**: 7B (6.08B params) initialized with correct vocab (50,304)
- **GPU**: NVIDIA A100 80GB verified and tested
- **Dependencies**: All installed and working
- **Optimizations**: Achieved 100% GPU utilization with 512-token sequences

### ðŸ“Š Performance Achieved
- **GPU Utilization**: 98-100%
- **Power Draw**: 393-404W (maxed out)
- **Memory Usage**: 50 GB / 80 GB
- **Training Speed**: ~2.3 sec/iteration
- **Configuration**: BF16 + TF32, AdamW Fused, Gradient Checkpointing

---

## Quick Start: Full Production Training

### Option 1: Use Existing Full Pipeline Script (Recommended)

```bash
# Navigate to project directory
cd /content/drive/MyDrive/claude-code-config/projects/llm-codev1

# Set environment variables
export USER=root
export HOME=/root
export TORCHINDUCTOR_CACHE_DIR=/tmp/torch_cache

# Run the complete pipeline (all stages)
bash scripts/run_full_pipeline.sh
```

This script will execute all 7 stages:
1. Data download and preparation
2. Model initialization
3. Smoke tests
4. Pretraining (100K steps)
5. Supervised Fine-Tuning
6. DPO (Preference Optimization)
7. LoRA Fine-tuning (optional)

### Option 2: Run Individual Stages

#### Stage 1: Prepare Production Data

```bash
# Download and prepare WikiText-103 (100K samples, 512 tokens each)
python scripts/prepare_production_data.py
```

Expected output:
- File: `data/packed/production_pretrain.npy`
- Size: ~200 MB (100K Ã— 512 tokens)
- Time: ~10-15 minutes

#### Stage 2: Verify Model Initialization

```bash
# Check model is ready (should already exist)
ls -lh checkpoints/init/

# If not present, initialize:
python scripts/04_init_model.py
```

Expected output:
- Model: 6.08B parameters
- Files: 5 safetensor shards (~23 GB total)

#### Stage 3: Start Production Training

```bash
# Launch production training with maximum GPU utilization
python scripts/production_pretrain.py
```

---

## Production Training Script

Create `scripts/production_pretrain.py` with this content:

```python
"""Production 7B model training - Optimized for 85-95% GPU utilization"""
import torch
from transformers import (
    Trainer, TrainingArguments, AutoTokenizer,
    AutoModelForCausalLM, DataCollatorForLanguageModeling
)
from datasets import Dataset
import numpy as np

def load_production_data():
    """Load production training data"""
    print("Loading production dataset...")
    data = np.load("data/packed/production_pretrain.npy")
    print(f"  Loaded {len(data):,} sequences of {data.shape[1]} tokens")

    # Split train/val
    split_idx = int(len(data) * 0.95)
    train_data = data[:split_idx]
    val_data = data[split_idx:]

    train_dataset = Dataset.from_dict({"input_ids": train_data.tolist()})
    val_dataset = Dataset.from_dict({"input_ids": val_data.tolist()})

    print(f"  Train: {len(train_dataset):,} samples")
    print(f"  Val: {len(val_dataset):,} samples")
    return train_dataset, val_dataset

def production_training(max_steps=100000):
    """Run full production training"""
    print("\\n" + "="*60)
    print("PRODUCTION 7B MODEL TRAINING")
    print("="*60 + "\\n")

    # Load model
    print("Loading 7B model...")
    model = AutoModelForCausalLM.from_pretrained(
        "checkpoints/init",
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained("configs/tokenizer")
    model.gradient_checkpointing_enable()

    print(f"  Model: {model.num_parameters() / 1e9:.2f}B parameters")

    # Load data
    train_dataset, val_dataset = load_production_data()

    # Training arguments - OPTIMIZED for 85-95% GPU
    training_args = TrainingArguments(
        output_dir="checkpoints/production_pretrain",
        run_name="production-7b-pretrain",

        # Training schedule
        max_steps=max_steps,
        per_device_train_batch_size=4,      # Optimized for A100
        gradient_accumulation_steps=8,      # Effective batch = 32

        # Learning rate schedule
        learning_rate=3e-4,
        lr_scheduler_type="cosine",
        warmup_steps=2000,

        # Precision & optimization
        bf16=True,
        tf32=True,
        optim="adamw_torch_fused",
        max_grad_norm=1.0,
        weight_decay=0.1,

        # Logging & evaluation
        logging_steps=10,
        logging_dir="logs/production_pretrain",
        eval_strategy="steps",
        eval_steps=500,
        report_to=["tensorboard"],

        # Checkpointing
        save_strategy="steps",
        save_steps=1000,
        save_total_limit=5,

        # Data loading
        dataloader_num_workers=8,
        dataloader_pin_memory=True,
        dataloader_prefetch_factor=4,

        # Memory optimization
        gradient_checkpointing=True,
    )

    print(f"\\nProduction Configuration:")
    print(f"  Sequence length: 512 tokens")
    print(f"  Batch size: {training_args.per_device_train_batch_size}")
    print(f"  Gradient accumulation: {training_args.gradient_accumulation_steps}")
    print(f"  Effective batch: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
    print(f"  Max steps: {max_steps:,}")
    print(f"  Learning rate: {training_args.learning_rate}")
    print(f"  Optimizer: {training_args.optim}")

    # Initialize trainer
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
    )

    # Start training
    print(f"\\nðŸš€ Starting production training ({max_steps:,} steps)...")
    print(f"   Expected GPU utilization: 85-95%")
    print(f"   Estimated time: ~{max_steps * 2.3 / 3600:.1f} hours\\n")

    trainer.train()

    # Save final model
    trainer.save_model("checkpoints/production_pretrain_final")
    print(f"\\nâœ“ Training complete!")
    print(f"  Checkpoint: checkpoints/production_pretrain_final/")

if __name__ == "__main__":
    production_training(max_steps=100000)
```

---

## Monitoring Training

### Real-time GPU Monitoring

```bash
# Watch GPU utilization (refresh every 2 seconds)
watch -n 2 nvidia-smi

# Detailed metrics
nvidia-smi --query-gpu=utilization.gpu,memory.used,power.draw,temperature.gpu --format=csv -l 5
```

### TensorBoard Monitoring

```bash
# Start TensorBoard
tensorboard --logdir logs/production_pretrain --port 6006

# Access at: http://localhost:6006
```

### Check Training Logs

```bash
# Real-time log monitoring
tail -f production_training.log

# Check latest progress
tail -50 production_training.log
```

---

## Expected Performance Metrics

### GPU Utilization Targets
- **85-95%**: Normal production training (batch_size=4, seq_len=512)
- **98-100%**: Maximum utilization (requires careful tuning)

### Training Speed
- **~2.3 seconds/iteration** at 512 tokens
- **~100,000 steps = 64 hours** (2.7 days)
- **Checkpoints every 1,000 steps** (~2.3 hours)

### Resource Usage
- **GPU Memory**: 48-52 GB / 80 GB
- **Power Draw**: 350-400W
- **Temperature**: 55-65Â°C (safe range)

---

## Troubleshooting

### Issue: OOM (Out of Memory)

```bash
# Reduce batch size in training script
# Change: per_device_train_batch_size=4
# To:     per_device_train_batch_size=2

# Or reduce gradient accumulation
# Change: gradient_accumulation_steps=8
# To:     gradient_accumulation_steps=4
```

### Issue: GPU Not Being Used

```bash
# Verify CUDA available
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"

# Check environment variables
echo $USER
echo $TORCHINDUCTOR_CACHE_DIR
```

### Issue: Training Crashed/Interrupted

```bash
# Resume from latest checkpoint
bash scripts/resume_pipeline.sh pretrain
```

---

## Full Pipeline Stages (run_full_pipeline.sh)

The complete pipeline includes:

1. **Data Pipeline** (scripts/01_download_data.py â†’ 03_tokenize_and_pack.py)
   - Downloads: WikiText, Wikipedia, OpenWebText
   - Processing: Cleaning, deduplication, tokenization
   - Output: Packed .npy files

2. **Model Initialization** (scripts/04_init_model.py)
   - Creates 7B model from scratch
   - Config: 32 layers, 4096 hidden, 50304 vocab
   - Output: checkpoints/init/

3. **Pretraining** (scripts/05_pretrain.py)
   - 100K steps with context curriculum (512â†’1024â†’2048)
   - Validation gates: Perplexity <15
   - Output: checkpoints/pretrain_final/

4. **SFT** (scripts/07_sft.py)
   - 3 epochs on instruction-following data
   - Validation gates: Accuracy >70%
   - Output: checkpoints/sft_final/

5. **DPO** (scripts/09_dpo.py)
   - 1 epoch on preference pairs
   - Validation gates: Preference >65%
   - Output: checkpoints/dpo_final/

6. **LoRA Fine-tuning** (scripts/10_lora_finetune.py)
   - Optional domain adaptation
   - Output: checkpoints/lora_merged/

---

## Configuration Files

All hyperparameters are in `/configs/`:

- `pretrain.yaml` - Pretraining settings
- `sft.yaml` - SFT settings
- `dpo.yaml` - DPO settings
- `model_7b.py` - Model architecture
- `promotion_gates.yaml` - Validation thresholds

---

## Key Optimizations Applied

1. âœ… **Vocab Size Fixed**: 50,304 (matches tokenizer)
2. âœ… **Sequence Length**: 512 tokens (4x baseline)
3. âœ… **Batch Size**: 4 per device, accumulate 8 (effective=32)
4. âœ… **Mixed Precision**: BF16 + TF32
5. âœ… **Optimizer**: AdamW Fused (fastest)
6. âœ… **Gradient Checkpointing**: Enabled
7. âœ… **Data Loading**: 8 workers, prefetch=4
8. âœ… **Flash Attention**: Enabled (memory efficient)

---

## Next Steps

1. **Prepare data**: `python scripts/prepare_production_data.py`
2. **Create training script**: Copy production_pretrain.py code above
3. **Start training**: `python scripts/production_pretrain.py`
4. **Monitor**: `watch -n 2 nvidia-smi`
5. **View logs**: `tensorboard --logdir logs/production_pretrain`

**Estimated Total Time**: ~64 hours for 100K pretraining steps

Good luck with your production training! ðŸš€
